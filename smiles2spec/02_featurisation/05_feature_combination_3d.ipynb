{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2A: 3D Feature Combination and ML Data Preparation\n",
    "\n",
    "This notebook implements the first part of Stage 2 for 3D features, focusing on combining comprehensive 3D molecular features (984 features) with spectral data and preparing ML-ready datasets.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### Responsibilities:\n",
    "1. **Load Intermediate Data**: Read 3D molecular features (984 features) and spectral data from Stage 1\n",
    "2. **Process Spectra**: Apply binning for regression and preserve full resolution for seq2seq\n",
    "3. **Combine Data**: Merge 3D molecular features with spectral data\n",
    "4. **Preprocess Features**: Scale, filter, and prepare 3D features for ML\n",
    "5. **Split Data**: Create train/validation/test sets\n",
    "6. **Generate ML Formats**:\n",
    "   - Regression format with binned spectra\n",
    "   - Seq2seq format with PCA-reduced 3D features and full resolution outputs\n",
    "7. **Save Visualization Data**: Store sample data for Stage 2B visualization\n",
    "\n",
    "### 3D Features (984 total):\n",
    "- Basic 3D descriptors: 11 features\n",
    "- AUTOCORR3D: 80 features  \n",
    "- RDF: 210 features\n",
    "- MORSE: 224 features\n",
    "- WHIM: 114 features\n",
    "- GETAWAY: 273 features\n",
    "- USR: 12 features\n",
    "- USRCAT: 60 features\n",
    "\n",
    "### Input (from Stage 1):\n",
    "- `data/tmp/{dataset_type}/raw_spectral_data_3d.jsonl`\n",
    "- `data/tmp/{dataset_type}/molecular_features_3d.jsonl`\n",
    "- `data/tmp/{dataset_type}/feature_importance_3d.json`\n",
    "- `data/tmp/{dataset_type}/dataset_metadata_3d.json`\n",
    "- `data/tmp/{dataset_type}/dataset_config_3d.json`\n",
    "\n",
    "### Output:\n",
    "#### Regression Format (`data/results/{dataset_type}/full_featurised_3d/`):\n",
    "- `train_data_3d.jsonl`, `val_data_3d.jsonl`, `test_data_3d.jsonl`\n",
    "- `feature_preprocessor_3d.pkl` - Scaling and preprocessing state\n",
    "- `feature_mapping_3d.jsonl` - 3D feature metadata\n",
    "- `visualization_data_3d.pkl` - Sample data for Stage 2B\n",
    "\n",
    "#### Seq2Seq Format (`data/results/{dataset_type}/seq2seq_featurised_3d/`):\n",
    "- `train_data_3d.jsonl`, `val_data_3d.jsonl`, `test_data_3d.jsonl`\n",
    "- `pca_transformer_3d.pkl` - PCA transformation matrix for 3D features\n",
    "- `seq2seq_config_3d.json` - Configuration metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries for spectral processing, ML preparation, and 3D feature handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import traceback\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import psutil\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configuration for Stage 2A with comprehensive 3D features (984 features), focusing on data processing and ML preparation.\n",
    "\n",
    "**Key Configuration Sections:**\n",
    "- `spectral`: Binning and peak processing parameters\n",
    "- `seq2seq`: Sequence-to-sequence model settings with 3D feature PCA\n",
    "- `feature_scaling`: Scaling options for 3D features\n",
    "- `target_scaling`: Scaling options for targets (spectra)\n",
    "- `data_split`: Train/validation/test split ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for dataset: hpj\n",
      "Seq2seq generation: DISABLED\n",
      "3D Feature PCA components: 300 (for 984 features)\n",
      "\n",
      "Scaling Configuration:\n",
      "  3D Features:\n",
      "    - Scale continuous: False (method: standard)\n",
      "    - Log transform: False\n",
      "  Targets:\n",
      "    - Enabled: False (method: standard)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Stage 2A 3D Configuration\n",
    "STAGE2A_3D_CONFIG = {\n",
    "    # Dataset configuration (must match Stage 1)\n",
    "    'dataset_type': 'hpj',  # Must match the dataset processed in Stage 1\n",
    "    \n",
    "    # Paths configuration\n",
    "    'paths': {\n",
    "        'data_root': '../data',\n",
    "        'temp_dir': lambda dtype: f\"../data/tmp/{dtype}\",  # Input from Stage 1\n",
    "        'results_dir': lambda dtype: f\"../data/results/{dtype}\",\n",
    "        'output_subdir': \"full_featurised_3d\",\n",
    "        'output_dir': lambda dtype: f\"../data/results/{dtype}/full_featurised_3d\",\n",
    "    },\n",
    "    \n",
    "    # Processing configuration\n",
    "    'processing': {\n",
    "        'n_jobs': -1,  # Use all available cores\n",
    "        'save_format': 'jsonl',\n",
    "        'save_visualization_sample': True,  # Save sample for Stage 2B\n",
    "        'visualization_sample_size': 100,\n",
    "    },\n",
    "    \n",
    "    # Spectral processing configuration\n",
    "    'spectral': {\n",
    "        'max_peaks': 499,\n",
    "        'bin_size': 1.0,  # For regression only\n",
    "        'max_mz': 499,\n",
    "        'sort_peaks_by': 'intensity',\n",
    "        'intensity_distribution_bins': 20,\n",
    "        'peak_distribution_bins': 50,\n",
    "    },\n",
    "    \n",
    "    # Seq2seq configuration\n",
    "    'seq2seq': {\n",
    "        'enabled': False,  # Toggle to enable/disable seq2seq generation\n",
    "        'pca_components': 300,  # Increased for 984 3D features (preserve ~95% variance)\n",
    "        'output_subdir': 'seq2seq_featurised_3d',  # Parallel to 'full_featurised_3d'\n",
    "        'max_sequence_length': 499,  # Same as max_peaks\n",
    "        'include_regression': True,  # Also keep regression format\n",
    "        'use_binning': False  # No binning for seq2seq\n",
    "    },\n",
    "    \n",
    "    # Feature scaling configuration (for 3D molecular features)\n",
    "    'feature_scaling': {\n",
    "        # Continuous features scaling (all 3D features are continuous)\n",
    "        'scale_continuous': False,  # Whether to scale continuous features\n",
    "        'continuous_scaling_method': 'standard',  # Options: 'standard', 'minmax', 'robust', 'none'\n",
    "        'apply_log_transform': False,  # 3D features typically don't need log transform\n",
    "        \n",
    "        # Feature filtering\n",
    "        'handle_nan_strategy': 'drop_feature',  # Options: 'drop_feature', 'fill_zero', 'fill_mean'\n",
    "        'min_variance_threshold': 1e-88,\n",
    "        'auto_detect_binary': False,  # 3D features are continuous\n",
    "    },\n",
    "    \n",
    "    # Target scaling configuration (for spectra)\n",
    "    'target_scaling': {\n",
    "        'enabled': False,  # Whether to scale target spectra\n",
    "        'scaling_method': 'standard',  # Options: 'standard', 'minmax', 'robust', 'none'\n",
    "        'fit_on': 'train',  # Options: 'train', 'all'\n",
    "    },\n",
    "    \n",
    "    # Data splitting configuration\n",
    "    'data_split': {\n",
    "        'random_seed': 41,\n",
    "        'train_size': 0.8,\n",
    "        'val_size': 0.1,\n",
    "        'test_size': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded for dataset: {STAGE2A_3D_CONFIG['dataset_type']}\")\n",
    "print(f\"Seq2seq generation: {'ENABLED' if STAGE2A_3D_CONFIG['seq2seq']['enabled'] else 'DISABLED'}\")\n",
    "print(f\"3D Feature PCA components: {STAGE2A_3D_CONFIG['seq2seq']['pca_components']} (for 984 features)\")\n",
    "print(\"\\nScaling Configuration:\")\n",
    "print(f\"  3D Features:\")\n",
    "print(f\"    - Scale continuous: {STAGE2A_3D_CONFIG['feature_scaling']['scale_continuous']} (method: {STAGE2A_3D_CONFIG['feature_scaling']['continuous_scaling_method']})\")\n",
    "print(f\"    - Log transform: {STAGE2A_3D_CONFIG['feature_scaling']['apply_log_transform']}\")\n",
    "print(f\"  Targets:\")\n",
    "print(f\"    - Enabled: {STAGE2A_3D_CONFIG['target_scaling']['enabled']} (method: {STAGE2A_3D_CONFIG['target_scaling']['scaling_method']})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Stage 1 Output\n",
    "\n",
    "Check that Stage 1 3D feature extraction has been completed and all required files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 3D output validated successfully\n",
      "Dataset: hpj\n",
      "Number of molecules: 2720\n",
      "Feature dimension: 984 (expected: 984)\n",
      "Feature categories present: Basic 3D, AUTOCORR3D, RDF, MORSE, WHIM, GETAWAY, USR, USRCAT\n",
      "Stage 1 complete: True\n",
      "\n",
      "Top 5 important 3D features:\n",
      "  1. PMI1: 0.0917\n",
      "  2. USRCAT_31: 0.0547\n",
      "  3. USRCAT_33: 0.0495\n",
      "  4. USRCAT_24: 0.0424\n",
      "  5. USRCAT_27: 0.0397\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validate Stage 1 completion\n",
    "dataset_type = STAGE2A_3D_CONFIG['dataset_type']\n",
    "temp_dir = STAGE2A_3D_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "\n",
    "# Check required files\n",
    "required_files = [\n",
    "    'raw_spectral_data_3d.jsonl',\n",
    "    'molecular_features_3d.jsonl',\n",
    "    'feature_importance_3d.json',\n",
    "    'dataset_metadata_3d.json',\n",
    "    'dataset_config_3d.json'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(temp_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"ERROR: Stage 1 3D output incomplete for dataset '{dataset_type}'\")\n",
    "    print(f\"Missing files: {missing_files}\")\n",
    "    print(f\"Please run Stage 1 3D (01_3d_molecular_featurization.ipynb) first.\")\n",
    "else:\n",
    "    # Load dataset config from Stage 1\n",
    "    with open(os.path.join(temp_dir, 'dataset_config_3d.json'), 'r') as f:\n",
    "        stage1_config = json.load(f)\n",
    "    \n",
    "    # Load feature importance from Stage 1\n",
    "    with open(os.path.join(temp_dir, 'feature_importance_3d.json'), 'r') as f:\n",
    "        feature_importance_data = json.load(f)\n",
    "    \n",
    "    print(f\"Stage 1 3D output validated successfully\")\n",
    "    print(f\"Dataset: {stage1_config['dataset_type']}\")\n",
    "    print(f\"Number of molecules: {stage1_config['num_molecules']}\")\n",
    "    print(f\"Feature dimension: {stage1_config['feature_dimension']} (expected: 984)\")\n",
    "    if 'feature_names' in stage1_config:\n",
    "        print(f\"Feature categories present: Basic 3D, AUTOCORR3D, RDF, MORSE, WHIM, GETAWAY, USR, USRCAT\")\n",
    "    print(f\"Stage 1 complete: {stage1_config.get('stage1_complete', False)}\")\n",
    "    print(f\"\\nTop 5 important 3D features:\")\n",
    "    for i, feat in enumerate(feature_importance_data['top_10_features'][:5]):\n",
    "        print(f\"  {i+1}. {feat['name']}: {feat['importance']:.4f}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Directory Setup\n",
    "\n",
    "Create output directories for ML-ready datasets with 3D features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:16:59,889 - INFO - Created directory: ../data/results/hpj\n",
      "2025-06-23 10:16:59,891 - INFO - Created directory: ../data/results/hpj/full_featurised_3d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up output directories...\n",
      "Directory setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_directories(dataset_type):\n",
    "    \"\"\"Create all necessary directories for Stage 2A 3D.\"\"\"\n",
    "    dirs_to_create = [\n",
    "        STAGE2A_3D_CONFIG['paths']['results_dir'](dataset_type),\n",
    "        STAGE2A_3D_CONFIG['paths']['output_dir'](dataset_type)\n",
    "    ]\n",
    "    \n",
    "    # Add seq2seq directory if enabled\n",
    "    if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "        seq2seq_dir = os.path.join(\n",
    "            STAGE2A_3D_CONFIG['paths']['results_dir'](dataset_type),\n",
    "            STAGE2A_3D_CONFIG['seq2seq']['output_subdir']\n",
    "        )\n",
    "        dirs_to_create.append(seq2seq_dir)\n",
    "    \n",
    "    for dir_path in dirs_to_create:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        logger.info(f\"Created directory: {dir_path}\")\n",
    "\n",
    "# Setup directories\n",
    "print(\"Setting up output directories...\")\n",
    "setup_directories(STAGE2A_3D_CONFIG['dataset_type'])\n",
    "print(\"Directory setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Functions and Base Classes\n",
    "\n",
    "Core utilities needed for Stage 2A processing with comprehensive 3D features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions and classes loaded\n",
      "Total 3D features defined: 984\n",
      "Initial memory usage: 1421.4 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Utilities ---------------------- #\n",
    "class Utilities:\n",
    "    @staticmethod\n",
    "    def get_memory_usage():\n",
    "        \"\"\"Get current memory usage in MB.\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        return f\"{mem_info.rss / (1024 * 1024):.1f} MB (RSS), {process.memory_percent():.1f}% of total\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_numpy_array(data):\n",
    "        \"\"\"Ensure data is a numpy array.\"\"\"\n",
    "        if isinstance(data, list):\n",
    "            return np.array(data)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_np_to_list(item):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist()\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: Utilities.convert_np_to_list(v) for k, v in item.items()}\n",
    "        elif isinstance(item, list):\n",
    "            return [Utilities.convert_np_to_list(v) for v in item]\n",
    "        else:\n",
    "            return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_scaler(method):\n",
    "        \"\"\"Get sklearn scaler based on method name.\"\"\"\n",
    "        scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'minmax': MinMaxScaler(),\n",
    "            'robust': RobustScaler(),\n",
    "            'none': None\n",
    "        }\n",
    "        return scalers.get(method, StandardScaler())\n",
    "\n",
    "# ---------------------- Error Handling Mixin ---------------------- #\n",
    "class ErrorHandlingMixin:\n",
    "    \"\"\"Provides standardized error handling for pipeline components.\"\"\"\n",
    "    \n",
    "    def handle_error(self, error, context=\"\", data=None):\n",
    "        \"\"\"Centralized error handling.\"\"\"\n",
    "        message = f\"Error in {self.__class__.__name__}\"\n",
    "        if context:\n",
    "            message += f\" ({context})\"\n",
    "        message += f\": {error}\"\n",
    "        \n",
    "        logger.error(message)\n",
    "        return None\n",
    "\n",
    "# ---------------------- 3D Feature Metadata ---------------------- #\n",
    "@dataclass\n",
    "class Feature3DMetadata:\n",
    "    feature_names: List[str]\n",
    "    feature_types: List[str]  # Basic3D, AUTOCORR3D, RDF, MORSE, WHIM, GETAWAY, USR, USRCAT\n",
    "    feature_importance: dict\n",
    "    segment_lengths: List[int]\n",
    "    valid_mask: Optional[np.ndarray] = field(default=None)\n",
    "\n",
    "    def total_features(self):\n",
    "        return sum(self.segment_lengths)\n",
    "\n",
    "# ---------------------- Feature Name Generation ---------------------- #\n",
    "def get_feature_names():\n",
    "    \"\"\"Get names for all 984 3D features in the correct order.\"\"\"\n",
    "    names = []\n",
    "    \n",
    "    # Basic 3D descriptors (11 features)\n",
    "    names.extend(['PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2', 'Asphericity',\n",
    "                 'Eccentricity', 'InertialShapeFactor', 'SpherocityIndex',\n",
    "                 'RadiusOfGyration', 'PBF'])\n",
    "    \n",
    "    # AUTOCORR3D (80 features)\n",
    "    names.extend([f'AUTOCORR3D_{i}' for i in range(80)])\n",
    "    \n",
    "    # RDF (210 features)\n",
    "    names.extend([f'RDF_{i}' for i in range(210)])\n",
    "    \n",
    "    # MORSE (224 features)\n",
    "    names.extend([f'MORSE_{i}' for i in range(224)])\n",
    "    \n",
    "    # WHIM (114 features)\n",
    "    names.extend([f'WHIM_{i}' for i in range(114)])\n",
    "    \n",
    "    # GETAWAY (273 features)\n",
    "    names.extend([f'GETAWAY_{i}' for i in range(273)])\n",
    "    \n",
    "    # USR (12 features)\n",
    "    names.extend([f'USR_{i}' for i in range(12)])\n",
    "    \n",
    "    # USRCAT (60 features)\n",
    "    names.extend([f'USRCAT_{i}' for i in range(60)])\n",
    "    \n",
    "    return names\n",
    "\n",
    "print(\"Utility functions and classes loaded\")\n",
    "print(f\"Total 3D features defined: {len(get_feature_names())}\")\n",
    "print(f\"Initial memory usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading Components\n",
    "\n",
    "Load 3D features and spectral data from Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading components initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DataLoader ---------------------- #\n",
    "class DataLoader(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "    \n",
    "    def load_data_from_jsonl(self, file_path):\n",
    "        \"\"\"Load data from JSONL file.\"\"\"\n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        data = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in tqdm(f, desc=f\"Loading {os.path.basename(file_path)}\"):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    smiles = record.get(\"smiles\")\n",
    "                    \n",
    "                    # Handle different data formats\n",
    "                    if \"data\" in record:\n",
    "                        data_content = record.get(\"data\", {})\n",
    "                    elif \"features_3d\" in record:\n",
    "                        # 3D features format\n",
    "                        data_content = {\n",
    "                            'features_3d': record.get('features_3d'),\n",
    "                            'feature_names': record.get('feature_names', [])\n",
    "                        }\n",
    "                    elif \"peaks\" in record:\n",
    "                        # Raw spectral data format\n",
    "                        data_content = record.get(\"peaks\", [])\n",
    "                    else:\n",
    "                        data_content = record\n",
    "                    \n",
    "                    data[smiles] = data_content\n",
    "                except Exception as e:\n",
    "                    self.handle_error(e, f\"loading JSON line from {file_path}\")\n",
    "        logger.info(f\"Loaded {len(data)} records from {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    def convert_raw_data_to_df(self, raw_data):\n",
    "        \"\"\"Convert raw data records to pandas DataFrames.\"\"\"\n",
    "        df_data = {}\n",
    "        for smiles, peaks in raw_data.items():\n",
    "            df_data[smiles] = pd.DataFrame(peaks, columns=[\"mz\", \"intensity\"])\n",
    "        return df_data\n",
    "\n",
    "print(\"Data loading components initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spectral Processing Components\n",
    "\n",
    "Process mass spectra with binning for regression and full resolution for seq2seq (same as 2D)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral processing components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- SpectralProcessor ---------------------- #\n",
    "class SpectralProcessor(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.max_observed_mz = 0\n",
    "        \n",
    "    def process(self, raw_spectra, use_binning=True):\n",
    "        \"\"\"\n",
    "        Process spectral data with optional binning.\n",
    "        \n",
    "        Args:\n",
    "            raw_spectra: Dictionary of SMILES -> peak DataFrame\n",
    "            use_binning: Whether to bin the spectra (True for regression, False for seq2seq)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing spectral sequences (binning={'enabled' if use_binning else 'disabled'})...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=self.config['processing']['n_jobs'])(\n",
    "            delayed(self._process_one_spectrum)(smi, comp, use_binning) \n",
    "            for smi, comp in tqdm(list(raw_spectra.items()), desc=\"Processing spectra\")\n",
    "        )\n",
    "        \n",
    "        # Memory-optimized: directly build dictionary\n",
    "        processed_sequences = {}\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                smi, proc = result\n",
    "                processed_sequences[smi] = proc\n",
    "        \n",
    "        # Calculate the maximum observed m/z value across all spectra\n",
    "        self.max_observed_mz = max([data.get('true_max_mz', 0) for data in processed_sequences.values()], default=0)\n",
    "        logger.info(f\"Maximum observed m/z across all spectra: {self.max_observed_mz}\")\n",
    "        \n",
    "        if use_binning:\n",
    "            logger.info(f\"Configured max_mz for binning: {self.config['spectral']['max_mz']}\")\n",
    "            \n",
    "            # Find what percentage of spectra have peaks beyond the max_mz\n",
    "            spectra_with_peaks_beyond_max = sum(1 for data in processed_sequences.values() \n",
    "                                                if data.get('true_max_mz', 0) > self.config['spectral']['max_mz'])\n",
    "            if processed_sequences:\n",
    "                percentage_beyond = (spectra_with_peaks_beyond_max / len(processed_sequences)) * 100\n",
    "                logger.info(f\"{spectra_with_peaks_beyond_max} spectra ({percentage_beyond:.2f}%) have peaks beyond configured max_mz\")\n",
    "        \n",
    "        logger.info(f\"Processed spectral sequences for {len(processed_sequences)} compounds\")\n",
    "        return processed_sequences\n",
    "    \n",
    "    def _process_one_spectrum(self, smiles, compound_data, use_binning):\n",
    "        \"\"\"Process a single spectrum with optional binning.\"\"\"\n",
    "        data = compound_data.copy()\n",
    "        try:\n",
    "            # Initialize required result variables\n",
    "            peak_array = np.zeros((self.config['spectral']['max_peaks'], 2), dtype=np.float32)\n",
    "            attention_mask = np.zeros(self.config['spectral']['max_peaks'], dtype=np.float32)\n",
    "            original_peak_count = 0\n",
    "            peak_distribution = np.zeros(self.config['spectral']['intensity_distribution_bins'], dtype=np.float32)\n",
    "            max_mz_value = 0\n",
    "            \n",
    "            # Capture the true maximum m/z value before any filtering\n",
    "            true_max_mz = data['mz'].max() if not data.empty else 0\n",
    "            \n",
    "            # Apply binning only if requested (for regression)\n",
    "            if use_binning and self.config['spectral']['bin_size'] and not data.empty:\n",
    "                data['binned_mz'] = (data['mz'] / self.config['spectral']['bin_size']).astype(int) * self.config['spectral']['bin_size']\n",
    "                data = data.groupby('binned_mz').agg({'intensity': 'max'}).reset_index().rename(columns={'binned_mz': 'mz'})\n",
    "            \n",
    "            # Count peaks beyond max_mz (only relevant for binned data)\n",
    "            peaks_beyond_max = 0\n",
    "            if use_binning:\n",
    "                peaks_beyond_max = sum(1 for mz in data['mz'] if mz > self.config['spectral']['max_mz']) if not data.empty else 0\n",
    "            \n",
    "            # Process peaks if data is not empty\n",
    "            if not data.empty:\n",
    "                sorted_peaks = data.sort_values(self.config['spectral']['sort_peaks_by'], ascending=False)\n",
    "                max_intensity = sorted_peaks['intensity'].max() if not sorted_peaks.empty else 0\n",
    "                if max_intensity > 0:\n",
    "                    sorted_peaks['intensity'] /= max_intensity\n",
    "                \n",
    "                peak_sequence = list(zip(sorted_peaks['mz'], sorted_peaks['intensity']))\n",
    "                original_peak_count = len(peak_sequence)\n",
    "                \n",
    "                if len(peak_sequence) > 0:\n",
    "                    # Process peak sequences\n",
    "                    if len(peak_sequence) > self.config['spectral']['max_peaks']:\n",
    "                        attention_mask = np.ones(self.config['spectral']['max_peaks'])\n",
    "                        peak_sequence = peak_sequence[:self.config['spectral']['max_peaks']]\n",
    "                    else:\n",
    "                        attention_mask = np.concatenate([\n",
    "                            np.ones(len(peak_sequence)), \n",
    "                            np.zeros(self.config['spectral']['max_peaks'] - len(peak_sequence))\n",
    "                        ])\n",
    "                        peak_sequence += [(0.0, 0.0)] * (self.config['spectral']['max_peaks'] - len(peak_sequence))\n",
    "                    \n",
    "                    peak_array = np.array(peak_sequence, dtype=np.float32)\n",
    "                    peak_distribution = self._create_intensity_distribution(\n",
    "                        peak_array, attention_mask, self.config['spectral']['intensity_distribution_bins']\n",
    "                    )\n",
    "                    \n",
    "                    max_mz_value = sorted_peaks['mz'].max() if not sorted_peaks.empty else 0\n",
    "            \n",
    "            # Free up memory\n",
    "            del data\n",
    "            \n",
    "            result = {\n",
    "                'peaks': peak_array,\n",
    "                'attention_mask': attention_mask,\n",
    "                'original_peak_count': original_peak_count,\n",
    "                'intensity_distribution': peak_distribution,\n",
    "                'max_mz': max_mz_value,\n",
    "                'true_max_mz': true_max_mz,\n",
    "                'binned': use_binning\n",
    "            }\n",
    "            \n",
    "            if use_binning:\n",
    "                result.update({\n",
    "                    'peaks_beyond_max': peaks_beyond_max,\n",
    "                    'bin_size': self.config['spectral']['bin_size']\n",
    "                })\n",
    "            \n",
    "            return smiles, result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self.handle_error(e, f\"processing spectrum for {smiles}\", data={\"smiles\": smiles})\n",
    "    \n",
    "    def _create_intensity_distribution(self, peak_array, mask, num_bins):\n",
    "        \"\"\"Calculate intensity distribution histogram.\"\"\"\n",
    "        valid_peaks = peak_array[mask.astype(bool)]\n",
    "        if len(valid_peaks) == 0:\n",
    "            return np.zeros(num_bins)\n",
    "        hist, _ = np.histogram(valid_peaks[:, 1], bins=num_bins, range=(0, 1))\n",
    "        return hist / np.sum(hist) if np.sum(hist) > 0 else hist\n",
    "\n",
    "print(\"Spectral processing components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Combination and Binning Components with 3D Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination and binning components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DataCombiner3D ---------------------- #\n",
    "class DataCombiner3D(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def process(self, spectra_sequences, molecular_features_3d):\n",
    "        \"\"\"Combine spectral and 3D molecular data.\"\"\"\n",
    "        combined_data = {}\n",
    "        missing_features = 0\n",
    "        \n",
    "        for smiles, spectral_data in tqdm(spectra_sequences.items(), desc=\"Combining data\"):\n",
    "            if smiles in molecular_features_3d:\n",
    "                combined_data[smiles] = self._combine_single_molecule(smiles, spectral_data, molecular_features_3d[smiles])\n",
    "            else:\n",
    "                missing_features += 1\n",
    "                \n",
    "        logger.info(f\"Combined data: {len(combined_data)} compounds, missing features for {missing_features}\")\n",
    "        return combined_data\n",
    "    \n",
    "    def _combine_single_molecule(self, smiles, spectral_data, features_3d):\n",
    "        \"\"\"Combine data for a single molecule with 3D features.\"\"\"\n",
    "        # Get molecular weight and exact mass from the molecule\n",
    "        from rdkit import Chem\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        molecular_weight = 0\n",
    "        exact_mass = 0\n",
    "        \n",
    "        if mol:\n",
    "            from rdkit.Chem import Descriptors\n",
    "            molecular_weight = Descriptors.MolWt(mol)\n",
    "            exact_mass = Descriptors.ExactMolWt(mol)\n",
    "                \n",
    "        # Combine all data\n",
    "        combined = {}\n",
    "        # First add spectral data\n",
    "        for key, value in spectral_data.items():\n",
    "            combined[key] = value\n",
    "        # Then add 3D features\n",
    "        combined['features_3d'] = features_3d.get('features_3d', [])\n",
    "        combined['feature_names'] = features_3d.get('feature_names', [])\n",
    "        combined['molecular_weight'] = molecular_weight\n",
    "        combined['exact_mass'] = exact_mass\n",
    "        \n",
    "        return combined\n",
    "\n",
    "# ---------------------- BinningProcessor ---------------------- #\n",
    "class BinningProcessor(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.max_observed_mz = 0\n",
    "        \n",
    "    def process(self, combined_data):\n",
    "        \"\"\"Create binned representations for regression.\"\"\"\n",
    "        # First find the maximum observed m/z value across all spectra\n",
    "        self.max_observed_mz = max([data.get('true_max_mz', 0) for data in combined_data.values()], default=0)\n",
    "        logger.info(f\"Maximum observed m/z for binning: {self.max_observed_mz}\")\n",
    "        \n",
    "        results = Parallel(n_jobs=self.config['processing']['n_jobs'])(\n",
    "            delayed(self._process_binning)(smi, data) \n",
    "            for smi, data in tqdm(list(combined_data.items()), desc=\"Binning spectra\")\n",
    "        )\n",
    "        \n",
    "        # Memory-optimized dictionary construction\n",
    "        binned_data = {}\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                smi, proc = result\n",
    "                binned_data[smi] = proc\n",
    "                \n",
    "        logger.info(f\"Created binned representations for {len(binned_data)} compounds\")\n",
    "        logger.info(f\"Memory usage after binning: {Utilities.get_memory_usage()}\")\n",
    "        \n",
    "        return binned_data\n",
    "    \n",
    "    def _process_binning(self, smiles, data):\n",
    "        \"\"\"Process binning for a single molecule.\"\"\"\n",
    "        peaks = Utilities.ensure_numpy_array(data['peaks'])\n",
    "        mask = Utilities.ensure_numpy_array(data['attention_mask'])\n",
    "        real_peaks = peaks[mask.astype(bool)]\n",
    "        \n",
    "        # Create binned spectrum up to configured max_mz\n",
    "        configured_num_bins = int(self.config['spectral']['max_mz'] / self.config['spectral']['bin_size']) + 1\n",
    "        binned_spectrum = np.zeros(configured_num_bins)\n",
    "        \n",
    "        # Create extended binned spectrum up to observed max_mz (for visualization)\n",
    "        extended_num_bins = int(max(self.max_observed_mz * 1.1, self.config['spectral']['max_mz']) / \n",
    "                              self.config['spectral']['bin_size']) + 1\n",
    "        extended_binned_spectrum = np.zeros(extended_num_bins)\n",
    "        \n",
    "        for mz, intensity in real_peaks:\n",
    "            # For the standard binned spectrum (used in the pipeline)\n",
    "            if mz < self.config['spectral']['max_mz']:\n",
    "                bin_idx = min(int(mz / self.config['spectral']['bin_size']), configured_num_bins - 1)\n",
    "                binned_spectrum[bin_idx] = max(binned_spectrum[bin_idx], intensity)\n",
    "            \n",
    "            # For the extended binned spectrum (used in visualization)\n",
    "            extended_bin_idx = min(int(mz / self.config['spectral']['bin_size']), extended_num_bins - 1)\n",
    "            extended_binned_spectrum[extended_bin_idx] = max(extended_binned_spectrum[extended_bin_idx], intensity)\n",
    "        \n",
    "        # Count peaks and intensity beyond max_mz\n",
    "        peaks_beyond_max = [(mz, intensity) for mz, intensity in real_peaks if mz > self.config['spectral']['max_mz']]\n",
    "        total_intensity_beyond_max = sum(intensity for _, intensity in peaks_beyond_max)\n",
    "        total_intensity = sum(intensity for _, intensity in real_peaks)\n",
    "        intensity_percentage_beyond_max = (total_intensity_beyond_max / total_intensity * 100) if total_intensity > 0 else 0\n",
    "        \n",
    "        # Create a new data dictionary\n",
    "        updated = {}\n",
    "        # Copy original data that's needed\n",
    "        for key in ['peaks', 'attention_mask', 'original_peak_count', \n",
    "                   'intensity_distribution', 'max_mz', 'true_max_mz', \n",
    "                   'peaks_beyond_max', 'bin_size', \n",
    "                   'molecular_weight', 'exact_mass']:\n",
    "            if key in data:\n",
    "                updated[key] = data[key]\n",
    "                \n",
    "        # Add new binning data\n",
    "        updated['binned_spectrum'] = binned_spectrum\n",
    "        updated['extended_binned_spectrum'] = extended_binned_spectrum\n",
    "        updated['peaks_beyond_max_count'] = len(peaks_beyond_max)\n",
    "        updated['intensity_beyond_max'] = total_intensity_beyond_max\n",
    "        updated['intensity_percentage_beyond_max'] = intensity_percentage_beyond_max\n",
    "        updated['extended_max_mz'] = extended_num_bins * self.config['spectral']['bin_size']\n",
    "        \n",
    "        # Keep 3D molecular feature data\n",
    "        updated['features_3d'] = data.get('features_3d', [])\n",
    "        updated['feature_names'] = data.get('feature_names', [])\n",
    "        \n",
    "        return smiles, updated\n",
    "\n",
    "print(\"Data combination and binning components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Preparation Components for 3D Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation components for 3D features loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- RegressionDataPreparer3D ---------------------- #\n",
    "class RegressionDataPreparer3D(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type, feature_importance_data):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.feature_importance_data = feature_importance_data\n",
    "        self.target_scaler = None\n",
    "    \n",
    "    def process(self, data_dict):\n",
    "        \"\"\"Extract 3D features and targets for regression.\"\"\"\n",
    "        logger.info(\"Extracting 3D features and targets for regression...\")\n",
    "        \n",
    "        # Get feature metadata from first sample\n",
    "        first_key = next(iter(data_dict.keys()), None)\n",
    "        feature_metadata = None\n",
    "        if first_key:\n",
    "            feature_metadata = self._extract_3d_feature_info(data_dict[first_key])\n",
    "        \n",
    "        results = []\n",
    "        for smi, data in tqdm(list(data_dict.items()), desc=\"Preparing 3D regression data\"):\n",
    "            result = self._process_regression_sample(smi, data)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "        \n",
    "        if not results:\n",
    "            logger.error(\"No valid regression samples found!\")\n",
    "            return None\n",
    "        \n",
    "        # Unpack results in a memory-efficient way\n",
    "        smiles_list = []\n",
    "        feature_list = []\n",
    "        target_list = []\n",
    "        for result in results:\n",
    "            smiles, features, target = result\n",
    "            smiles_list.append(smiles)\n",
    "            feature_list.append(features)\n",
    "            target_list.append(target)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(feature_list)\n",
    "        y = np.array(target_list)\n",
    "        \n",
    "        # Clear original results list to free memory\n",
    "        del results\n",
    "        del feature_list\n",
    "        del target_list\n",
    "        \n",
    "        logger.info(f\"Memory usage after 3D regression data preparation: {Utilities.get_memory_usage()}\")\n",
    "        logger.info(f\"Prepared 3D regression matrices - X: {X.shape}, y: {y.shape}\")\n",
    "        \n",
    "        return X, y, smiles_list, feature_metadata\n",
    "    \n",
    "    def scale_targets(self, y_train, y_val=None, y_test=None):\n",
    "        \"\"\"Scale target spectra if enabled.\"\"\"\n",
    "        if not self.config['target_scaling']['enabled']:\n",
    "            logger.info(\"Target scaling disabled, returning original targets\")\n",
    "            return y_train, y_val, y_test\n",
    "        \n",
    "        logger.info(f\"Scaling targets using method: {self.config['target_scaling']['scaling_method']}\")\n",
    "        \n",
    "        # Get the appropriate scaler\n",
    "        self.target_scaler = Utilities.get_scaler(self.config['target_scaling']['scaling_method'])\n",
    "        \n",
    "        if self.target_scaler is None:\n",
    "            return y_train, y_val, y_test\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train)\n",
    "        \n",
    "        # Transform validation and test if provided\n",
    "        y_val_scaled = self.target_scaler.transform(y_val) if y_val is not None else None\n",
    "        y_test_scaled = self.target_scaler.transform(y_test) if y_test is not None else None\n",
    "        \n",
    "        return y_train_scaled, y_val_scaled, y_test_scaled\n",
    "    \n",
    "    def _extract_3d_feature_info(self, data) -> Feature3DMetadata:\n",
    "        \"\"\"Extract 3D feature metadata for 984 features.\"\"\"\n",
    "        feature_names = data.get('feature_names', [])\n",
    "        if not feature_names or len(feature_names) != 984:\n",
    "            # Use default names from get_feature_names function\n",
    "            feature_names = get_feature_names()\n",
    "        \n",
    "        # Categorize features\n",
    "        feature_types = []\n",
    "        for name in feature_names:\n",
    "            if name in ['PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2', 'Asphericity', \n",
    "                       'Eccentricity', 'InertialShapeFactor', 'SpherocityIndex', \n",
    "                       'RadiusOfGyration', 'PBF']:\n",
    "                feature_types.append('Basic3D')\n",
    "            elif 'AUTOCORR3D' in name:\n",
    "                feature_types.append('AUTOCORR3D')\n",
    "            elif 'RDF' in name:\n",
    "                feature_types.append('RDF')\n",
    "            elif 'MORSE' in name:\n",
    "                feature_types.append('MORSE')\n",
    "            elif 'WHIM' in name:\n",
    "                feature_types.append('WHIM')\n",
    "            elif 'GETAWAY' in name:\n",
    "                feature_types.append('GETAWAY')\n",
    "            elif 'USR' in name and 'USRCAT' not in name:\n",
    "                feature_types.append('USR')\n",
    "            elif 'USRCAT' in name:\n",
    "                feature_types.append('USRCAT')\n",
    "            else:\n",
    "                feature_types.append('Unknown')\n",
    "        \n",
    "        # Define segment lengths for each feature type\n",
    "        segment_lengths = [11, 80, 210, 224, 114, 273, 12, 60]  # Total: 984\n",
    "        \n",
    "        return Feature3DMetadata(\n",
    "            feature_names=feature_names,\n",
    "            feature_types=feature_types,\n",
    "            feature_importance=self.feature_importance_data,\n",
    "            segment_lengths=segment_lengths\n",
    "        )\n",
    "    \n",
    "    def _process_regression_sample(self, smiles, data):\n",
    "        \"\"\"Process a single regression sample with 3D features.\"\"\"\n",
    "        features_3d = data.get('features_3d', [])\n",
    "        binned_spectrum = data.get('binned_spectrum', [])\n",
    "        \n",
    "        if not features_3d or not isinstance(binned_spectrum, (list, np.ndarray)):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            features_3d = Utilities.ensure_numpy_array(features_3d)\n",
    "            binned_spectrum = Utilities.ensure_numpy_array(binned_spectrum)\n",
    "            return (smiles, features_3d, binned_spectrum)\n",
    "        except Exception as e:\n",
    "            return self.handle_error(e, f\"processing 3D features for {smiles}\", data={\"smiles\": smiles})\n",
    "\n",
    "# ---------------------- Seq2SeqDataPreparer3D ---------------------- #\n",
    "class Seq2SeqDataPreparer3D(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.pca = None\n",
    "        \n",
    "    def process(self, X_scaled, combined_data, smiles_list):\n",
    "        \"\"\"\n",
    "        Convert 3D features to seq2seq format.\n",
    "        \n",
    "        Args:\n",
    "            X_scaled: Scaled 3D feature matrix from regression preprocessing\n",
    "            combined_data: Dictionary with full resolution spectral data\n",
    "            smiles_list: List of SMILES strings in same order as X_scaled\n",
    "            \n",
    "        Returns:\n",
    "            List of seq2seq data dictionaries\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing seq2seq data with comprehensive 3D features...\")\n",
    "        \n",
    "        # Apply PCA to reduce features (more aggressive for 984 features)\n",
    "        n_components = min(self.config['seq2seq']['pca_components'], X_scaled.shape[1])\n",
    "        logger.info(f\"Applying PCA to reduce 3D features from {X_scaled.shape[1]} to {n_components} dimensions\")\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Log explained variance\n",
    "        explained_var = np.sum(self.pca.explained_variance_ratio_)\n",
    "        logger.info(f\"PCA explained variance: {explained_var:.2%}\")\n",
    "        \n",
    "        # Log cumulative explained variance\n",
    "        cumsum_var = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        for threshold in [0.8, 0.9, 0.95, 0.99]:\n",
    "            n_comp_threshold = np.argmax(cumsum_var >= threshold) + 1\n",
    "            logger.info(f\"  Components needed for {threshold*100:.0f}% variance: {n_comp_threshold}\")\n",
    "        \n",
    "        # Extract peak sequences from combined_data\n",
    "        seq2seq_data = []\n",
    "        \n",
    "        for i, smiles in enumerate(tqdm(smiles_list, desc=\"Creating seq2seq samples\")):\n",
    "            if smiles in combined_data:\n",
    "                data = combined_data[smiles]\n",
    "                \n",
    "                # Get full resolution peaks (not binned)\n",
    "                peaks = data['peaks']  # Already (mz, intensity) pairs\n",
    "                mask = data['attention_mask']\n",
    "                \n",
    "                seq2seq_data.append({\n",
    "                    'smiles': smiles,\n",
    "                    'input_sequence': X_pca[i],  # PCA-reduced 3D features\n",
    "                    'output_sequence': peaks,     # Full resolution (mz, intensity) pairs\n",
    "                    'output_mask': mask,          # Attention mask\n",
    "                    'original_peak_count': data['original_peak_count'],\n",
    "                    'molecular_weight': data.get('molecular_weight', 0),\n",
    "                    'exact_mass': data.get('exact_mass', 0)\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Created {len(seq2seq_data)} seq2seq samples with 3D features\")\n",
    "        return seq2seq_data\n",
    "    \n",
    "    def save_pca(self, filepath):\n",
    "        \"\"\"Save PCA transformer for inference.\"\"\"\n",
    "        if self.pca is not None:\n",
    "            dump(self.pca, filepath)\n",
    "            logger.info(f\"Saved PCA transformer to {filepath}\")\n",
    "        else:\n",
    "            logger.warning(\"No PCA transformer to save\")\n",
    "\n",
    "print(\"Data preparation components for 3D features loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Preprocessing for 3D Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preprocessor for 3D features loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- FeaturePreprocessor3D ---------------------- #\n",
    "class FeaturePreprocessor3D(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.valid_feature_mask = None\n",
    "        self.scaler = None\n",
    "        self.nan_mask = None\n",
    "    \n",
    "    def _sign_preserving_log_transform(self, X):\n",
    "        \"\"\"Apply sign-preserving log transform: sign(x)*log1p(|x|).\"\"\"\n",
    "        return np.sign(X) * np.log1p(np.abs(X))\n",
    "    \n",
    "    def process(self, X, fit=True, metadata: Feature3DMetadata = None):\n",
    "        \"\"\"Preprocess 3D features with scaling and filtering.\"\"\"\n",
    "        if X is None or len(X) == 0:\n",
    "            logger.warning(\"Empty feature matrix, skipping preprocessing\")\n",
    "            return None\n",
    "            \n",
    "        X = np.array(X) if not isinstance(X, np.ndarray) else X\n",
    "        \n",
    "        if fit:\n",
    "            return self._fit_transform(X, metadata)\n",
    "        else:\n",
    "            return self._transform(X, metadata)\n",
    "    \n",
    "    def _fit_transform(self, X, metadata: Feature3DMetadata = None):\n",
    "        \"\"\"Fit preprocessor and transform 3D features.\"\"\"\n",
    "        # Handle NaN values\n",
    "        X = self._handle_nans(X)\n",
    "        \n",
    "        # Apply log transform if enabled (all 3D features are continuous)\n",
    "        X = X.copy()\n",
    "        if self.config['feature_scaling'].get('apply_log_transform', False):\n",
    "            X = self._sign_preserving_log_transform(X)\n",
    "            logger.info(\"Applied log transform to 3D features\")\n",
    "        \n",
    "        # Create valid feature mask\n",
    "        self._create_valid_feature_mask(X, metadata)\n",
    "        \n",
    "        # Apply mask\n",
    "        X_valid = X[:, self.valid_feature_mask]\n",
    "        \n",
    "        # Scale features (3D features are all continuous)\n",
    "        X_scaled = self._scale_features(X_valid, fit=True)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def _transform(self, X, metadata: Feature3DMetadata = None):\n",
    "        \"\"\"Transform features using fitted preprocessor.\"\"\"\n",
    "        if self.valid_feature_mask is None:\n",
    "            logger.error(\"FeaturePreprocessor not fitted before transform\")\n",
    "            return X\n",
    "            \n",
    "        X = self._handle_nan_values(X)\n",
    "        \n",
    "        # Apply log transform if enabled\n",
    "        if self.config['feature_scaling'].get('apply_log_transform', False):\n",
    "            X = self._sign_preserving_log_transform(X)\n",
    "        \n",
    "        X_valid = X[:, self.valid_feature_mask]\n",
    "        X_scaled = self._scale_features(X_valid, fit=False)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def _handle_nans(self, X):\n",
    "        \"\"\"Check for and handle NaN and infinity values.\"\"\"\n",
    "        # Check for NaN values\n",
    "        nan_counts = np.isnan(X).sum(axis=0)\n",
    "        # Check for infinity values\n",
    "        inf_counts = np.isinf(X).sum(axis=0)\n",
    "        \n",
    "        total_invalid = nan_counts + inf_counts\n",
    "        if total_invalid.sum() > 0:\n",
    "            logger.info(f\"Found {nan_counts.sum()} NaN values and {inf_counts.sum()} infinity values across {(total_invalid > 0).sum()} features\")\n",
    "        \n",
    "        # Create mask for valid values (no NaN or inf)\n",
    "        self.nan_mask = total_invalid == 0\n",
    "        \n",
    "        # Handle invalid values\n",
    "        X = self._handle_nan_values(X)\n",
    "        return X\n",
    "    \n",
    "    def _handle_nan_values(self, X):\n",
    "        \"\"\"Handle both NaN and infinity values based on strategy.\"\"\"\n",
    "        strategy = self.config['feature_scaling'].get('handle_nan_strategy', 'drop_feature')\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        if strategy == 'fill_zero':\n",
    "            # Handle both NaN and infinity\n",
    "            X_copy = np.nan_to_num(X_copy, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        elif strategy == 'fill_mean':\n",
    "            # Calculate means ignoring both NaN and infinity\n",
    "            finite_mask = np.isfinite(X_copy)\n",
    "            col_means = np.zeros(X_copy.shape[1])\n",
    "            \n",
    "            for col_idx in range(X_copy.shape[1]):\n",
    "                col_data = X_copy[:, col_idx]\n",
    "                finite_col_data = col_data[finite_mask[:, col_idx]]\n",
    "                if len(finite_col_data) > 0:\n",
    "                    col_means[col_idx] = np.mean(finite_col_data)\n",
    "            \n",
    "            # Replace both NaN and infinity with means\n",
    "            non_finite_mask = ~np.isfinite(X_copy)\n",
    "            for col_idx in range(X_copy.shape[1]):\n",
    "                X_copy[non_finite_mask[:, col_idx], col_idx] = col_means[col_idx]\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "    def _create_valid_feature_mask(self, X, metadata: Feature3DMetadata = None):\n",
    "        \"\"\"Create mask for valid 3D features.\"\"\"\n",
    "        valid_mask = np.ones(X.shape[1], dtype=bool)\n",
    "        strategy = self.config['feature_scaling'].get('handle_nan_strategy', 'drop_feature')\n",
    "        \n",
    "        if strategy == 'drop_feature' and self.nan_mask is not None:\n",
    "            valid_mask = valid_mask & self.nan_mask\n",
    "            nan_count = np.sum(np.isnan(X).any(axis=0))\n",
    "            inf_count = np.sum(np.isinf(X).any(axis=0))\n",
    "            logger.info(f\"Dropped {np.sum(~self.nan_mask)} 3D features with invalid values ({nan_count} with NaN, {inf_count} with infinity)\")\n",
    "        \n",
    "        # Check variance threshold\n",
    "        variance_threshold = self.config['feature_scaling'].get('min_variance_threshold', 1e-8)\n",
    "        if variance_threshold > 0:\n",
    "            X_for_var = X.copy()\n",
    "            X_for_var[np.isinf(X_for_var)] = np.nan\n",
    "            variances = np.nanvar(X_for_var, axis=0)\n",
    "            variances = np.nan_to_num(variances, nan=0.0)\n",
    "            \n",
    "            high_variance_mask = variances > variance_threshold\n",
    "            valid_mask = valid_mask & high_variance_mask\n",
    "            logger.info(f\"Dropped {np.sum(~high_variance_mask)} 3D features with variance below {variance_threshold}\")\n",
    "            \n",
    "            del X_for_var\n",
    "            del variances\n",
    "        \n",
    "        self.valid_feature_mask = valid_mask\n",
    "        if metadata is not None:\n",
    "            metadata.valid_mask = valid_mask\n",
    "        \n",
    "        logger.info(f\"Final 3D feature count: {np.sum(valid_mask)} of {X.shape[1]} original features\")\n",
    "        return valid_mask\n",
    "    \n",
    "    def _scale_features(self, X, fit=True):\n",
    "        \"\"\"Scale 3D features (all continuous).\"\"\"\n",
    "        if X is None or X.shape[0] == 0 or X.shape[1] == 0:\n",
    "            return X\n",
    "            \n",
    "        scale_continuous = self.config['feature_scaling'].get('scale_continuous', True)\n",
    "        continuous_method = self.config['feature_scaling'].get('continuous_scaling_method', 'standard')\n",
    "        \n",
    "        if scale_continuous:\n",
    "            if fit:\n",
    "                self.scaler = Utilities.get_scaler(continuous_method)\n",
    "                if self.scaler is not None:\n",
    "                    X_scaled = self.scaler.fit_transform(X)\n",
    "                    logger.info(f\"Applied {continuous_method} scaling to 3D features\")\n",
    "                else:\n",
    "                    X_scaled = X\n",
    "            else:\n",
    "                if self.scaler is None:\n",
    "                    logger.warning(\"Scaler not fitted before transform. Returning unscaled features.\")\n",
    "                    X_scaled = X\n",
    "                else:\n",
    "                    X_scaled = self.scaler.transform(X)\n",
    "        else:\n",
    "            X_scaled = X\n",
    "            \n",
    "        X_clipped = np.clip(X_scaled, np.finfo(np.float32).min, np.finfo(np.float32).max)\n",
    "        return X_clipped.astype(np.float32)\n",
    "    \n",
    "    def update_metadata(self, metadata: Feature3DMetadata):\n",
    "        \"\"\"Update the Feature3DMetadata by filtering out features dropped by the valid mask.\"\"\"\n",
    "        if metadata is None or metadata.valid_mask is None:\n",
    "            logger.warning(\"No valid metadata or valid_mask provided for update.\")\n",
    "            return metadata\n",
    "            \n",
    "        filtered_names = [name for name, valid in zip(metadata.feature_names, metadata.valid_mask) if valid]\n",
    "        filtered_types = [typ for typ, valid in zip(metadata.feature_types, metadata.valid_mask) if valid]\n",
    "        \n",
    "        # Recalculate segment lengths based on filtered features\n",
    "        type_counts = {}\n",
    "        for ftype in ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT']:\n",
    "            type_counts[ftype] = filtered_types.count(ftype)\n",
    "        \n",
    "        updated_segment_lengths = [\n",
    "            type_counts['Basic3D'],\n",
    "            type_counts['AUTOCORR3D'],\n",
    "            type_counts['RDF'],\n",
    "            type_counts['MORSE'],\n",
    "            type_counts['WHIM'],\n",
    "            type_counts['GETAWAY'],\n",
    "            type_counts['USR'],\n",
    "            type_counts['USRCAT']\n",
    "        ]\n",
    "        \n",
    "        logger.info(f\"Updated 3D metadata: {len(filtered_names)} valid features retained out of {metadata.total_features()}.\")\n",
    "        logger.info(f\"Feature breakdown after filtering:\")\n",
    "        for ftype, count in type_counts.items():\n",
    "            if count > 0:\n",
    "                logger.info(f\"  {ftype}: {count} features\")\n",
    "        \n",
    "        return Feature3DMetadata(\n",
    "            feature_names=filtered_names,\n",
    "            feature_types=filtered_types,\n",
    "            feature_importance=metadata.feature_importance,\n",
    "            segment_lengths=updated_segment_lengths,\n",
    "            valid_mask=np.array([True]*len(filtered_names))\n",
    "        )\n",
    "    \n",
    "    def save(self, output_path):\n",
    "        \"\"\"Save preprocessor state.\"\"\"\n",
    "        state = {\n",
    "            'valid_feature_mask': self.valid_feature_mask,\n",
    "            'scaler': self.scaler,\n",
    "            'nan_mask': getattr(self, 'nan_mask', None),\n",
    "            'config_feature_scaling': self.config['feature_scaling'],\n",
    "            'config_spectral': self.config['spectral'],\n",
    "            'feature_type': '3D',\n",
    "            'total_features': 984\n",
    "        }\n",
    "        dump(state, output_path)\n",
    "        logger.info(f\"Saved 3D feature preprocessor to {output_path}\")\n",
    "\n",
    "print(\"Feature preprocessor for 3D features loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Saving Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- FeatureMapper3D ---------------------- #\n",
    "class FeatureMapper3D(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.feature_map = {}\n",
    "        self.output_file = os.path.join(config['paths']['output_dir'](dataset_type), 'feature_mapping_3d.jsonl')\n",
    "        self.full_mapping_file = os.path.join(config['paths']['output_dir'](dataset_type), 'feature_mapping_3d_full.json')\n",
    "        \n",
    "    def collect_feature_info(self, metadata: Feature3DMetadata, target_scaler=None):\n",
    "        \"\"\"Collect 3D feature mapping information.\"\"\"\n",
    "        if metadata.valid_mask is None or len(metadata.valid_mask) != metadata.total_features():\n",
    "            logger.error(\"Mismatch between valid mask and total features in metadata.\")\n",
    "            raise ValueError(\"Inconsistent feature metadata.\")\n",
    "        \n",
    "        # Store the mapping with original indices (BEFORE filtering)\n",
    "        feature_indices = [i for i, _ in enumerate(metadata.feature_names)]\n",
    "        filtered_indices = [i for i, valid in zip(feature_indices, metadata.valid_mask) if valid]\n",
    "        filtered_names = [name for name, valid in zip(metadata.feature_names, metadata.valid_mask) if valid]\n",
    "        filtered_types = [typ for typ, valid in zip(metadata.feature_types, metadata.valid_mask) if valid]\n",
    "        \n",
    "        # Create feature map for filtered features with original indices\n",
    "        self.feature_map = {\n",
    "            i: {\n",
    "                'name': name, \n",
    "                'type': typ, \n",
    "                'new_index': i,\n",
    "                'original_index': original_idx,\n",
    "                'importance': self._get_feature_importance(name, metadata.feature_importance)\n",
    "            }\n",
    "            for i, (name, typ, original_idx) in enumerate(zip(filtered_names, filtered_types, filtered_indices))\n",
    "        }\n",
    "        \n",
    "        # Save complete feature information for backend\n",
    "        self.complete_feature_info = {\n",
    "            'feature_type': '3D',\n",
    "            'all_feature_names': metadata.feature_names,\n",
    "            'all_feature_types': metadata.feature_types,\n",
    "            'valid_mask': metadata.valid_mask.tolist() if isinstance(metadata.valid_mask, np.ndarray) else metadata.valid_mask,\n",
    "            'filtered_indices': filtered_indices,\n",
    "            'filtered_names': filtered_names,\n",
    "            'filtered_types': filtered_types,\n",
    "            'input_dimension': len(metadata.feature_names),\n",
    "            'output_dimension': len(filtered_names),\n",
    "            'feature_importance': metadata.feature_importance,\n",
    "            'feature_categories': ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT'],\n",
    "            'feature_scaling_config': self.config['feature_scaling'],\n",
    "            'target_scaling_config': self.config['target_scaling'],\n",
    "            'target_scaler_fitted': target_scaler is not None,\n",
    "            'total_3d_features': 984\n",
    "        }\n",
    "    \n",
    "    def _get_feature_importance(self, feature_name, importance_data):\n",
    "        \"\"\"Get importance score for a specific feature.\"\"\"\n",
    "        if importance_data:\n",
    "            for feat in importance_data.get('top_10_features', []):\n",
    "                if feat['name'] == feature_name:\n",
    "                    return feat['importance']\n",
    "        return 0.0\n",
    "        \n",
    "    def save_mapping(self):\n",
    "        \"\"\"Save 3D feature mapping to files.\"\"\"\n",
    "        if not self.feature_map:\n",
    "            logger.warning(\"No feature mapping to save\")\n",
    "            return\n",
    "            \n",
    "        # Save individual feature mappings as JSONL\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            for idx, info in self.feature_map.items():\n",
    "                mapping = {\n",
    "                    'index': idx,\n",
    "                    'name': info['name'],\n",
    "                    'type': info['type'],\n",
    "                    'original_index': info['original_index'],\n",
    "                    'importance': info['importance']\n",
    "                }\n",
    "                f.write(json.dumps(mapping) + \"\\n\")\n",
    "                \n",
    "        # Save complete mapping information as single JSON for easier loading in backend\n",
    "        with open(self.full_mapping_file, 'w') as f:\n",
    "            json.dump(self.complete_feature_info, f, indent=2)\n",
    "                \n",
    "        logger.info(f\"Saved 3D feature mapping for {len(self.feature_map)} features to {self.output_file}\")\n",
    "        logger.info(f\"Saved complete 3D feature info to {self.full_mapping_file}\")\n",
    "\n",
    "# ---------------------- DataSaver ---------------------- #\n",
    "class DataSaver(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def save_split(self, smiles_list, X, y, indices, filepath, is_features_scaled=True, is_targets_scaled=False, feature_type='3D'):\n",
    "        \"\"\"Save regression data split with 3D features.\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w') as f:\n",
    "                for idx in tqdm(indices, desc=f\"Saving to {os.path.basename(filepath)}\"):\n",
    "                    sample = {\n",
    "                        \"smiles\": smiles_list[idx],\n",
    "                        \"features\": X[idx].tolist(),\n",
    "                        \"target\": y[idx].tolist(),\n",
    "                        \"is_features_scaled\": is_features_scaled,\n",
    "                        \"is_targets_scaled\": is_targets_scaled,\n",
    "                        \"feature_type\": feature_type,\n",
    "                        \"feature_count\": len(X[idx])\n",
    "                    }\n",
    "                    f.write(json.dumps(sample) + \"\\n\")\n",
    "                    # Clear sample to free memory\n",
    "                    sample = None\n",
    "            logger.info(f\"Saved {len(indices)} samples to {filepath}\")\n",
    "            return len(indices)\n",
    "        except Exception as e:\n",
    "            self.handle_error(e, f\"saving data to {filepath}\", data={\"file_path\": filepath, \"indices_count\": len(indices)})\n",
    "            return 0\n",
    "    \n",
    "    def save_seq2seq_split(self, seq2seq_data, indices, filepath):\n",
    "        \"\"\"Save seq2seq formatted data with 3D features.\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w') as f:\n",
    "                for idx in tqdm(indices, desc=f\"Saving seq2seq to {os.path.basename(filepath)}\"):\n",
    "                    sample = seq2seq_data[idx]\n",
    "                    # Convert numpy arrays to lists for JSON\n",
    "                    json_sample = {\n",
    "                        \"smiles\": sample['smiles'],\n",
    "                        \"input_sequence\": sample['input_sequence'].tolist(),\n",
    "                        \"output_sequence\": sample['output_sequence'].tolist(),\n",
    "                        \"output_mask\": sample['output_mask'].tolist(),\n",
    "                        \"original_peak_count\": sample['original_peak_count'],\n",
    "                        \"molecular_weight\": sample.get('molecular_weight', 0),\n",
    "                        \"exact_mass\": sample.get('exact_mass', 0),\n",
    "                        \"feature_type\": \"3D\",\n",
    "                        \"input_dim\": len(sample['input_sequence'])\n",
    "                    }\n",
    "                    f.write(json.dumps(json_sample) + \"\\n\")\n",
    "            logger.info(f\"Saved {len(indices)} seq2seq samples with 3D features to {filepath}\")\n",
    "        except Exception as e:\n",
    "            self.handle_error(e, f\"saving seq2seq data to {filepath}\")\n",
    "    \n",
    "    def save_target_scaler(self, scaler, filepath):\n",
    "        \"\"\"Save target scaler if target scaling is enabled.\"\"\"\n",
    "        if scaler is not None:\n",
    "            dump(scaler, filepath)\n",
    "            logger.info(f\"Saved target scaler to {filepath}\")\n",
    "\n",
    "print(\"Data saving components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Execute Stage 2A: Process 3D Features and Prepare ML Data\n",
    "\n",
    "Run the complete Stage 2A pipeline with comprehensive 3D features (984 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2A: 3D FEATURE COMBINATION AND ML PREPARATION\n",
      "============================================================\n",
      "Loading comprehensive 3D features (984 features) and spectral data from Stage 1...\n",
      "Initial memory usage: 1416.4 MB (RSS), 1.1% of total\n",
      "\n",
      "Loading raw spectral data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:00,091 - INFO - Loading data from ../data/tmp/hpj/raw_spectral_data_3d.jsonl\n",
      "Loading raw_spectral_data_3d.jsonl: 2720it [00:00, 3263.19it/s]\n",
      "2025-06-23 10:17:00,932 - INFO - Loaded 2720 records from ../data/tmp/hpj/raw_spectral_data_3d.jsonl\n",
      "2025-06-23 10:17:01,358 - INFO - Loading data from ../data/tmp/hpj/molecular_features_3d.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading raw data: 1418.5 MB (RSS), 1.1% of total\n",
      "\n",
      "Loading comprehensive 3D molecular features (984 features)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading molecular_features_3d.jsonl: 2720it [00:00, 3176.99it/s]\n",
      "2025-06-23 10:17:02,217 - INFO - Loaded 2720 records from ../data/tmp/hpj/molecular_features_3d.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading 3D molecular features: 1418.5 MB (RSS), 1.1% of total\n",
      "\n",
      "Loaded 2720 spectral records and 2720 3D molecule features\n",
      "3D feature dimension check: 984 features (expected: 984)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"STAGE 2A: 3D FEATURE COMBINATION AND ML PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from Stage 1\n",
    "print(\"Loading comprehensive 3D features (984 features) and spectral data from Stage 1...\")\n",
    "\n",
    "# Define paths\n",
    "dataset_type = STAGE2A_3D_CONFIG['dataset_type']\n",
    "temp_dir = STAGE2A_3D_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "raw_data_path = os.path.join(temp_dir, 'raw_spectral_data_3d.jsonl')\n",
    "mol_features_path = os.path.join(temp_dir, 'molecular_features_3d.jsonl')\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(STAGE2A_3D_CONFIG, dataset_type)\n",
    "\n",
    "# Log initial memory usage\n",
    "print(f\"Initial memory usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"\")\n",
    "\n",
    "# Step 1: Load raw spectral data and convert to DataFrames\n",
    "print(\"Loading raw spectral data...\")\n",
    "raw_data_records = data_loader.load_data_from_jsonl(raw_data_path)\n",
    "raw_data = data_loader.convert_raw_data_to_df(raw_data_records)\n",
    "\n",
    "# Free up memory\n",
    "del raw_data_records\n",
    "\n",
    "print(f\"Memory usage after loading raw data: {Utilities.get_memory_usage()}\")\n",
    "\n",
    "# Step 2: Load 3D molecular features\n",
    "print(\"\\nLoading comprehensive 3D molecular features (984 features)...\")\n",
    "molecular_features_3d = data_loader.load_data_from_jsonl(mol_features_path)\n",
    "\n",
    "print(f\"Memory usage after loading 3D molecular features: {Utilities.get_memory_usage()}\")\n",
    "print(f\"\\nLoaded {len(raw_data)} spectral records and {len(molecular_features_3d)} 3D molecule features\")\n",
    "\n",
    "# Verify feature dimensions\n",
    "first_smiles = next(iter(molecular_features_3d.keys()))\n",
    "first_features = molecular_features_3d[first_smiles].get('features_3d', [])\n",
    "print(f\"3D feature dimension check: {len(first_features)} features (expected: 984)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing processing components for comprehensive 3D features...\n",
      "Components initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize all processing components\n",
    "print(\"Initializing processing components for comprehensive 3D features...\")\n",
    "spectral_processor = SpectralProcessor(STAGE2A_3D_CONFIG, dataset_type)\n",
    "data_combiner = DataCombiner3D(STAGE2A_3D_CONFIG, dataset_type)\n",
    "binning_processor = BinningProcessor(STAGE2A_3D_CONFIG, dataset_type)\n",
    "regression_preparer = RegressionDataPreparer3D(STAGE2A_3D_CONFIG, dataset_type, feature_importance_data)\n",
    "feature_preprocessor = FeaturePreprocessor3D(STAGE2A_3D_CONFIG, dataset_type)\n",
    "data_saver = DataSaver(STAGE2A_3D_CONFIG, dataset_type)\n",
    "feature_mapper = FeatureMapper3D(STAGE2A_3D_CONFIG, dataset_type)\n",
    "\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    seq2seq_preparer = Seq2SeqDataPreparer3D(STAGE2A_3D_CONFIG, dataset_type)\n",
    "    print(\"Seq2seq data preparer for 3D features initialized\")\n",
    "\n",
    "print(\"Components initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:02,249 - INFO - Processing spectral sequences (binning=enabled)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spectral data...\n",
      "Processing binned spectra for regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra: 100%|| 2720/2720 [00:01<00:00, 2304.79it/s]\n",
      "2025-06-23 10:17:04,160 - INFO - Maximum observed m/z across all spectra: 683.0\n",
      "2025-06-23 10:17:04,160 - INFO - Configured max_mz for binning: 499\n",
      "2025-06-23 10:17:04,162 - INFO - 10 spectra (0.37%) have peaks beyond configured max_mz\n",
      "2025-06-23 10:17:04,163 - INFO - Processed spectral sequences for 2720 compounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after spectral processing: 1418.5 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Process spectral data\n",
    "print(\"Processing spectral data...\")\n",
    "\n",
    "# For regression, we need binned spectra\n",
    "print(\"Processing binned spectra for regression...\")\n",
    "spectra_sequences_binned = spectral_processor.process(raw_data, use_binning=True)\n",
    "\n",
    "# For seq2seq, we need full resolution spectra\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"\\nProcessing full resolution spectra for seq2seq...\")\n",
    "    spectra_sequences_full = spectral_processor.process(raw_data, use_binning=False)\n",
    "\n",
    "# Free up raw data after processing\n",
    "del raw_data\n",
    "\n",
    "print(f\"\\nMemory usage after spectral processing: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining comprehensive 3D molecular features with spectral data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining data:  24%|       | 640/2720 [00:00<00:00, 6396.74it/s][10:17:04] WARNING: not removing hydrogen atom without neighbors\n",
      "Combining data: 100%|| 2720/2720 [00:00<00:00, 5006.26it/s]\n",
      "2025-06-23 10:17:04,728 - INFO - Combined data: 2720 compounds, missing features for 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after data combination: 1418.5 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine data\n",
    "print(\"Combining comprehensive 3D molecular features with spectral data...\")\n",
    "\n",
    "# Combine for regression (using binned spectra)\n",
    "combined_data_binned = data_combiner.process(spectra_sequences_binned, molecular_features_3d)\n",
    "\n",
    "# Also combine for seq2seq if enabled (using full resolution)\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    combined_data_full = data_combiner.process(spectra_sequences_full, molecular_features_3d)\n",
    "    del spectra_sequences_full  # Free memory\n",
    "\n",
    "# Free up memory after combining\n",
    "del spectra_sequences_binned\n",
    "del molecular_features_3d\n",
    "\n",
    "print(f\"\\nMemory usage after data combination: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:04,741 - INFO - Maximum observed m/z for binning: 683.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binned representations for regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning spectra: 100%|| 2720/2720 [00:03<00:00, 874.20it/s] \n",
      "2025-06-23 10:17:08,051 - INFO - Created binned representations for 2720 compounds\n",
      "2025-06-23 10:17:08,052 - INFO - Memory usage after binning: 1426.4 MB (RSS), 1.1% of total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after binning: 1426.4 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Bin spectral data for regression\n",
    "print(\"Creating binned representations for regression...\")\n",
    "binned_data = binning_processor.process(combined_data_binned)\n",
    "\n",
    "# Free up the combined data after binning\n",
    "del combined_data_binned\n",
    "\n",
    "print(f\"\\nMemory usage after binning: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving visualization sample...\n",
      "Saved 100 samples for visualization to ../data/results/hpj/full_featurised_3d/visualization_data_3d.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Save visualization sample for Stage 2B\n",
    "if STAGE2A_3D_CONFIG['processing']['save_visualization_sample']:\n",
    "    print(\"Saving visualization sample...\")\n",
    "    visualization_sample = {k: binned_data[k] for k in random.sample(list(binned_data.keys()), \n",
    "                                                                     min(STAGE2A_3D_CONFIG['processing']['visualization_sample_size'], \n",
    "                                                                         len(binned_data)))}\n",
    "    \n",
    "    # Save visualization data\n",
    "    viz_data_path = os.path.join(STAGE2A_3D_CONFIG['paths']['output_dir'](dataset_type), 'visualization_data_3d.pkl')\n",
    "    dump(visualization_sample, viz_data_path)\n",
    "    print(f\"Saved {len(visualization_sample)} samples for visualization to {viz_data_path}\")\n",
    "    \n",
    "    # Also save spectral processor stats and feature importance\n",
    "    viz_stats = {\n",
    "        'max_observed_mz': spectral_processor.max_observed_mz,\n",
    "        'binning_processor_max_mz': binning_processor.max_observed_mz,\n",
    "        'spectral_config': STAGE2A_3D_CONFIG['spectral'],\n",
    "        'feature_importance': feature_importance_data,\n",
    "        'feature_count': 984,\n",
    "        'feature_categories': ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT']\n",
    "    }\n",
    "    viz_stats_path = os.path.join(STAGE2A_3D_CONFIG['paths']['output_dir'](dataset_type), 'visualization_stats_3d.json')\n",
    "    with open(viz_stats_path, 'w') as f:\n",
    "        json.dump(viz_stats, f, indent=2)\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:08,663 - INFO - Extracting 3D features and targets for regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing 3D regression data with 984 features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing 3D regression data: 100%|| 2720/2720 [00:00<00:00, 18928.06it/s]\n",
      "2025-06-23 10:17:08,822 - INFO - Memory usage after 3D regression data preparation: 1424.4 MB (RSS), 1.1% of total\n",
      "2025-06-23 10:17:08,823 - INFO - Prepared 3D regression matrices - X: (2720, 984), y: (2720, 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3D Regression data prepared:\n",
      "  3D Feature matrix shape: (2720, 984) (expected: N x 984)\n",
      "  Target matrix shape: (2720, 500)\n",
      "  Number of samples: 2720\n",
      "  Feature categories: 8 unique types\n",
      "\n",
      "3D Feature breakdown:\n",
      "  Basic3D: 11 features (expected: 11)\n",
      "  AUTOCORR3D: 80 features (expected: 80)\n",
      "  RDF: 210 features (expected: 210)\n",
      "  MORSE: 224 features (expected: 224)\n",
      "  WHIM: 114 features (expected: 114)\n",
      "  GETAWAY: 273 features (expected: 273)\n",
      "  USR: 12 features (expected: 12)\n",
      "  USRCAT: 60 features (expected: 60)\n",
      "\n",
      "Memory usage after 3D regression preparation: 1424.4 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Prepare regression data with 3D features\n",
    "print(\"Preparing 3D regression data with 984 features...\")\n",
    "reg_result = regression_preparer.process(binned_data)\n",
    "\n",
    "if reg_result is None:\n",
    "    print(\"ERROR: 3D regression data preparation failed\")\n",
    "else:\n",
    "    X, y, all_smiles, feature_metadata = reg_result\n",
    "    print(f\"\\n3D Regression data prepared:\")\n",
    "    print(f\"  3D Feature matrix shape: {X.shape} (expected: N x 984)\")\n",
    "    print(f\"  Target matrix shape: {y.shape}\")\n",
    "    print(f\"  Number of samples: {len(all_smiles)}\")\n",
    "    print(f\"  Feature categories: {len(set(feature_metadata.feature_types))} unique types\")\n",
    "    \n",
    "    # Show feature breakdown\n",
    "    print(\"\\n3D Feature breakdown:\")\n",
    "    feature_types = ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT']\n",
    "    expected_counts = [11, 80, 210, 224, 114, 273, 12, 60]\n",
    "    for ftype, expected in zip(feature_types, expected_counts):\n",
    "        actual = feature_metadata.feature_types.count(ftype)\n",
    "        print(f\"  {ftype}: {actual} features (expected: {expected})\")\n",
    "    \n",
    "    # Free up binned data after regression preparation (keep if needed for seq2seq)\n",
    "    if not STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "        del binned_data\n",
    "    \n",
    "    print(f\"\\nMemory usage after 3D regression preparation: {Utilities.get_memory_usage()}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:08,910 - INFO - Dropped 0 3D features with invalid values (0 with NaN, 0 with infinity)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:08,932 - INFO - Dropped 901 3D features with variance below 1e-88\n",
      "2025-06-23 10:17:08,933 - INFO - Final 3D feature count: 83 of 984 original features\n",
      "2025-06-23 10:17:08,938 - INFO - Saved 3D feature preprocessor to ../data/results/hpj/full_featurised_3d/feature_preprocessor_3d.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing 984 3D features...\n",
      "3D Feature preprocessor saved to ../data/results/hpj/full_featurised_3d/feature_preprocessor_3d.pkl\n",
      "\n",
      "Preprocessed 3D feature shape: (2720, 83) (from 984 original features)\n",
      "Features removed: 901\n",
      "Memory usage after 3D feature preprocessing: 1423.4 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Preprocess 3D features\n",
    "print(\"Preprocessing 984 3D features...\")\n",
    "X_processed = feature_preprocessor.process(X, fit=True, metadata=feature_metadata)\n",
    "\n",
    "# Save preprocessor\n",
    "output_dir = STAGE2A_3D_CONFIG['paths']['output_dir'](dataset_type)\n",
    "preprocessor_output = os.path.join(output_dir, 'feature_preprocessor_3d.pkl')\n",
    "feature_preprocessor.save(preprocessor_output)\n",
    "print(f\"3D Feature preprocessor saved to {preprocessor_output}\")\n",
    "\n",
    "print(f\"\\nPreprocessed 3D feature shape: {X_processed.shape} (from {X.shape[1]} original features)\")\n",
    "print(f\"Features removed: {X.shape[1] - X_processed.shape[1]}\")\n",
    "print(f\"Memory usage after 3D feature preprocessing: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/val/test sets...\n",
      "Split ratios: train=0.8, val=0.1, test=0.1\n",
      "\n",
      "Data split complete:\n",
      "  Train: 2176 samples (83 features)\n",
      "  Val: 272 samples (83 features)\n",
      "  Test: 272 samples (83 features)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Split data\n",
    "print(\"Splitting data into train/val/test sets...\")\n",
    "print(f\"Split ratios: train={STAGE2A_3D_CONFIG['data_split']['train_size']}, \"\n",
    "      f\"val={STAGE2A_3D_CONFIG['data_split']['val_size']}, \"\n",
    "      f\"test={STAGE2A_3D_CONFIG['data_split']['test_size']}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(STAGE2A_3D_CONFIG['data_split']['random_seed'])\n",
    "np.random.seed(STAGE2A_3D_CONFIG['data_split']['random_seed'])\n",
    "\n",
    "# First split into train and temp\n",
    "train_size_ratio = STAGE2A_3D_CONFIG['data_split']['train_size']\n",
    "X_train, X_temp, y_train, y_temp, smiles_train, smiles_temp = train_test_split(\n",
    "    X_processed, y, all_smiles, \n",
    "    test_size=1-train_size_ratio, \n",
    "    random_state=STAGE2A_3D_CONFIG['data_split']['random_seed']\n",
    ")\n",
    "\n",
    "# Then split temp into validation and test\n",
    "val_ratio = STAGE2A_3D_CONFIG['data_split']['val_size'] / (STAGE2A_3D_CONFIG['data_split']['val_size'] + STAGE2A_3D_CONFIG['data_split']['test_size'])\n",
    "X_val, X_test, y_val, y_test, smiles_val, smiles_test = train_test_split(\n",
    "    X_temp, y_temp, smiles_temp,\n",
    "    test_size=1-val_ratio,\n",
    "    random_state=STAGE2A_3D_CONFIG['data_split']['random_seed']\n",
    ")\n",
    "\n",
    "# Free temporary variables\n",
    "del X_temp, y_temp, smiles_temp\n",
    "\n",
    "print(f\"\\nData split complete:\")\n",
    "print(f\"  Train: {len(X_train)} samples ({X_train.shape[1]} features)\")\n",
    "print(f\"  Val: {len(X_val)} samples ({X_val.shape[1]} features)\")\n",
    "print(f\"  Test: {len(X_test)} samples ({X_test.shape[1]} features)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:08,970 - INFO - Target scaling disabled, returning original targets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target scaling...\n",
      "Targets scaled: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Scale targets if enabled\n",
    "print(\"Processing target scaling...\")\n",
    "y_train_scaled, y_val_scaled, y_test_scaled = regression_preparer.scale_targets(y_train, y_val, y_test)\n",
    "\n",
    "# Save target scaler if used\n",
    "if STAGE2A_3D_CONFIG['target_scaling']['enabled'] and regression_preparer.target_scaler is not None:\n",
    "    target_scaler_path = os.path.join(output_dir, 'target_scaler_3d.pkl')\n",
    "    data_saver.save_target_scaler(regression_preparer.target_scaler, target_scaler_path)\n",
    "    print(f\"Target scaler saved to {target_scaler_path}\")\n",
    "\n",
    "# Determine if targets were scaled\n",
    "targets_scaled = STAGE2A_3D_CONFIG['target_scaling']['enabled'] and regression_preparer.target_scaler is not None\n",
    "print(f\"Targets scaled: {targets_scaled}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 10:17:08,981 - INFO - Updated 3D metadata: 83 valid features retained out of 984.\n",
      "2025-06-23 10:17:08,983 - INFO - Feature breakdown after filtering:\n",
      "2025-06-23 10:17:08,984 - INFO -   Basic3D: 11 features\n",
      "2025-06-23 10:17:08,985 - INFO -   USR: 12 features\n",
      "2025-06-23 10:17:08,986 - INFO -   USRCAT: 60 features\n",
      "2025-06-23 10:17:08,995 - INFO - Saved 3D feature mapping for 83 features to ../data/results/hpj/full_featurised_3d/feature_mapping_3d.jsonl\n",
      "2025-06-23 10:17:08,996 - INFO - Saved complete 3D feature info to ../data/results/hpj/full_featurised_3d/feature_mapping_3d_full.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating 3D feature metadata...\n",
      "3D Feature mapping saved\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Update and save 3D feature metadata\n",
    "print(\"Updating 3D feature metadata...\")\n",
    "updated_metadata = feature_preprocessor.update_metadata(feature_metadata)\n",
    "feature_mapper.collect_feature_info(updated_metadata, regression_preparer.target_scaler)\n",
    "feature_mapper.save_mapping()\n",
    "print(\"3D Feature mapping saved\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 3D regression data splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving to train_data_3d.jsonl: 100%|| 2176/2176 [00:00<00:00, 3179.84it/s]\n",
      "2025-06-23 10:17:09,711 - INFO - Saved 2176 samples to ../data/results/hpj/full_featurised_3d/train_data_3d.jsonl\n",
      "Saving to val_data_3d.jsonl: 100%|| 272/272 [00:00<00:00, 3005.99it/s]\n",
      "2025-06-23 10:17:09,811 - INFO - Saved 272 samples to ../data/results/hpj/full_featurised_3d/val_data_3d.jsonl\n",
      "Saving to test_data_3d.jsonl: 100%|| 272/272 [00:00<00:00, 2822.57it/s]\n",
      "2025-06-23 10:17:09,917 - INFO - Saved 272 samples to ../data/results/hpj/full_featurised_3d/test_data_3d.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3D Regression data saved to:\n",
      "  ../data/results/hpj/full_featurised_3d/train_data_3d.jsonl\n",
      "  ../data/results/hpj/full_featurised_3d/val_data_3d.jsonl\n",
      "  ../data/results/hpj/full_featurised_3d/test_data_3d.jsonl\n",
      "\n",
      "Saved configuration to ../data/results/hpj/full_featurised_3d/dataset_config_3d.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Save regression splits with 3D features\n",
    "print(\"Saving 3D regression data splits...\")\n",
    "\n",
    "# Define output paths\n",
    "train_output = os.path.join(output_dir, 'train_data_3d.jsonl')\n",
    "val_output = os.path.join(output_dir, 'val_data_3d.jsonl')\n",
    "test_output = os.path.join(output_dir, 'test_data_3d.jsonl')\n",
    "\n",
    "train_indices = list(range(len(smiles_train)))\n",
    "val_indices = list(range(len(smiles_val)))\n",
    "test_indices = list(range(len(smiles_test)))\n",
    "\n",
    "# Save with scaling information\n",
    "data_saver.save_split(smiles_train, X_train, y_train_scaled, train_indices, train_output, \n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled, feature_type='3D')\n",
    "data_saver.save_split(smiles_val, X_val, y_val_scaled, val_indices, val_output,\n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled, feature_type='3D')\n",
    "data_saver.save_split(smiles_test, X_test, y_test_scaled, test_indices, test_output,\n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled, feature_type='3D')\n",
    "\n",
    "print(\"\\n3D Regression data saved to:\")\n",
    "print(f\"  {train_output}\")\n",
    "print(f\"  {val_output}\")\n",
    "print(f\"  {test_output}\")\n",
    "\n",
    "# Save dataset configuration\n",
    "config_path = os.path.join(output_dir, 'dataset_config_3d.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'dataset_type': dataset_type,\n",
    "        'stage2a_complete': True,\n",
    "        'num_samples': len(all_smiles),\n",
    "        'feature_dim': X_processed.shape[1],\n",
    "        'original_feature_dim': 984,\n",
    "        'target_dim': y.shape[1],\n",
    "        'spectral_config': STAGE2A_3D_CONFIG['spectral'],\n",
    "        'feature_scaling_config': STAGE2A_3D_CONFIG['feature_scaling'],\n",
    "        'target_scaling_config': STAGE2A_3D_CONFIG['target_scaling'],\n",
    "        'features_scaled': True,\n",
    "        'targets_scaled': targets_scaled,\n",
    "        'feature_type': '3D',\n",
    "        'feature_names': updated_metadata.feature_names,\n",
    "        'feature_types': updated_metadata.feature_types,\n",
    "        'feature_categories': ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT'],\n",
    "        'visualization_saved': STAGE2A_3D_CONFIG['processing']['save_visualization_sample']\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nSaved configuration to {config_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after saving all data: 1422.4 MB (RSS), 1.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Generate seq2seq data with 3D features if enabled\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"GENERATING SEQ2SEQ DATASET WITH COMPREHENSIVE 3D FEATURES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process seq2seq data with 3D features\n",
    "    seq2seq_data = seq2seq_preparer.process(X_processed, combined_data_full, all_smiles)\n",
    "    \n",
    "    # Split seq2seq data using same indices as regression\n",
    "    # Create mapping from SMILES to indices\n",
    "    smiles_to_idx = {smiles: idx for idx, smiles in enumerate(all_smiles)}\n",
    "    \n",
    "    # Get indices for each split\n",
    "    train_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_train]\n",
    "    val_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_val]\n",
    "    test_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_test]\n",
    "    \n",
    "    # Create seq2seq splits\n",
    "    seq2seq_train = [seq2seq_data[i] for i in train_indices_seq2seq]\n",
    "    seq2seq_val = [seq2seq_data[i] for i in val_indices_seq2seq]\n",
    "    seq2seq_test = [seq2seq_data[i] for i in test_indices_seq2seq]\n",
    "    \n",
    "    # Define seq2seq output paths\n",
    "    seq2seq_output_dir = os.path.join(\n",
    "        STAGE2A_3D_CONFIG['paths']['results_dir'](dataset_type),\n",
    "        STAGE2A_3D_CONFIG['seq2seq']['output_subdir']\n",
    "    )\n",
    "    os.makedirs(seq2seq_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving seq2seq data with 3D features to {seq2seq_output_dir}...\")\n",
    "    \n",
    "    # Save seq2seq splits\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_train, \n",
    "        list(range(len(seq2seq_train))), \n",
    "        os.path.join(seq2seq_output_dir, 'train_data_3d.jsonl')\n",
    "    )\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_val,\n",
    "        list(range(len(seq2seq_val))),\n",
    "        os.path.join(seq2seq_output_dir, 'val_data_3d.jsonl')\n",
    "    )\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_test,\n",
    "        list(range(len(seq2seq_test))),\n",
    "        os.path.join(seq2seq_output_dir, 'test_data_3d.jsonl')\n",
    "    )\n",
    "    \n",
    "    # Save PCA transformer\n",
    "    pca_path = os.path.join(seq2seq_output_dir, 'pca_transformer_3d.pkl')\n",
    "    seq2seq_preparer.save_pca(pca_path)\n",
    "    \n",
    "    # Save seq2seq config with 3D feature info\n",
    "    seq2seq_config = {\n",
    "        'pca_components': seq2seq_preparer.pca.n_components_,\n",
    "        'max_sequence_length': STAGE2A_3D_CONFIG['seq2seq']['max_sequence_length'],\n",
    "        'input_dim': seq2seq_preparer.pca.n_components_,\n",
    "        'output_dim': 2,  # (mz, intensity)\n",
    "        'dataset_type': dataset_type,\n",
    "        'pca_explained_variance': float(np.sum(seq2seq_preparer.pca.explained_variance_ratio_)),\n",
    "        'feature_type': '3D',\n",
    "        'original_feature_dim': X_processed.shape[1],\n",
    "        'original_feature_count': 984,\n",
    "        'feature_names': updated_metadata.feature_names,\n",
    "        'feature_categories': ['Basic3D', 'AUTOCORR3D', 'RDF', 'MORSE', 'WHIM', 'GETAWAY', 'USR', 'USRCAT']\n",
    "    }\n",
    "    \n",
    "    seq2seq_config_path = os.path.join(seq2seq_output_dir, 'seq2seq_config_3d.json')\n",
    "    with open(seq2seq_config_path, 'w') as f:\n",
    "        json.dump(seq2seq_config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSeq2seq data with 3D features saved to {seq2seq_output_dir}\")\n",
    "    print(f\"PCA explained variance: {seq2seq_config['pca_explained_variance']:.2%}\")\n",
    "    print(f\"PCA components: {seq2seq_config['pca_components']} (from {seq2seq_config['original_feature_dim']} filtered features, originally 984)\")\n",
    "    \n",
    "    # Clean up\n",
    "    del combined_data_full\n",
    "    del seq2seq_data\n",
    "    del seq2seq_train\n",
    "    del seq2seq_val\n",
    "    del seq2seq_test\n",
    "\n",
    "# Free up memory after saving data splits\n",
    "del X, y, X_processed\n",
    "del X_train, y_train, smiles_train, y_train_scaled\n",
    "del X_val, y_val, smiles_val, y_val_scaled\n",
    "del X_test, y_test, smiles_test, y_test_scaled\n",
    "\n",
    "print(f\"\\nMemory usage after saving all data: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Summary\n",
    "\n",
    "Display final summary of Stage 2A processing with comprehensive 3D features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2A 3D FEATURE SUMMARY\n",
      "============================================================\n",
      "  Dataset Type: hpj\n",
      "  Total Samples: 2720\n",
      "\n",
      "  3D Feature Dimensions: 83 (from 984 original)\n",
      "  Feature Type: 3D (Comprehensive)\n",
      "  Feature Categories: Basic3D (11), AUTOCORR3D (80), RDF (210), MORSE (224), WHIM (114), GETAWAY (273), USR (12), USRCAT (60)\n",
      "\n",
      "  Max Observed m/z: 683.0 (configured max_mz: 499)\n",
      "\n",
      "  Scaling Configuration:\n",
      "    3D Features:\n",
      "      - Scale continuous: False (method: standard)\n",
      "      - Log transform: False\n",
      "    Targets:\n",
      "      - Enabled: False (method: standard)\n",
      "      - Applied: False\n",
      "\n",
      "  Top 5 3D Features:\n",
      "    1. PMI1: 0.0917\n",
      "    2. USRCAT_31: 0.0547\n",
      "    3. USRCAT_33: 0.0495\n",
      "    4. USRCAT_24: 0.0424\n",
      "    5. USRCAT_27: 0.0397\n",
      "\n",
      "  Regression Output Files:\n",
      "    - ../data/results/hpj/full_featurised_3d/train_data_3d.jsonl\n",
      "    - ../data/results/hpj/full_featurised_3d/val_data_3d.jsonl\n",
      "    - ../data/results/hpj/full_featurised_3d/test_data_3d.jsonl\n",
      "    - ../data/results/hpj/full_featurised_3d/feature_preprocessor_3d.pkl\n",
      "    - ../data/results/hpj/full_featurised_3d/feature_mapping_3d.jsonl\n",
      "\n",
      "  Visualization Files:\n",
      "    - ../data/results/hpj/full_featurised_3d/visualization_data_3d.pkl\n",
      "    - ../data/results/hpj/full_featurised_3d/visualization_stats_3d.json\n",
      "\n",
      "  Final Memory Usage: 1421.4 MB (RSS), 1.1% of total\n",
      "\n",
      "============================================================\n",
      "STAGE 2A 3D COMPLETE!\n",
      "Next: Run 02b_3d_featurization_visualization.ipynb for visualizations\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Log summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2A 3D FEATURE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dimensions = \"\"\n",
    "if feature_preprocessor.valid_feature_mask is not None:\n",
    "    feature_dimensions = f\"\\n  3D Feature Dimensions: {np.sum(feature_preprocessor.valid_feature_mask)} (from 984 original)\"\n",
    "    \n",
    "# Include statistics about m/z range coverage in summary\n",
    "max_observed_mz_info = \"\"\n",
    "if hasattr(spectral_processor, 'max_observed_mz') and spectral_processor.max_observed_mz > 0:\n",
    "    max_observed_mz_info = (f\"\\n  Max Observed m/z: {spectral_processor.max_observed_mz:.1f} \" +\n",
    "                           f\"(configured max_mz: {STAGE2A_3D_CONFIG['spectral']['max_mz']})\")\n",
    "    \n",
    "print(f\"  Dataset Type: {dataset_type}\")\n",
    "print(f\"  Total Samples: {len(all_smiles)}\")\n",
    "print(feature_dimensions)\n",
    "print(f\"  Feature Type: 3D (Comprehensive)\")\n",
    "print(f\"  Feature Categories: Basic3D (11), AUTOCORR3D (80), RDF (210), MORSE (224), WHIM (114), GETAWAY (273), USR (12), USRCAT (60)\")\n",
    "print(max_observed_mz_info)\n",
    "\n",
    "print(f\"\\n  Scaling Configuration:\")\n",
    "print(f\"    3D Features:\")\n",
    "print(f\"      - Scale continuous: {STAGE2A_3D_CONFIG['feature_scaling']['scale_continuous']} (method: {STAGE2A_3D_CONFIG['feature_scaling']['continuous_scaling_method']})\")\n",
    "print(f\"      - Log transform: {STAGE2A_3D_CONFIG['feature_scaling']['apply_log_transform']}\")\n",
    "print(f\"    Targets:\")\n",
    "print(f\"      - Enabled: {STAGE2A_3D_CONFIG['target_scaling']['enabled']} (method: {STAGE2A_3D_CONFIG['target_scaling']['scaling_method']})\")\n",
    "print(f\"      - Applied: {targets_scaled}\")\n",
    "\n",
    "# Print top 3D features\n",
    "if feature_importance_data and 'top_10_features' in feature_importance_data:\n",
    "    print(\"\\n  Top 5 3D Features:\")\n",
    "    for i, feat in enumerate(feature_importance_data['top_10_features'][:5]):\n",
    "        print(f\"    {i+1}. {feat['name']}: {feat['importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n  Regression Output Files:\")\n",
    "print(f\"    - {train_output}\")\n",
    "print(f\"    - {val_output}\")\n",
    "print(f\"    - {test_output}\")\n",
    "print(f\"    - {preprocessor_output}\")\n",
    "print(f\"    - {feature_mapper.output_file}\")\n",
    "if targets_scaled:\n",
    "    print(f\"    - {target_scaler_path}\")\n",
    "\n",
    "if STAGE2A_3D_CONFIG['processing']['save_visualization_sample']:\n",
    "    print(f\"\\n  Visualization Files:\")\n",
    "    print(f\"    - {viz_data_path}\")\n",
    "    print(f\"    - {viz_stats_path}\")\n",
    "\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    print(f\"\\n  Seq2Seq Output Files (3D):\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'train_data_3d.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'val_data_3d.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'test_data_3d.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'pca_transformer_3d.pkl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'seq2seq_config_3d.json')}\")\n",
    "\n",
    "print(f\"\\n  Final Memory Usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2A 3D COMPLETE!\")\n",
    "print(\"Next: Run 02b_3d_featurization_visualization.ipynb for visualizations\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Output Summary and Next Steps\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "**3D Regression Format (data/results/{dataset}/full_featurised_3d/):**\n",
    "- `train_data_3d.jsonl` - Training set with scaled 3D features (984 features) and targets\n",
    "- `val_data_3d.jsonl` - Validation set\n",
    "- `test_data_3d.jsonl` - Test set\n",
    "- `feature_preprocessor_3d.pkl` - Feature scaling and preprocessing state for 3D features\n",
    "- `target_scaler_3d.pkl` - Target scaling state (if enabled)\n",
    "- `feature_mapping_3d.jsonl` - 3D feature metadata for interpretability\n",
    "- `feature_mapping_3d_full.json` - Complete 3D feature and scaling information\n",
    "- `dataset_config_3d.json` - Configuration metadata\n",
    "- `visualization_data_3d.pkl` - Sample data for Stage 2B\n",
    "- `visualization_stats_3d.json` - Statistics for Stage 2B\n",
    "\n",
    "**3D Seq2Seq Format (data/results/{dataset}/seq2seq_featurised_3d/):**\n",
    "- `train_data_3d.jsonl` - Training set with PCA-reduced 3D features and full resolution spectra\n",
    "- `val_data_3d.jsonl` - Validation set\n",
    "- `test_data_3d.jsonl` - Test set\n",
    "- `pca_transformer_3d.pkl` - PCA transformation matrix for 3D features\n",
    "- `seq2seq_config_3d.json` - Configuration and metadata\n",
    "\n",
    "### Comprehensive 3D Features Used (984 total):\n",
    "- **Basic 3D**: PMI1-3, NPR1-2, Asphericity, Eccentricity, InertialShapeFactor, SpherocityIndex, RadiusOfGyration, PBF (11 features)\n",
    "- **AUTOCORR3D**: 3D autocorrelation descriptors (80 features)\n",
    "- **RDF**: Radial Distribution Function descriptors (210 features)\n",
    "- **MORSE**: 3D-MoRSE descriptors (224 features)\n",
    "- **WHIM**: Weighted Holistic Invariant Molecular descriptors (114 features)\n",
    "- **GETAWAY**: GEometry, Topology, and Atom-Weights AssemblY descriptors (273 features)\n",
    "- **USR**: Ultrafast Shape Recognition descriptors (12 features)\n",
    "- **USRCAT**: USR with pharmacophoric information (60 features)\n",
    "\n",
    "### Next Steps:\n",
    "Run Stage 2B (02b_3d_featurization_visualization.ipynb) to create comprehensive visualizations of the processed 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file sizes:\n",
      "\n",
      "3D Regression files:\n",
      "  - dataset_config_3d.json: 0.00 MB\n",
      "  - feature_mapping_3d.jsonl: 0.01 MB\n",
      "  - feature_mapping_3d_full.json: 0.04 MB\n",
      "  - feature_preprocessor_3d.pkl: 0.00 MB\n",
      "  - test_data_3d.jsonl: 1.66 MB\n",
      "  - train_data_3d.jsonl: 13.10 MB\n",
      "  - val_data_3d.jsonl: 1.65 MB\n",
      "  - visualization_data_3d.pkl: 3.77 MB\n",
      "  - visualization_stats_3d.json: 0.03 MB\n",
      "\n",
      "Temporary files (can be deleted after pipeline completion):\n",
      "  - molecular_features_3d.jsonl: 48.53 MB\n",
      "  - dataset_config.json: 0.00 MB\n",
      "  - dataset_metadata_3d.json: 0.00 MB\n",
      "  - 3d_features.pkl: 10.96 MB\n",
      "  - corrupted_records.jsonl: 0.00 MB\n",
      "  - corrupted_3d_records.jsonl: 0.00 MB\n",
      "  - dataset_config_3d.json: 0.02 MB\n",
      "  - molecular_features.jsonl: 111.92 MB\n",
      "  - feature_importance_3d.json: 0.03 MB\n",
      "  - raw_spectral_data.jsonl: 16.40 MB\n",
      "  - raw_spectral_data_3d.jsonl: 7.40 MB\n",
      "\n",
      "Total temporary storage: 195.25 MB\n",
      "\n",
      "============================================================\n",
      "COMPREHENSIVE 3D FEATURIZATION PIPELINE COMPLETE\n",
      "984 3D features extracted and processed successfully!\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display file sizes\n",
    "import os\n",
    "\n",
    "print(\"Generated file sizes:\")\n",
    "print(\"\\n3D Regression files:\")\n",
    "output_dir = STAGE2A_3D_CONFIG['paths']['output_dir'](dataset_type)\n",
    "if os.path.exists(output_dir):\n",
    "    for file in sorted(os.listdir(output_dir)):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "if STAGE2A_3D_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"\\n3D Seq2seq files:\")\n",
    "    seq2seq_dir = os.path.join(STAGE2A_3D_CONFIG['paths']['results_dir'](dataset_type), \n",
    "                               STAGE2A_3D_CONFIG['seq2seq']['output_subdir'])\n",
    "    if os.path.exists(seq2seq_dir):\n",
    "        for file in sorted(os.listdir(seq2seq_dir)):\n",
    "            file_path = os.path.join(seq2seq_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nTemporary files (can be deleted after pipeline completion):\")\n",
    "temp_dir = STAGE2A_3D_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "if os.path.exists(temp_dir):\n",
    "    total_temp_size = 0\n",
    "    for file in os.listdir(temp_dir):\n",
    "        file_path = os.path.join(temp_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            total_temp_size += size_mb\n",
    "            print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "    print(f\"\\nTotal temporary storage: {total_temp_size:.2f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE 3D FEATURIZATION PIPELINE COMPLETE\")\n",
    "print(\"984 3D features extracted and processed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio729p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
