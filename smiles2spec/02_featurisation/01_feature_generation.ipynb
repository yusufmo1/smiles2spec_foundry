{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 1: Molecular Featurization Pipeline\n",
    "\n",
    "This notebook implements the first stage of the mass spectrometry data processing pipeline, focusing on molecular feature extraction from SMILES strings.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### Responsibilities:\n",
    "1. **Load Raw Spectral Data**: Read JSONL files containing SMILES and peak data\n",
    "2. **Extract Molecular Features**:\n",
    "   - RDKit descriptors (200+ properties)\n",
    "   - Multiple fingerprint types (Morgan, MACCS, Avalon, etc.)\n",
    "   - Electronic properties and charge distributions\n",
    "3. **Post-process Features**: Remove NaN values and zero-variance features\n",
    "4. **Save Intermediate Data**: Store processed data for Stage 2\n",
    "\n",
    "### Input:\n",
    "- `data/input/{dataset_type}/spectral_data.jsonl`\n",
    "\n",
    "### Output (to temporary directory):\n",
    "- `data/tmp/{dataset_type}/raw_spectral_data.jsonl` - Original spectral data\n",
    "- `data/tmp/{dataset_type}/molecular_features.jsonl` - Extracted molecular features\n",
    "- `data/tmp/{dataset_type}/dataset_config.json` - Configuration metadata\n",
    "- `data/tmp/{dataset_type}/corrupted_records.jsonl` - Error log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and configure the environment for molecular feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import traceback\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from collections import defaultdict\n",
    "import psutil\n",
    "\n",
    "# RDKit imports\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdFingerprintGenerator, rdMolDescriptors, MACCSkeys, AllChem\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "from rdkit.Avalon import pyAvalonTools\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configuration specific to Stage 1: molecular featurization.\n",
    "\n",
    "**Key Configuration Sections:**\n",
    "- `dataset_type`: Specifies which dataset to process\n",
    "- `features`: Controls which molecular features to extract\n",
    "- `processing`: Parallel processing and error handling settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for dataset: hpj\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Stage 1 Configuration\n",
    "STAGE1_CONFIG = {\n",
    "    # Dataset configuration\n",
    "    'dataset_type': 'hpj',  # Change this to process different datasets\n",
    "    \n",
    "    # Paths configuration\n",
    "    'paths': {\n",
    "        'data_root': '../data',\n",
    "        'input_dir': lambda dtype: f\"../data/input/{dtype}\",\n",
    "        'input_file': lambda dtype: f\"../data/input/{dtype}/spectral_data.jsonl\",\n",
    "        'temp_dir': lambda dtype: f\"../data/tmp/{dtype}\",  # Intermediate storage\n",
    "        'corrupted_output': lambda dtype: f\"../data/tmp/{dtype}/corrupted_records.jsonl\",\n",
    "    },\n",
    "    \n",
    "    # Processing configuration\n",
    "    'processing': {\n",
    "        'n_jobs': -1,  # Use all available cores\n",
    "        'log_level': 'INFO',\n",
    "        'error_log_detail_level': 'standard'\n",
    "    },\n",
    "    \n",
    "    # Feature extraction settings\n",
    "    'features': {\n",
    "        'extract_descriptors': True,\n",
    "        'extract_electronic': True,\n",
    "        'extract_substructures': False,\n",
    "        'atom_pair_fp_size': 512,\n",
    "        \n",
    "        # Fingerprint configurations\n",
    "        'fingerprints': {\n",
    "            'morgan': {'enabled': True, 'radii': [1, 2, 3], 'size': 1024},\n",
    "            'morgan_feature': {'enabled': True, 'radius': 2, 'size': 1024},\n",
    "            'maccs': {'enabled': True},\n",
    "            'topological': {'enabled': True, 'size': 1024},\n",
    "            'rdkit': {'enabled': True, 'size': 2048},\n",
    "            'avalon': {'enabled': True, 'size': 1024},\n",
    "            'pattern': {'enabled': True, 'size': 1024},\n",
    "            'layered': {'enabled': True, 'size': 2048}\n",
    "        },\n",
    "        \n",
    "        # Count features\n",
    "        'count_features': {\n",
    "            'bond_counts': True,\n",
    "            'atom_counts': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded for dataset: {STAGE1_CONFIG['dataset_type']}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Directory Setup\n",
    "\n",
    "Create necessary directories for intermediate data storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 07:09:30,430 - INFO - Created directory: ../data/tmp/hpj\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up directories...\n",
      "Temporary directory: ../data/tmp/hpj\n",
      "Directory setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_directories(dataset_type):\n",
    "    \"\"\"Create all necessary directories for Stage 1.\"\"\"\n",
    "    temp_dir = STAGE1_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    logger.info(f\"Created directory: {temp_dir}\")\n",
    "    return temp_dir\n",
    "\n",
    "# Setup directories\n",
    "print(\"Setting up directories...\")\n",
    "temp_dir = setup_directories(STAGE1_CONFIG['dataset_type'])\n",
    "print(f\"Temporary directory: {temp_dir}\")\n",
    "print(\"Directory setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Utility Functions\n",
    "\n",
    "Core utilities for memory monitoring, data conversion, and error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions loaded\n",
      "Initial memory usage: 180.9 MB (RSS), 0.1% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Utilities ---------------------- #\n",
    "class Utilities:\n",
    "    @staticmethod\n",
    "    def get_memory_usage():\n",
    "        \"\"\"Get current memory usage in MB.\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        return f\"{mem_info.rss / (1024 * 1024):.1f} MB (RSS), {process.memory_percent():.1f}% of total\"\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_np_to_list(item):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist()\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: Utilities.convert_np_to_list(v) for k, v in item.items()}\n",
    "        elif isinstance(item, list):\n",
    "            return [Utilities.convert_np_to_list(v) for v in item]\n",
    "        else:\n",
    "            return item\n",
    "\n",
    "# ---------------------- Error Handling Mixin ---------------------- #\n",
    "class ErrorHandlingMixin:\n",
    "    \"\"\"Provides standardized error handling for pipeline components.\"\"\"\n",
    "    \n",
    "    def handle_error(self, error, context=\"\", data=None):\n",
    "        \"\"\"Centralized error handling with configurable detail level.\"\"\"\n",
    "        message = f\"Error in {self.__class__.__name__}\"\n",
    "        if context:\n",
    "            message += f\" ({context})\"\n",
    "        message += f\": {error}\"\n",
    "        \n",
    "        if hasattr(self, 'config') and self.config and 'processing' in self.config:\n",
    "            detail_level = self.config['processing'].get('error_log_detail_level', 'standard')\n",
    "        else:\n",
    "            detail_level = 'standard'\n",
    "            \n",
    "        if detail_level == 'full':\n",
    "            logger.error(message, exc_info=True)\n",
    "        elif detail_level == 'standard':\n",
    "            logger.error(message)\n",
    "        else:\n",
    "            logger.error(f\"Error in {self.__class__.__name__}: {error}\")\n",
    "        \n",
    "        if hasattr(self, 'config') and self.config and 'paths' in self.config:\n",
    "            if hasattr(self, 'dataset_type'):\n",
    "                corrupted_output = self.config['paths']['corrupted_output'](self.dataset_type)\n",
    "            else:\n",
    "                corrupted_output = self.config['paths']['corrupted_output'](self.config.get('dataset_type', 'unknown'))\n",
    "            \n",
    "            self._log_corrupted_record(\n",
    "                record_type=self.__class__.__name__,\n",
    "                data=data if data is not None else {'context': context},\n",
    "                error=error,\n",
    "                log_file=corrupted_output\n",
    "            )\n",
    "        return None\n",
    "    \n",
    "    def _log_corrupted_record(self, record_type, data, error, log_file):\n",
    "        \"\"\"Log corrupted records with standardized format.\"\"\"\n",
    "        record = {\n",
    "            'type': record_type,\n",
    "            'data': data,\n",
    "            'error': str(error),\n",
    "            'traceback': traceback.format_exc()\n",
    "        }\n",
    "        try:\n",
    "            with open(log_file, 'a') as fout:\n",
    "                fout.write(json.dumps(record) + \"\\n\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to log corrupted record: {e}\")\n",
    "\n",
    "# Helper function\n",
    "def is_valid_smiles(smi, error_log=None):\n",
    "    \"\"\"Check if SMILES string is valid.\"\"\"\n",
    "    if not isinstance(smi, str):\n",
    "        return False\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        return mol is not None\n",
    "    except Exception as e:\n",
    "        if error_log:\n",
    "            with open(error_log, 'a') as f:\n",
    "                f.write(json.dumps({\"error\": \"invalid SMILES\", \"smiles\": smi, \"exception\": str(e)}) + \"\\n\")\n",
    "        return False\n",
    "\n",
    "print(\"Utility functions loaded\")\n",
    "print(f\"Initial memory usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Loading\n",
    "\n",
    "Load raw spectral data from JSONL file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DataLoader ---------------------- #\n",
    "class DataLoader(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "    \n",
    "    def load_data(self, input_path):\n",
    "        \"\"\"Load spectral data from JSONL file.\"\"\"\n",
    "        error_log = self.config['paths']['corrupted_output'](self.dataset_type)\n",
    "        logging.info(f\"Loading spectral data from {input_path}\")\n",
    "        \n",
    "        data = {}\n",
    "        with open(input_path, 'r') as f:\n",
    "            for line in tqdm(f, desc=\"Loading data\"):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    smiles = record.get(\"smiles\")\n",
    "                    peaks = record.get(\"peaks\", [])\n",
    "                    df = pd.DataFrame(peaks, columns=[\"mz\", \"intensity\"]) if peaks else pd.DataFrame(columns=[\"mz\", \"intensity\"])\n",
    "                    data[smiles] = df\n",
    "                except Exception as e:\n",
    "                    self.handle_error(e, \"loading JSON line\", None)\n",
    "        \n",
    "        logging.info(f\"Loaded {len(data)} records\")\n",
    "        return data\n",
    "\n",
    "print(\"Data loader initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Molecular Feature Extraction\n",
    "\n",
    "Extract chemical descriptors and fingerprints from SMILES strings using RDKit.\n",
    "\n",
    "### Features Extracted:\n",
    "- **RDKit Descriptors**: 200+ molecular properties\n",
    "- **Fingerprints**: Morgan, MACCS, Avalon, RDKit, etc.\n",
    "- **Electronic Properties**: Partial charges, charge statistics\n",
    "- **Structural Counts**: Atom and bond type counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Feature Extraction Functions ---------------------- #\n",
    "def extract_fingerprint(mol, fp_type, size=1024, radius=2, **kwargs):\n",
    "    \"\"\"Generic fingerprint extraction function.\"\"\"\n",
    "    try:\n",
    "        if fp_type == 'morgan':\n",
    "            generator = rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=size)\n",
    "        elif fp_type == 'morgan_feature':\n",
    "            generator = rdFingerprintGenerator.GetMorganGenerator(\n",
    "                radius=radius, \n",
    "                fpSize=size,\n",
    "                atomInvariantsGenerator=rdFingerprintGenerator.GetMorganFeatureAtomInvGen()\n",
    "            )\n",
    "        elif fp_type == 'topological':\n",
    "            generator = rdFingerprintGenerator.GetTopologicalTorsionGenerator(fpSize=size)\n",
    "        elif fp_type == 'rdkit':\n",
    "            return np.array([int(b) for b in Chem.RDKFingerprint(mol, fpSize=size).ToBitString()])\n",
    "        elif fp_type == 'avalon':\n",
    "            return np.array([int(b) for b in pyAvalonTools.GetAvalonFP(mol, nBits=size).ToBitString()])\n",
    "        elif fp_type == 'pattern':\n",
    "            return np.array([int(b) for b in Chem.PatternFingerprint(mol, fpSize=size).ToBitString()])\n",
    "        elif fp_type == 'layered':\n",
    "            return np.array([int(b) for b in Chem.LayeredFingerprint(mol, fpSize=size).ToBitString()])\n",
    "        elif fp_type == 'maccs':\n",
    "            return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "        fp = generator.GetFingerprint(mol)\n",
    "        return np.array([int(b) for b in fp.ToBitString()])\n",
    "    except Exception:\n",
    "        return np.zeros(size, dtype=np.int32)\n",
    "\n",
    "def extract_electronic_features(mol):\n",
    "    \"\"\"Extract electronic features from molecule.\"\"\"\n",
    "    features = {}\n",
    "    electronic_features = {}\n",
    "    \n",
    "    # Try to compute Gasteiger charges\n",
    "    try:\n",
    "        AllChem.ComputeGasteigerCharges(mol)\n",
    "        charges = [atom.GetDoubleProp('_GasteigerCharge') if atom.HasProp('_GasteigerCharge') else 0.0 \n",
    "                  for atom in mol.GetAtoms()]\n",
    "        \n",
    "        if charges:\n",
    "            electronic_features['min_partial_charge'] = float(min(charges))\n",
    "            electronic_features['max_partial_charge'] = float(max(charges))\n",
    "            electronic_features['mean_partial_charge'] = float(np.mean(charges))\n",
    "            electronic_features['charge_std'] = float(np.std(charges))\n",
    "            \n",
    "            if len(charges) > 1 and electronic_features['charge_std'] > 0:\n",
    "                electronic_features['charge_skew'] = float(np.mean(\n",
    "                    [(c - electronic_features['mean_partial_charge'])**3 for c in charges]\n",
    "                ) / (electronic_features['charge_std']**3))\n",
    "                electronic_features['charge_kurtosis'] = float(np.mean(\n",
    "                    [(c - electronic_features['mean_partial_charge'])**4 for c in charges]\n",
    "                ) / (electronic_features['charge_std']**4))\n",
    "            else:\n",
    "                electronic_features['charge_skew'] = 0.0\n",
    "                electronic_features['charge_kurtosis'] = 0.0\n",
    "        else:\n",
    "            electronic_features.update({k: 0.0 for k in [\n",
    "                'min_partial_charge', 'max_partial_charge', \n",
    "                'mean_partial_charge', 'charge_std', \n",
    "                'charge_skew', 'charge_kurtosis'\n",
    "            ]})\n",
    "    except Exception:\n",
    "        electronic_features.update({k: 0.0 for k in [\n",
    "            'min_partial_charge', 'max_partial_charge', \n",
    "            'mean_partial_charge', 'charge_std', \n",
    "            'charge_skew', 'charge_kurtosis'\n",
    "        ]})\n",
    "    \n",
    "    # Try to compute other electronic properties\n",
    "    try:\n",
    "        peoe_vsa = rdMolDescriptors.PEOE_VSA_(mol)\n",
    "        electronic_features['PEOE_VSA1'] = peoe_vsa[1]\n",
    "        electronic_features['PEOE_VSA2'] = peoe_vsa[2]\n",
    "        electronic_features['PEOE_VSA3'] = peoe_vsa[3]\n",
    "        \n",
    "        crippen_contribs = rdMolDescriptors.GetCrippenContribs(mol)\n",
    "        logp_values = [c[0] for c in crippen_contribs]\n",
    "        mr_values = [c[1] for c in crippen_contribs]\n",
    "        \n",
    "        electronic_features['max_logp_contrib'] = float(max(logp_values)) if logp_values else 0.0\n",
    "        electronic_features['min_logp_contrib'] = float(min(logp_values)) if logp_values else 0.0\n",
    "        electronic_features['max_mr_contrib'] = float(max(mr_values)) if mr_values else 0.0\n",
    "        electronic_features['min_mr_contrib'] = float(min(mr_values)) if mr_values else 0.0\n",
    "    except Exception:\n",
    "        electronic_features.update({k: 0.0 for k in [\n",
    "            'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3',\n",
    "            'max_logp_contrib', 'min_logp_contrib',\n",
    "            'max_mr_contrib', 'min_mr_contrib'\n",
    "        ]})\n",
    "    \n",
    "    features['electronic_features'] = np.array(list(electronic_features.values()))\n",
    "    features['electronic_feature_names'] = list(electronic_features.keys())\n",
    "    return features\n",
    "\n",
    "def compute_features(smi, config, error_log=None):\n",
    "    \"\"\"Compute all molecular features for a given SMILES string.\"\"\"\n",
    "    if not is_valid_smiles(smi, error_log):\n",
    "        logging.warning(f\"Skipping invalid SMILES: {smi}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        features = {}\n",
    "        \n",
    "        # Extract RDKit descriptors\n",
    "        if config['features']['extract_descriptors']:\n",
    "            descriptor_names = [desc[0] for desc in Descriptors._descList]\n",
    "            calc = MoleculeDescriptors.MolecularDescriptorCalculator(descriptor_names)\n",
    "            features['descriptors'] = np.array(calc.CalcDescriptors(mol))\n",
    "            features['descriptor_names'] = descriptor_names\n",
    "        \n",
    "        # Extract fingerprints\n",
    "        for fp_name, fp_config in config['features']['fingerprints'].items():\n",
    "            if fp_config.get('enabled', False):\n",
    "                if fp_name == 'morgan':\n",
    "                    for radius in fp_config['radii']:\n",
    "                        features[f'morgan_fp_r{radius}'] = extract_fingerprint(\n",
    "                            mol, 'morgan', size=fp_config['size'], radius=radius\n",
    "                        )\n",
    "                        if radius == fp_config['radii'][0]:\n",
    "                            features['morgan_fingerprint'] = features[f'morgan_fp_r{radius}']\n",
    "                elif fp_name == 'morgan_feature':\n",
    "                    features['morgan_feature_fp'] = extract_fingerprint(\n",
    "                        mol, 'morgan_feature', size=fp_config['size'], radius=fp_config['radius']\n",
    "                    )\n",
    "                else:\n",
    "                    features[f'{fp_name}_fingerprint'] = extract_fingerprint(\n",
    "                        mol, fp_name, size=fp_config.get('size', 1024)\n",
    "                    )\n",
    "        \n",
    "        # Count features\n",
    "        if config['features']['count_features']['bond_counts']:\n",
    "            bond_counts = defaultdict(int)\n",
    "            for bond in mol.GetBonds():\n",
    "                bond_type = str(bond.GetBondType())\n",
    "                bond_counts[bond_type] += 1\n",
    "            features['bond_counts'] = np.array(list(bond_counts.values()))\n",
    "            features['bond_types'] = list(bond_counts.keys())\n",
    "            \n",
    "        if config['features']['count_features']['atom_counts']:\n",
    "            atom_counts = defaultdict(int)\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_counts[atom.GetSymbol()] += 1\n",
    "            features['atom_counts'] = np.array(list(atom_counts.values()))\n",
    "            features['atom_types'] = list(atom_counts.keys())\n",
    "        \n",
    "        # Extract electronic features\n",
    "        if config['features']['extract_electronic']:\n",
    "            features.update(extract_electronic_features(mol))\n",
    "            \n",
    "        return (smi, features)\n",
    "    except Exception as e:\n",
    "        if error_log:\n",
    "            with open(error_log, 'a') as f:\n",
    "                f.write(json.dumps({\"error\": f\"processing SMILES {smi}\", \"smiles\": smi, \"exception\": str(e)}) + \"\\n\")\n",
    "        return None\n",
    "\n",
    "def postprocess_descriptors(molecular_features):\n",
    "    \"\"\"Process descriptors to remove NaN values and zero variance columns.\"\"\"\n",
    "    if not molecular_features:\n",
    "        return molecular_features\n",
    "        \n",
    "    # Find a molecule with descriptors to get descriptor names\n",
    "    valid_smiles = []\n",
    "    descriptors_list = []\n",
    "    \n",
    "    for smi, f in molecular_features.items():\n",
    "        if 'descriptors' in f:\n",
    "            descriptors_list.append(f['descriptors'])\n",
    "            valid_smiles.append(smi)\n",
    "    \n",
    "    if not descriptors_list:\n",
    "        return molecular_features\n",
    "        \n",
    "    # Process all descriptors at once\n",
    "    descriptors_matrix = np.array(descriptors_list)\n",
    "    \n",
    "    # Check for NaN values\n",
    "    nan_mask = ~np.isnan(descriptors_matrix).any(axis=0)\n",
    "    if not np.all(nan_mask):\n",
    "        logging.info(f\"Dropping {np.sum(~nan_mask)} descriptor(s) with NaN values\")\n",
    "        descriptors_matrix = descriptors_matrix[:, nan_mask]\n",
    "        original_names = molecular_features[valid_smiles[0]]['descriptor_names']\n",
    "        new_names = [name for name, keep in zip(original_names, nan_mask) if keep]\n",
    "        \n",
    "        for smi in valid_smiles:\n",
    "            molecular_features[smi]['descriptor_names'] = new_names\n",
    "    \n",
    "    # Check for zero variance\n",
    "    var_mask = np.std(descriptors_matrix, axis=0) > 0\n",
    "    if not np.all(var_mask):\n",
    "        logging.info(f\"Dropping {np.sum(~var_mask)} descriptor(s) with 0 variance\")\n",
    "        descriptors_matrix = descriptors_matrix[:, var_mask]\n",
    "        current_names = molecular_features[valid_smiles[0]]['descriptor_names']\n",
    "        new_names = [name for name, keep in zip(current_names, var_mask) if keep]\n",
    "        \n",
    "        for smi in valid_smiles:\n",
    "            molecular_features[smi]['descriptor_names'] = new_names\n",
    "    \n",
    "    # Update the descriptors in the original dictionary\n",
    "    for i, smi in enumerate(valid_smiles):\n",
    "        molecular_features[smi]['descriptors'] = descriptors_matrix[i]\n",
    "    \n",
    "    # Memory cleanup\n",
    "    del descriptors_list\n",
    "    del descriptors_matrix\n",
    "    \n",
    "    return molecular_features\n",
    "\n",
    "print(\"Feature extraction functions loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Stage 1: Molecular Featurization\n",
    "\n",
    "Run the molecular feature extraction pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 07:09:30,479 - INFO - Loading spectral data from ../data/input/hpj/spectral_data.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: MOLECULAR FEATURIZATION\n",
      "============================================================\n",
      "Processing dataset: hpj\n",
      "Input file: ../data/input/hpj/spectral_data.jsonl\n",
      "Temporary directory: ../data/tmp/hpj\n",
      "\n",
      "Loading raw spectral data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading data: 2720it [00:00, 9886.82it/s]\n",
      "2025-06-23 07:09:30,763 - INFO - Loaded 2720 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after data loading: 206.7 MB (RSS), 0.2% of total\n",
      "\n",
      "Extracting molecular features...\n",
      "Processing 2720 molecules with -1 parallel jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing molecular features:  47%|████▋     | 1280/2720 [00:01<00:01, 1037.97it/s][07:09:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[07:09:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[07:09:32] WARNING: not removing hydrogen atom without neighbors\n",
      "Computing molecular features: 100%|██████████| 2720/2720 [00:02<00:00, 937.85it/s] \n",
      "2025-06-23 07:09:34,487 - INFO - Dropping 12 descriptor(s) with NaN values\n",
      "2025-06-23 07:09:34,491 - INFO - Dropping 9 descriptor(s) with 0 variance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully extracted features for 2720 molecules\n",
      "Failed: 0 molecules\n",
      "\n",
      "Post-processing descriptors...\n",
      "Memory usage after feature extraction: 582.9 MB (RSS), 0.4% of total\n",
      "\n",
      "Saving intermediate data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving raw spectral data: 100%|██████████| 2720/2720 [00:00<00:00, 12548.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw spectral data to ../data/tmp/hpj/raw_spectral_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving molecular features: 100%|██████████| 2720/2720 [00:01<00:00, 1623.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved molecular features to ../data/tmp/hpj/molecular_features.jsonl\n",
      "Saved configuration to ../data/tmp/hpj/dataset_config.json\n",
      "\n",
      "Stage 1 complete. All data saved to ../data/tmp/hpj\n",
      "Final memory usage: 618.1 MB (RSS), 0.5% of total\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"STAGE 1: MOLECULAR FEATURIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Validate dataset exists\n",
    "dataset_type = STAGE1_CONFIG['dataset_type']\n",
    "input_path = STAGE1_CONFIG['paths']['input_file'](dataset_type)\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"ERROR: Dataset '{dataset_type}' not found at {input_path}\")\n",
    "    print(f\"Available datasets: {[d for d in os.listdir('../data/input') if os.path.isdir(os.path.join('../data/input', d))]}\")\n",
    "else:\n",
    "    # Initialize settings\n",
    "    temp_dir = STAGE1_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "    corrupted_output = STAGE1_CONFIG['paths']['corrupted_output'](dataset_type)\n",
    "    \n",
    "    # Clear corrupted output file\n",
    "    open(corrupted_output, 'w').close()\n",
    "    \n",
    "    print(f\"Processing dataset: {dataset_type}\")\n",
    "    print(f\"Input file: {input_path}\")\n",
    "    print(f\"Temporary directory: {temp_dir}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Initialize data loader\n",
    "    data_loader = DataLoader(STAGE1_CONFIG, dataset_type)\n",
    "    \n",
    "    # Step 1: Load data\n",
    "    print(\"Loading raw spectral data...\")\n",
    "    raw_data = data_loader.load_data(input_path)\n",
    "    print(f\"Memory usage after data loading: {Utilities.get_memory_usage()}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Step 2: Extract molecular features\n",
    "    print(\"Extracting molecular features...\")\n",
    "    smiles_list = list(raw_data.keys())\n",
    "    print(f\"Processing {len(smiles_list)} molecules with {STAGE1_CONFIG['processing']['n_jobs']} parallel jobs\")\n",
    "    \n",
    "    # Parallel processing of features\n",
    "    results = Parallel(n_jobs=STAGE1_CONFIG['processing']['n_jobs'])(\n",
    "        delayed(compute_features)(smi, STAGE1_CONFIG, corrupted_output) \n",
    "        for smi in tqdm(smiles_list, desc=\"Computing molecular features\")\n",
    "    )\n",
    "    \n",
    "    # Collect results\n",
    "    molecular_features = {}\n",
    "    for result in results:\n",
    "        if result is not None:\n",
    "            smi, features = result\n",
    "            molecular_features[smi] = features\n",
    "    \n",
    "    print(f\"\\nSuccessfully extracted features for {len(molecular_features)} molecules\")\n",
    "    print(f\"Failed: {len(smiles_list) - len(molecular_features)} molecules\")\n",
    "    \n",
    "    # Process descriptors if needed\n",
    "    if STAGE1_CONFIG['features']['extract_descriptors']:\n",
    "        print(\"\\nPost-processing descriptors...\")\n",
    "        molecular_features = postprocess_descriptors(molecular_features)\n",
    "    \n",
    "    print(f\"Memory usage after feature extraction: {Utilities.get_memory_usage()}\")\n",
    "    print(\"\")\n",
    "    \n",
    "    # Step 3: Save intermediate data\n",
    "    print(\"Saving intermediate data...\")\n",
    "    \n",
    "    # Convert DataFrames to serializable format for raw_data\n",
    "    serializable_raw_data = {smiles: df.to_dict(orient='records') for smiles, df in raw_data.items()}\n",
    "    raw_data = None  # Free memory\n",
    "    \n",
    "    # Save raw spectral data\n",
    "    raw_data_path = os.path.join(temp_dir, 'raw_spectral_data.jsonl')\n",
    "    with open(raw_data_path, 'w') as f:\n",
    "        for smiles, peaks in tqdm(serializable_raw_data.items(), desc=\"Saving raw spectral data\"):\n",
    "            record = {\"smiles\": smiles, \"data\": peaks}\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "    print(f\"Saved raw spectral data to {raw_data_path}\")\n",
    "    serializable_raw_data = None  # Free memory\n",
    "    \n",
    "    # Save molecular features\n",
    "    mol_features_path = os.path.join(temp_dir, 'molecular_features.jsonl')\n",
    "    with open(mol_features_path, 'w') as f:\n",
    "        for smiles, features in tqdm(molecular_features.items(), desc=\"Saving molecular features\"):\n",
    "            record = {\"smiles\": smiles, \"data\": Utilities.convert_np_to_list(features)}\n",
    "            f.write(json.dumps(record) + \"\\n\")\n",
    "    print(f\"Saved molecular features to {mol_features_path}\")\n",
    "    \n",
    "    # Save dataset configuration\n",
    "    config_path = os.path.join(temp_dir, 'dataset_config.json')\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'dataset_type': dataset_type,\n",
    "            'stage1_complete': True,\n",
    "            'num_molecules': len(molecular_features),\n",
    "            'feature_config': STAGE1_CONFIG['features']\n",
    "        }, f, indent=2)\n",
    "    print(f\"Saved configuration to {config_path}\")\n",
    "    \n",
    "    print(f\"\\nStage 1 complete. All data saved to {temp_dir}\")\n",
    "    print(f\"Final memory usage: {Utilities.get_memory_usage()}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 1 Summary\n",
    "\n",
    "Summary of files created for Stage 2 processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STAGE 1 OUTPUT FILES:\n",
      "============================================================\n",
      "  - 3d_features.pkl: 10.96 MB\n",
      "  - corrupted_3d_records.jsonl: 0.00 MB\n",
      "  - corrupted_records.jsonl: 0.00 MB\n",
      "  - dataset_config.json: 0.00 MB\n",
      "  - dataset_config_3d.json: 0.00 MB\n",
      "  - dataset_metadata_3d.json: 0.00 MB\n",
      "  - feature_importance_3d.json: 0.00 MB\n",
      "  - molecular_features.jsonl: 111.92 MB\n",
      "  - molecular_features_3d.jsonl: 2.21 MB\n",
      "  - raw_spectral_data.jsonl: 16.40 MB\n",
      "  - raw_spectral_data_3d.jsonl: 7.40 MB\n",
      "\n",
      "These files will be used as input for Stage 2.\n",
      "Next: Run 02_spectral_processing_ml_preparation.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Display generated files\n",
    "print(\"\\nSTAGE 1 OUTPUT FILES:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "temp_dir = STAGE1_CONFIG['paths']['temp_dir'](STAGE1_CONFIG['dataset_type'])\n",
    "if os.path.exists(temp_dir):\n",
    "    files = sorted(os.listdir(temp_dir))\n",
    "    for file in files:\n",
    "        file_path = os.path.join(temp_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nThese files will be used as input for Stage 2.\")\n",
    "print(\"Next: Run 02_spectral_processing_ml_preparation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio729p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
