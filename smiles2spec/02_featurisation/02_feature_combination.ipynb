{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage 2A: Feature Combination and ML Data Preparation\n",
    "\n",
    "This notebook implements the first part of Stage 2, focusing on combining molecular features with spectral data and preparing ML-ready datasets.\n",
    "\n",
    "## Pipeline Overview:\n",
    "\n",
    "### Responsibilities:\n",
    "1. **Load Intermediate Data**: Read molecular features and spectral data from Stage 1\n",
    "2. **Process Spectra**: Apply binning for regression and preserve full resolution for seq2seq\n",
    "3. **Combine Data**: Merge molecular features with spectral data\n",
    "4. **Preprocess Features**: Scale, filter, and prepare features for ML\n",
    "5. **Split Data**: Create train/validation/test sets\n",
    "6. **Generate ML Formats**:\n",
    "   - Regression format with binned spectra\n",
    "   - Seq2seq format with PCA-reduced inputs and full resolution outputs\n",
    "7. **Save Visualization Data**: Store sample data for Stage 2B visualization\n",
    "\n",
    "### Input (from Stage 1):\n",
    "- `data/tmp/{dataset_type}/raw_spectral_data.jsonl`\n",
    "- `data/tmp/{dataset_type}/molecular_features.jsonl`\n",
    "- `data/tmp/{dataset_type}/dataset_config.json`\n",
    "\n",
    "### Output:\n",
    "#### Regression Format (`data/results/{dataset_type}/full_featurised/`):\n",
    "- `train_data.jsonl`, `val_data.jsonl`, `test_data.jsonl`\n",
    "- `feature_preprocessor.pkl` - Scaling and preprocessing state\n",
    "- `feature_mapping.jsonl` - Feature metadata\n",
    "- `visualization_data.pkl` - Sample data for Stage 2B\n",
    "\n",
    "#### Seq2Seq Format (`data/results/{dataset_type}/seq2seq_featurised/`):\n",
    "- `train_data.jsonl`, `val_data.jsonl`, `test_data.jsonl`\n",
    "- `pca_transformer.pkl` - PCA transformation matrix\n",
    "- `seq2seq_config.json` - Configuration metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries for spectral processing and ML preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import traceback\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed, dump, load\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "import psutil\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"Environment setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configuration for Stage 2A focusing on data processing and ML preparation.\n",
    "\n",
    "**Key Configuration Sections:**\n",
    "- `spectral`: Binning and peak processing parameters\n",
    "- `seq2seq`: Sequence-to-sequence model settings\n",
    "- `feature_scaling`: Scaling options for input features (molecular descriptors)\n",
    "- `target_scaling`: Scaling options for targets (spectra)\n",
    "- `data_split`: Train/validation/test split ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded for dataset: hpj\n",
      "Seq2seq generation: DISABLED\n",
      "\n",
      "Scaling Configuration:\n",
      "  Features:\n",
      "    - Continuous: False (method: standard)\n",
      "    - Log transform: False\n",
      "    - Binary: False (method: none)\n",
      "  Targets:\n",
      "    - Enabled: False (method: minmax)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Stage 2A Configuration\n",
    "STAGE2A_CONFIG = {\n",
    "    # Dataset configuration (must match Stage 1)\n",
    "    'dataset_type': 'hpj',  # Must match the dataset processed in Stage 1\n",
    "    \n",
    "    # Paths configuration\n",
    "    'paths': {\n",
    "        'data_root': '../data',\n",
    "        'temp_dir': lambda dtype: f\"../data/tmp/{dtype}\",  # Input from Stage 1\n",
    "        'results_dir': lambda dtype: f\"../data/results/{dtype}\",\n",
    "        'output_subdir': \"full_featurised\",\n",
    "        'output_dir': lambda dtype: f\"../data/results/{dtype}/full_featurised\",\n",
    "    },\n",
    "    \n",
    "    # Processing configuration\n",
    "    'processing': {\n",
    "        'n_jobs': -1,  # Use all available cores\n",
    "        'save_format': 'jsonl',\n",
    "        'save_visualization_sample': True,  # Save sample for Stage 2B\n",
    "        'visualization_sample_size': 100,\n",
    "    },\n",
    "    \n",
    "    # Spectral processing configuration\n",
    "    'spectral': {\n",
    "        'max_peaks': 499,\n",
    "        'bin_size': 1.0,  # For regression only\n",
    "        'max_mz': 499,\n",
    "        'sort_peaks_by': 'intensity',\n",
    "        'intensity_distribution_bins': 20,\n",
    "        'peak_distribution_bins': 50,\n",
    "    },\n",
    "    \n",
    "    # Seq2seq configuration\n",
    "    'seq2seq': {\n",
    "        'enabled': False,  # Toggle to enable/disable seq2seq generation\n",
    "        'pca_components': 500,  # PCA dimension for input sequence\n",
    "        'output_subdir': 'seq2seq_featurised',  # Parallel to 'full_featurised'\n",
    "        'max_sequence_length': 499,  # Same as max_peaks\n",
    "        'include_regression': True,  # Also keep regression format\n",
    "        'use_binning': False  # No binning for seq2seq\n",
    "    },\n",
    "    \n",
    "    # Feature scaling configuration (for molecular descriptors/fingerprints)\n",
    "    'feature_scaling': {\n",
    "        # Continuous features scaling\n",
    "        'scale_continuous': False,  # Whether to scale continuous features\n",
    "        'continuous_scaling_method': 'standard',  # Options: 'standard', 'minmax', 'robust', 'none'\n",
    "        'apply_log_transform': False,  # Apply sign-preserving log transform before scaling\n",
    "        \n",
    "        # Binary features scaling\n",
    "        'scale_binary': False,  # Whether to scale binary features\n",
    "        'binary_scaling_method': 'none',  # Options: 'standard', 'none'\n",
    "        \n",
    "        # Feature filtering\n",
    "        'handle_nan_strategy': 'drop_feature',  # Options: 'drop_feature', 'fill_zero', 'fill_mean'\n",
    "        'min_variance_threshold': 1e-8,\n",
    "        'auto_detect_binary': True,\n",
    "    },\n",
    "    \n",
    "    # Target scaling configuration (for spectra)\n",
    "    'target_scaling': {\n",
    "        'enabled': False,  # Whether to scale target spectra\n",
    "        'scaling_method': 'minmax',  # Options: 'standard', 'minmax', 'robust', 'none'\n",
    "        'fit_on': 'train',  # Options: 'train', 'all'\n",
    "    },\n",
    "    \n",
    "    # Data splitting configuration\n",
    "    'data_split': {\n",
    "        'random_seed': 41,\n",
    "        'train_size': 0.8,\n",
    "        'val_size': 0.1,\n",
    "        'test_size': 0.1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Configuration loaded for dataset: {STAGE2A_CONFIG['dataset_type']}\")\n",
    "print(f\"Seq2seq generation: {'ENABLED' if STAGE2A_CONFIG['seq2seq']['enabled'] else 'DISABLED'}\")\n",
    "print(\"\\nScaling Configuration:\")\n",
    "print(f\"  Features:\")\n",
    "print(f\"    - Continuous: {STAGE2A_CONFIG['feature_scaling']['scale_continuous']} (method: {STAGE2A_CONFIG['feature_scaling']['continuous_scaling_method']})\")\n",
    "print(f\"    - Log transform: {STAGE2A_CONFIG['feature_scaling']['apply_log_transform']}\")\n",
    "print(f\"    - Binary: {STAGE2A_CONFIG['feature_scaling']['scale_binary']} (method: {STAGE2A_CONFIG['feature_scaling']['binary_scaling_method']})\")\n",
    "print(f\"  Targets:\")\n",
    "print(f\"    - Enabled: {STAGE2A_CONFIG['target_scaling']['enabled']} (method: {STAGE2A_CONFIG['target_scaling']['scaling_method']})\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Validate Stage 1 Output\n",
    "\n",
    "Check that Stage 1 has been completed and all required files exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage 1 output validated successfully\n",
      "Dataset: hpj\n",
      "Number of molecules: 2720\n",
      "Stage 1 complete: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Validate Stage 1 completion\n",
    "dataset_type = STAGE2A_CONFIG['dataset_type']\n",
    "temp_dir = STAGE2A_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "\n",
    "# Check required files\n",
    "required_files = [\n",
    "    'raw_spectral_data.jsonl',\n",
    "    'molecular_features.jsonl',\n",
    "    'dataset_config.json'\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "for file in required_files:\n",
    "    file_path = os.path.join(temp_dir, file)\n",
    "    if not os.path.exists(file_path):\n",
    "        missing_files.append(file)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"ERROR: Stage 1 output incomplete for dataset '{dataset_type}'\")\n",
    "    print(f\"Missing files: {missing_files}\")\n",
    "    print(f\"Please run Stage 1 (01_molecular_featurization.ipynb) first.\")\n",
    "else:\n",
    "    # Load dataset config from Stage 1\n",
    "    with open(os.path.join(temp_dir, 'dataset_config.json'), 'r') as f:\n",
    "        stage1_config = json.load(f)\n",
    "    \n",
    "    print(f\"Stage 1 output validated successfully\")\n",
    "    print(f\"Dataset: {stage1_config['dataset_type']}\")\n",
    "    print(f\"Number of molecules: {stage1_config['num_molecules']}\")\n",
    "    print(f\"Stage 1 complete: {stage1_config.get('stage1_complete', False)}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Directory Setup\n",
    "\n",
    "Create output directories for ML-ready datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:12,160 - INFO - Created directory: ../data/results/hpj\n",
      "2025-06-23 09:05:12,161 - INFO - Created directory: ../data/results/hpj/full_featurised\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up output directories...\n",
      "Directory setup complete\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def setup_directories(dataset_type):\n",
    "    \"\"\"Create all necessary directories for Stage 2A.\"\"\"\n",
    "    dirs_to_create = [\n",
    "        STAGE2A_CONFIG['paths']['results_dir'](dataset_type),\n",
    "        STAGE2A_CONFIG['paths']['output_dir'](dataset_type)\n",
    "    ]\n",
    "    \n",
    "    # Add seq2seq directory if enabled\n",
    "    if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "        seq2seq_dir = os.path.join(\n",
    "            STAGE2A_CONFIG['paths']['results_dir'](dataset_type),\n",
    "            STAGE2A_CONFIG['seq2seq']['output_subdir']\n",
    "        )\n",
    "        dirs_to_create.append(seq2seq_dir)\n",
    "    \n",
    "    for dir_path in dirs_to_create:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        logger.info(f\"Created directory: {dir_path}\")\n",
    "\n",
    "# Setup directories\n",
    "print(\"Setting up output directories...\")\n",
    "setup_directories(STAGE2A_CONFIG['dataset_type'])\n",
    "print(\"Directory setup complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Utility Functions and Base Classes\n",
    "\n",
    "Core utilities needed for Stage 2A processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions and classes loaded\n",
      "Initial memory usage: 213.7 MB (RSS), 0.2% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Utilities ---------------------- #\n",
    "class Utilities:\n",
    "    @staticmethod\n",
    "    def get_memory_usage():\n",
    "        \"\"\"Get current memory usage in MB.\"\"\"\n",
    "        process = psutil.Process(os.getpid())\n",
    "        mem_info = process.memory_info()\n",
    "        return f\"{mem_info.rss / (1024 * 1024):.1f} MB (RSS), {process.memory_percent():.1f}% of total\"\n",
    "\n",
    "    @staticmethod\n",
    "    def ensure_numpy_array(data):\n",
    "        \"\"\"Ensure data is a numpy array.\"\"\"\n",
    "        if isinstance(data, list):\n",
    "            return np.array(data)\n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_np_to_list(item):\n",
    "        \"\"\"Recursively convert numpy arrays to lists for JSON serialization.\"\"\"\n",
    "        if isinstance(item, np.ndarray):\n",
    "            return item.tolist()\n",
    "        elif isinstance(item, dict):\n",
    "            return {k: Utilities.convert_np_to_list(v) for k, v in item.items()}\n",
    "        elif isinstance(item, list):\n",
    "            return [Utilities.convert_np_to_list(v) for v in item]\n",
    "        else:\n",
    "            return item\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_scaler(method):\n",
    "        \"\"\"Get sklearn scaler based on method name.\"\"\"\n",
    "        scalers = {\n",
    "            'standard': StandardScaler(),\n",
    "            'minmax': MinMaxScaler(),\n",
    "            'robust': RobustScaler(),\n",
    "            'none': None\n",
    "        }\n",
    "        return scalers.get(method, StandardScaler())\n",
    "\n",
    "# ---------------------- Error Handling Mixin ---------------------- #\n",
    "class ErrorHandlingMixin:\n",
    "    \"\"\"Provides standardized error handling for pipeline components.\"\"\"\n",
    "    \n",
    "    def handle_error(self, error, context=\"\", data=None):\n",
    "        \"\"\"Centralized error handling.\"\"\"\n",
    "        message = f\"Error in {self.__class__.__name__}\"\n",
    "        if context:\n",
    "            message += f\" ({context})\"\n",
    "        message += f\": {error}\"\n",
    "        \n",
    "        logger.error(message)\n",
    "        return None\n",
    "\n",
    "# ---------------------- Feature Metadata ---------------------- #\n",
    "@dataclass\n",
    "class FeatureMetadata:\n",
    "    feature_names: List[str]\n",
    "    feature_types: List[str]\n",
    "    segment_lengths: List[int]\n",
    "    valid_mask: Optional[np.ndarray] = field(default=None)\n",
    "\n",
    "    def total_features(self):\n",
    "        return sum(self.segment_lengths)\n",
    "\n",
    "print(\"Utility functions and classes loaded\")\n",
    "print(f\"Initial memory usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Loading Components\n",
    "\n",
    "Load intermediate data from Stage 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading components initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DataLoader ---------------------- #\n",
    "class DataLoader(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "    \n",
    "    def load_data_from_jsonl(self, file_path):\n",
    "        \"\"\"Load data from JSONL file.\"\"\"\n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        data = {}\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in tqdm(f, desc=f\"Loading {os.path.basename(file_path)}\"):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    smiles = record.get(\"smiles\")\n",
    "                    data_content = record.get(\"data\", {})\n",
    "                    data[smiles] = data_content\n",
    "                except Exception as e:\n",
    "                    self.handle_error(e, f\"loading JSON line from {file_path}\")\n",
    "        logger.info(f\"Loaded {len(data)} records from {file_path}\")\n",
    "        return data\n",
    "    \n",
    "    def convert_raw_data_to_df(self, raw_data):\n",
    "        \"\"\"Convert raw data records to pandas DataFrames.\"\"\"\n",
    "        df_data = {}\n",
    "        for smiles, peaks in raw_data.items():\n",
    "            df_data[smiles] = pd.DataFrame(peaks, columns=[\"mz\", \"intensity\"])\n",
    "        return df_data\n",
    "\n",
    "print(\"Data loading components initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spectral Processing Components\n",
    "\n",
    "Process mass spectra with binning for regression and full resolution for seq2seq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral processing components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- SpectralProcessor ---------------------- #\n",
    "class SpectralProcessor(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.max_observed_mz = 0\n",
    "        \n",
    "    def process(self, raw_spectra, use_binning=True):\n",
    "        \"\"\"\n",
    "        Process spectral data with optional binning.\n",
    "        \n",
    "        Args:\n",
    "            raw_spectra: Dictionary of SMILES -> peak DataFrame\n",
    "            use_binning: Whether to bin the spectra (True for regression, False for seq2seq)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Processing spectral sequences (binning={'enabled' if use_binning else 'disabled'})...\")\n",
    "        \n",
    "        results = Parallel(n_jobs=self.config['processing']['n_jobs'])(\n",
    "            delayed(self._process_one_spectrum)(smi, comp, use_binning) \n",
    "            for smi, comp in tqdm(list(raw_spectra.items()), desc=\"Processing spectra\")\n",
    "        )\n",
    "        \n",
    "        # Memory-optimized: directly build dictionary\n",
    "        processed_sequences = {}\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                smi, proc = result\n",
    "                processed_sequences[smi] = proc\n",
    "        \n",
    "        # Calculate the maximum observed m/z value across all spectra\n",
    "        self.max_observed_mz = max([data.get('true_max_mz', 0) for data in processed_sequences.values()], default=0)\n",
    "        logger.info(f\"Maximum observed m/z across all spectra: {self.max_observed_mz}\")\n",
    "        \n",
    "        if use_binning:\n",
    "            logger.info(f\"Configured max_mz for binning: {self.config['spectral']['max_mz']}\")\n",
    "            \n",
    "            # Find what percentage of spectra have peaks beyond the max_mz\n",
    "            spectra_with_peaks_beyond_max = sum(1 for data in processed_sequences.values() \n",
    "                                                if data.get('true_max_mz', 0) > self.config['spectral']['max_mz'])\n",
    "            if processed_sequences:\n",
    "                percentage_beyond = (spectra_with_peaks_beyond_max / len(processed_sequences)) * 100\n",
    "                logger.info(f\"{spectra_with_peaks_beyond_max} spectra ({percentage_beyond:.2f}%) have peaks beyond configured max_mz\")\n",
    "        \n",
    "        logger.info(f\"Processed spectral sequences for {len(processed_sequences)} compounds\")\n",
    "        return processed_sequences\n",
    "    \n",
    "    def _process_one_spectrum(self, smiles, compound_data, use_binning):\n",
    "        \"\"\"Process a single spectrum with optional binning.\"\"\"\n",
    "        data = compound_data.copy()\n",
    "        try:\n",
    "            # Initialize required result variables\n",
    "            peak_array = np.zeros((self.config['spectral']['max_peaks'], 2), dtype=np.float32)\n",
    "            attention_mask = np.zeros(self.config['spectral']['max_peaks'], dtype=np.float32)\n",
    "            original_peak_count = 0\n",
    "            peak_distribution = np.zeros(self.config['spectral']['intensity_distribution_bins'], dtype=np.float32)\n",
    "            max_mz_value = 0\n",
    "            \n",
    "            # Capture the true maximum m/z value before any filtering\n",
    "            true_max_mz = data['mz'].max() if not data.empty else 0\n",
    "            \n",
    "            # Apply binning only if requested (for regression)\n",
    "            if use_binning and self.config['spectral']['bin_size'] and not data.empty:\n",
    "                data['binned_mz'] = (data['mz'] / self.config['spectral']['bin_size']).astype(int) * self.config['spectral']['bin_size']\n",
    "                data = data.groupby('binned_mz').agg({'intensity': 'max'}).reset_index().rename(columns={'binned_mz': 'mz'})\n",
    "            \n",
    "            # Count peaks beyond max_mz (only relevant for binned data)\n",
    "            peaks_beyond_max = 0\n",
    "            if use_binning:\n",
    "                peaks_beyond_max = sum(1 for mz in data['mz'] if mz > self.config['spectral']['max_mz']) if not data.empty else 0\n",
    "            \n",
    "            # Process peaks if data is not empty\n",
    "            if not data.empty:\n",
    "                sorted_peaks = data.sort_values(self.config['spectral']['sort_peaks_by'], ascending=False)\n",
    "                max_intensity = sorted_peaks['intensity'].max() if not sorted_peaks.empty else 0\n",
    "                if max_intensity > 0:\n",
    "                    sorted_peaks['intensity'] /= max_intensity\n",
    "                \n",
    "                peak_sequence = list(zip(sorted_peaks['mz'], sorted_peaks['intensity']))\n",
    "                original_peak_count = len(peak_sequence)\n",
    "                \n",
    "                if len(peak_sequence) > 0:\n",
    "                    # Process peak sequences\n",
    "                    if len(peak_sequence) > self.config['spectral']['max_peaks']:\n",
    "                        attention_mask = np.ones(self.config['spectral']['max_peaks'])\n",
    "                        peak_sequence = peak_sequence[:self.config['spectral']['max_peaks']]\n",
    "                    else:\n",
    "                        attention_mask = np.concatenate([\n",
    "                            np.ones(len(peak_sequence)), \n",
    "                            np.zeros(self.config['spectral']['max_peaks'] - len(peak_sequence))\n",
    "                        ])\n",
    "                        peak_sequence += [(0.0, 0.0)] * (self.config['spectral']['max_peaks'] - len(peak_sequence))\n",
    "                    \n",
    "                    peak_array = np.array(peak_sequence, dtype=np.float32)\n",
    "                    peak_distribution = self._create_intensity_distribution(\n",
    "                        peak_array, attention_mask, self.config['spectral']['intensity_distribution_bins']\n",
    "                    )\n",
    "                    \n",
    "                    max_mz_value = sorted_peaks['mz'].max() if not sorted_peaks.empty else 0\n",
    "            \n",
    "            # Free up memory\n",
    "            del data\n",
    "            \n",
    "            result = {\n",
    "                'peaks': peak_array,\n",
    "                'attention_mask': attention_mask,\n",
    "                'original_peak_count': original_peak_count,\n",
    "                'intensity_distribution': peak_distribution,\n",
    "                'max_mz': max_mz_value,\n",
    "                'true_max_mz': true_max_mz,\n",
    "                'binned': use_binning\n",
    "            }\n",
    "            \n",
    "            if use_binning:\n",
    "                result.update({\n",
    "                    'peaks_beyond_max': peaks_beyond_max,\n",
    "                    'bin_size': self.config['spectral']['bin_size']\n",
    "                })\n",
    "            \n",
    "            return smiles, result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return self.handle_error(e, f\"processing spectrum for {smiles}\", data={\"smiles\": smiles})\n",
    "    \n",
    "    def _create_intensity_distribution(self, peak_array, mask, num_bins):\n",
    "        \"\"\"Calculate intensity distribution histogram.\"\"\"\n",
    "        valid_peaks = peak_array[mask.astype(bool)]\n",
    "        if len(valid_peaks) == 0:\n",
    "            return np.zeros(num_bins)\n",
    "        hist, _ = np.histogram(valid_peaks[:, 1], bins=num_bins, range=(0, 1))\n",
    "        return hist / np.sum(hist) if np.sum(hist) > 0 else hist\n",
    "\n",
    "print(\"Spectral processing components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Combination and Binning Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data combination and binning components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- DataCombiner ---------------------- #\n",
    "class DataCombiner(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def process(self, spectra_sequences, molecular_features):\n",
    "        \"\"\"Combine spectral and molecular data.\"\"\"\n",
    "        combined_data = {}\n",
    "        missing_features = 0\n",
    "        \n",
    "        for smiles, spectral_data in tqdm(spectra_sequences.items(), desc=\"Combining data\"):\n",
    "            if smiles in molecular_features:\n",
    "                combined_data[smiles] = self._combine_single_molecule(smiles, spectral_data, molecular_features[smiles])\n",
    "            else:\n",
    "                missing_features += 1\n",
    "                \n",
    "        logger.info(f\"Combined data: {len(combined_data)} compounds, missing features for {missing_features}\")\n",
    "        return combined_data\n",
    "    \n",
    "    def _combine_single_molecule(self, smiles, spectral_data, features):\n",
    "        \"\"\"Combine data for a single molecule.\"\"\"\n",
    "        molecular_weight = exact_mass = 0\n",
    "        \n",
    "        if 'descriptors' in features and 'descriptor_names' in features:\n",
    "            try:\n",
    "                molwt_idx = features['descriptor_names'].index('MolWt')\n",
    "                molecular_weight = float(features['descriptors'][molwt_idx])\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "            try:\n",
    "                exactwt_idx = features['descriptor_names'].index('ExactMolWt')\n",
    "                exact_mass = float(features['descriptors'][exactwt_idx])\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "                \n",
    "        # Combine all data\n",
    "        combined = {}\n",
    "        # First add spectral data\n",
    "        for key, value in spectral_data.items():\n",
    "            combined[key] = value\n",
    "        # Then add features\n",
    "        for key, value in features.items():\n",
    "            combined[key] = value\n",
    "        combined['molecular_weight'] = molecular_weight\n",
    "        combined['exact_mass'] = exact_mass\n",
    "        \n",
    "        return combined\n",
    "\n",
    "# ---------------------- BinningProcessor ---------------------- #\n",
    "class BinningProcessor(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.max_observed_mz = 0\n",
    "        \n",
    "    def process(self, combined_data):\n",
    "        \"\"\"Create binned representations for regression.\"\"\"\n",
    "        # First find the maximum observed m/z value across all spectra\n",
    "        self.max_observed_mz = max([data.get('true_max_mz', 0) for data in combined_data.values()], default=0)\n",
    "        logger.info(f\"Maximum observed m/z for binning: {self.max_observed_mz}\")\n",
    "        \n",
    "        results = Parallel(n_jobs=self.config['processing']['n_jobs'])(\n",
    "            delayed(self._process_binning)(smi, data) \n",
    "            for smi, data in tqdm(list(combined_data.items()), desc=\"Binning spectra\")\n",
    "        )\n",
    "        \n",
    "        # Memory-optimized dictionary construction\n",
    "        binned_data = {}\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                smi, proc = result\n",
    "                binned_data[smi] = proc\n",
    "                \n",
    "        logger.info(f\"Created binned representations for {len(binned_data)} compounds\")\n",
    "        logger.info(f\"Memory usage after binning: {Utilities.get_memory_usage()}\")\n",
    "        \n",
    "        return binned_data\n",
    "    \n",
    "    def _process_binning(self, smiles, data):\n",
    "        \"\"\"Process binning for a single molecule.\"\"\"\n",
    "        peaks = Utilities.ensure_numpy_array(data['peaks'])\n",
    "        mask = Utilities.ensure_numpy_array(data['attention_mask'])\n",
    "        real_peaks = peaks[mask.astype(bool)]\n",
    "        \n",
    "        # Create binned spectrum up to configured max_mz\n",
    "        configured_num_bins = int(self.config['spectral']['max_mz'] / self.config['spectral']['bin_size']) + 1\n",
    "        binned_spectrum = np.zeros(configured_num_bins)\n",
    "        \n",
    "        # Create extended binned spectrum up to observed max_mz (for visualization)\n",
    "        extended_num_bins = int(max(self.max_observed_mz * 1.1, self.config['spectral']['max_mz']) / \n",
    "                              self.config['spectral']['bin_size']) + 1\n",
    "        extended_binned_spectrum = np.zeros(extended_num_bins)\n",
    "        \n",
    "        for mz, intensity in real_peaks:\n",
    "            # For the standard binned spectrum (used in the pipeline)\n",
    "            if mz < self.config['spectral']['max_mz']:\n",
    "                bin_idx = min(int(mz / self.config['spectral']['bin_size']), configured_num_bins - 1)\n",
    "                binned_spectrum[bin_idx] = max(binned_spectrum[bin_idx], intensity)\n",
    "            \n",
    "            # For the extended binned spectrum (used in visualization)\n",
    "            extended_bin_idx = min(int(mz / self.config['spectral']['bin_size']), extended_num_bins - 1)\n",
    "            extended_binned_spectrum[extended_bin_idx] = max(extended_binned_spectrum[extended_bin_idx], intensity)\n",
    "        \n",
    "        # Count peaks and intensity beyond max_mz\n",
    "        peaks_beyond_max = [(mz, intensity) for mz, intensity in real_peaks if mz > self.config['spectral']['max_mz']]\n",
    "        total_intensity_beyond_max = sum(intensity for _, intensity in peaks_beyond_max)\n",
    "        total_intensity = sum(intensity for _, intensity in real_peaks)\n",
    "        intensity_percentage_beyond_max = (total_intensity_beyond_max / total_intensity * 100) if total_intensity > 0 else 0\n",
    "        \n",
    "        # Create a new data dictionary\n",
    "        updated = {}\n",
    "        # Copy original data that's needed\n",
    "        for key in ['peaks', 'attention_mask', 'original_peak_count', \n",
    "                   'intensity_distribution', 'max_mz', 'true_max_mz', \n",
    "                   'peaks_beyond_max', 'bin_size', \n",
    "                   'molecular_weight', 'exact_mass']:\n",
    "            if key in data:\n",
    "                updated[key] = data[key]\n",
    "                \n",
    "        # Add new binning data\n",
    "        updated['binned_spectrum'] = binned_spectrum\n",
    "        updated['extended_binned_spectrum'] = extended_binned_spectrum\n",
    "        updated['peaks_beyond_max_count'] = len(peaks_beyond_max)\n",
    "        updated['intensity_beyond_max'] = total_intensity_beyond_max\n",
    "        updated['intensity_percentage_beyond_max'] = intensity_percentage_beyond_max\n",
    "        updated['extended_max_mz'] = extended_num_bins * self.config['spectral']['bin_size']\n",
    "        \n",
    "        # Keep molecular feature data\n",
    "        for key in data:\n",
    "            if key not in updated and (key.endswith('fingerprint') or key.endswith('descriptors') or \n",
    "                                      key.endswith('feature_names') or key.endswith('features')):\n",
    "                updated[key] = data[key]\n",
    "        \n",
    "        return smiles, updated\n",
    "\n",
    "print(\"Data combination and binning components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Preparation Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- RegressionDataPreparer ---------------------- #\n",
    "class RegressionDataPreparer(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.target_scaler = None\n",
    "    \n",
    "    def process(self, data_dict):\n",
    "        \"\"\"Extract features and targets for regression.\"\"\"\n",
    "        logger.info(\"Extracting features and targets for regression...\")\n",
    "        \n",
    "        first_key = next(iter(data_dict.keys()), None)\n",
    "        feature_metadata = None\n",
    "        if first_key:\n",
    "            feature_metadata = self._extract_feature_info(data_dict[first_key])\n",
    "        \n",
    "        results = []\n",
    "        for smi, data in tqdm(list(data_dict.items()), desc=\"Preparing regression data\"):\n",
    "            result = self._process_regression_sample(smi, data)\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "        \n",
    "        if not results:\n",
    "            logger.error(\"No valid regression samples found!\")\n",
    "            return None\n",
    "        \n",
    "        # Unpack results in a memory-efficient way\n",
    "        smiles_list = []\n",
    "        feature_list = []\n",
    "        target_list = []\n",
    "        for result in results:\n",
    "            smiles, features, target = result\n",
    "            smiles_list.append(smiles)\n",
    "            feature_list.append(features)\n",
    "            target_list.append(target)\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        X = np.array(feature_list)\n",
    "        y = np.array(target_list)\n",
    "        \n",
    "        # Clear original results list to free memory\n",
    "        del results\n",
    "        del feature_list\n",
    "        del target_list\n",
    "        \n",
    "        logger.info(f\"Memory usage after regression data preparation: {Utilities.get_memory_usage()}\")\n",
    "        \n",
    "        if feature_metadata:\n",
    "            expected_count = feature_metadata.total_features()\n",
    "            actual_count = X.shape[1]\n",
    "            if expected_count != actual_count:\n",
    "                logger.warning(f\"Feature dimension mismatch: expected {expected_count}, got {actual_count}\")\n",
    "                feature_metadata.feature_names = [f\"feature_{i}\" for i in range(actual_count)]\n",
    "                feature_metadata.feature_types = [\"unknown\"] * actual_count\n",
    "                feature_metadata.segment_lengths = [actual_count]\n",
    "        \n",
    "        logger.info(f\"Prepared regression matrices - X: {X.shape}, y: {y.shape}\")\n",
    "        return X, y, smiles_list, feature_metadata\n",
    "    \n",
    "    def scale_targets(self, y_train, y_val=None, y_test=None):\n",
    "        \"\"\"Scale target spectra if enabled.\"\"\"\n",
    "        if not self.config['target_scaling']['enabled']:\n",
    "            logger.info(\"Target scaling disabled, returning original targets\")\n",
    "            return y_train, y_val, y_test\n",
    "        \n",
    "        logger.info(f\"Scaling targets using method: {self.config['target_scaling']['scaling_method']}\")\n",
    "        \n",
    "        # Get the appropriate scaler\n",
    "        self.target_scaler = Utilities.get_scaler(self.config['target_scaling']['scaling_method'])\n",
    "        \n",
    "        if self.target_scaler is None:\n",
    "            return y_train, y_val, y_test\n",
    "        \n",
    "        # Fit scaler on training data\n",
    "        y_train_scaled = self.target_scaler.fit_transform(y_train)\n",
    "        \n",
    "        # Transform validation and test if provided\n",
    "        y_val_scaled = self.target_scaler.transform(y_val) if y_val is not None else None\n",
    "        y_test_scaled = self.target_scaler.transform(y_test) if y_test is not None else None\n",
    "        \n",
    "        return y_train_scaled, y_val_scaled, y_test_scaled\n",
    "    \n",
    "    def _extract_feature_info(self, data) -> FeatureMetadata:\n",
    "        \"\"\"Extract feature metadata.\"\"\"\n",
    "        names = []\n",
    "        types = []\n",
    "        segments = []\n",
    "        \n",
    "        if 'descriptors' in data and data['descriptors'] is not None and 'descriptor_names' in data:\n",
    "            names.extend(data['descriptor_names'])\n",
    "            types.extend(['descriptor'] * len(data['descriptor_names']))\n",
    "            segments.append(len(data['descriptors']))\n",
    "            \n",
    "        if 'morgan_fingerprint' in data and data['morgan_fingerprint'] is not None:\n",
    "            names.extend([f'morgan_bit_{i}' for i in range(len(data['morgan_fingerprint']))])\n",
    "            types.extend(['morgan_fingerprint'] * len(data['morgan_fingerprint']))\n",
    "            segments.append(len(data['morgan_fingerprint']))\n",
    "        \n",
    "        additional_keys = ['morgan_feature_fp', 'atom_pairs', 'electronic_features', 'substructure_features',\n",
    "                           'rdkit_fingerprint', 'avalon_fingerprint', 'pattern_fingerprint', 'layered_fingerprint']\n",
    "        for key in additional_keys:\n",
    "            if key in data and data[key] is not None:\n",
    "                if f'{key}_names' in data:\n",
    "                    names.extend(data[f'{key}_names'])\n",
    "                    types.extend([key] * len(data[f'{key}_names']))\n",
    "                else:\n",
    "                    names.extend([f'{key}_{i}' for i in range(len(data[key]))])\n",
    "                    types.extend([key] * len(data[key]))\n",
    "                segments.append(len(data[key]))\n",
    "                \n",
    "        return FeatureMetadata(feature_names=names, feature_types=types, segment_lengths=segments)\n",
    "    \n",
    "    def _process_regression_sample(self, smiles, data):\n",
    "        \"\"\"Process a single regression sample.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Use a more memory-efficient approach for data conversion\n",
    "        feature_data = {}\n",
    "        for k, v in data.items():\n",
    "            if isinstance(v, (list, np.ndarray)):\n",
    "                feature_data[k] = Utilities.ensure_numpy_array(v)\n",
    "        \n",
    "        if 'descriptors' in feature_data and feature_data['descriptors'] is not None:\n",
    "            features.append(feature_data['descriptors'])\n",
    "        if 'morgan_fingerprint' in feature_data and feature_data['morgan_fingerprint'] is not None:\n",
    "            features.append(feature_data['morgan_fingerprint'])\n",
    "        \n",
    "        additional_keys = ['morgan_feature_fp', 'atom_pairs', 'electronic_features', 'substructure_features',\n",
    "                           'rdkit_fingerprint', 'avalon_fingerprint', 'pattern_fingerprint', 'layered_fingerprint']\n",
    "        for key in additional_keys:\n",
    "            if key in feature_data and feature_data[key] is not None:\n",
    "                features.append(feature_data[key])\n",
    "        \n",
    "        if not features or 'binned_spectrum' not in feature_data:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            concatenated = np.concatenate(features)\n",
    "            return (smiles, concatenated, feature_data['binned_spectrum'])\n",
    "        except Exception as e:\n",
    "            return self.handle_error(e, f\"concatenating features for {smiles}\", data={\"smiles\": smiles})\n",
    "\n",
    "# ---------------------- Seq2SeqDataPreparer ---------------------- #\n",
    "class Seq2SeqDataPreparer(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.pca = None\n",
    "        \n",
    "    def process(self, X_scaled, combined_data, smiles_list):\n",
    "        \"\"\"\n",
    "        Convert regression features to seq2seq format.\n",
    "        \n",
    "        Args:\n",
    "            X_scaled: Scaled feature matrix from regression preprocessing\n",
    "            combined_data: Dictionary with full resolution spectral data\n",
    "            smiles_list: List of SMILES strings in same order as X_scaled\n",
    "            \n",
    "        Returns:\n",
    "            List of seq2seq data dictionaries\n",
    "        \"\"\"\n",
    "        logger.info(\"Preparing seq2seq data...\")\n",
    "        \n",
    "        # Apply PCA to reduce features\n",
    "        logger.info(f\"Applying PCA to reduce features from {X_scaled.shape[1]} to {self.config['seq2seq']['pca_components']} dimensions\")\n",
    "        self.pca = PCA(n_components=self.config['seq2seq']['pca_components'])\n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "        \n",
    "        # Log explained variance\n",
    "        explained_var = np.sum(self.pca.explained_variance_ratio_)\n",
    "        logger.info(f\"PCA explained variance: {explained_var:.2%}\")\n",
    "        \n",
    "        # Extract peak sequences from combined_data\n",
    "        seq2seq_data = []\n",
    "        \n",
    "        for i, smiles in enumerate(tqdm(smiles_list, desc=\"Creating seq2seq samples\")):\n",
    "            if smiles in combined_data:\n",
    "                data = combined_data[smiles]\n",
    "                \n",
    "                # Get full resolution peaks (not binned)\n",
    "                peaks = data['peaks']  # Already (mz, intensity) pairs\n",
    "                mask = data['attention_mask']\n",
    "                \n",
    "                seq2seq_data.append({\n",
    "                    'smiles': smiles,\n",
    "                    'input_sequence': X_pca[i],  # 500D PCA features\n",
    "                    'output_sequence': peaks,     # Full resolution (mz, intensity) pairs\n",
    "                    'output_mask': mask,          # Attention mask\n",
    "                    'original_peak_count': data['original_peak_count'],\n",
    "                    'molecular_weight': data.get('molecular_weight', 0),\n",
    "                    'exact_mass': data.get('exact_mass', 0)\n",
    "                })\n",
    "        \n",
    "        logger.info(f\"Created {len(seq2seq_data)} seq2seq samples\")\n",
    "        return seq2seq_data\n",
    "    \n",
    "    def save_pca(self, filepath):\n",
    "        \"\"\"Save PCA transformer for inference.\"\"\"\n",
    "        if self.pca is not None:\n",
    "            dump(self.pca, filepath)\n",
    "            logger.info(f\"Saved PCA transformer to {filepath}\")\n",
    "        else:\n",
    "            logger.warning(\"No PCA transformer to save\")\n",
    "\n",
    "print(\"Data preparation components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preprocessor loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- FeaturePreprocessor ---------------------- #\n",
    "class FeaturePreprocessor(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.binary_feature_mask = None\n",
    "        self.valid_feature_mask = None\n",
    "        self.scaler = None\n",
    "        self.binary_scaler = None\n",
    "        self.nan_mask = None\n",
    "    \n",
    "    def _sign_preserving_log_transform(self, X):\n",
    "        \"\"\"Apply sign-preserving log transform: sign(x)*log1p(|x|).\"\"\"\n",
    "        return np.sign(X) * np.log1p(np.abs(X))\n",
    "    \n",
    "    def process(self, X, fit=True, metadata: FeatureMetadata = None):\n",
    "        \"\"\"Preprocess features with scaling and filtering.\"\"\"\n",
    "        if X is None or len(X) == 0:\n",
    "            logger.warning(\"Empty feature matrix, skipping preprocessing\")\n",
    "            return None\n",
    "            \n",
    "        X = np.array(X) if not isinstance(X, np.ndarray) else X\n",
    "        \n",
    "        if fit:\n",
    "            return self._fit_transform(X, metadata)\n",
    "        else:\n",
    "            return self._transform(X, metadata)\n",
    "    \n",
    "    def _fit_transform(self, X, metadata: FeatureMetadata = None):\n",
    "        \"\"\"Fit preprocessor and transform features.\"\"\"\n",
    "        self._handle_nans(X)\n",
    "        \n",
    "        if self.config['feature_scaling'].get('auto_detect_binary', True):\n",
    "            self._detect_binary_features(X)\n",
    "            \n",
    "        # Apply the sign-preserving log transform only to continuous features if enabled\n",
    "        X = X.copy()\n",
    "        if self.config['feature_scaling'].get('apply_log_transform', True) and self.binary_feature_mask is not None:\n",
    "            continuous_indices = np.where(~self.binary_feature_mask)[0]\n",
    "            if len(continuous_indices) > 0:\n",
    "                X[:, continuous_indices] = self._sign_preserving_log_transform(X[:, continuous_indices])\n",
    "                logger.info(f\"Applied log transform to {len(continuous_indices)} continuous features\")\n",
    "                \n",
    "        self._create_valid_feature_mask(X, metadata)\n",
    "        X_valid = X[:, self.valid_feature_mask]\n",
    "        X_scaled = self._scale_features(X_valid, fit=True)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def _transform(self, X, metadata: FeatureMetadata = None):\n",
    "        \"\"\"Transform features using fitted preprocessor.\"\"\"\n",
    "        if self.valid_feature_mask is None:\n",
    "            logger.error(\"FeaturePreprocessor not fitted before transform\")\n",
    "            return X\n",
    "            \n",
    "        X = self._handle_nan_values(X)\n",
    "        \n",
    "        # Apply the sign-preserving log transform to continuous features if enabled\n",
    "        if self.config['feature_scaling'].get('apply_log_transform', True) and self.binary_feature_mask is not None:\n",
    "            continuous_indices = np.where(~self.binary_feature_mask)[0]\n",
    "            if len(continuous_indices) > 0:\n",
    "                X[:, continuous_indices] = self._sign_preserving_log_transform(X[:, continuous_indices])\n",
    "                \n",
    "        X_valid = X[:, self.valid_feature_mask]\n",
    "        X_scaled = self._scale_features(X_valid, fit=False)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def _handle_nans(self, X):\n",
    "        \"\"\"Check for and handle NaN and infinity values.\"\"\"\n",
    "        # Check for NaN values\n",
    "        nan_counts = np.isnan(X).sum(axis=0)\n",
    "        # Check for infinity values\n",
    "        inf_counts = np.isinf(X).sum(axis=0)\n",
    "        \n",
    "        total_invalid = nan_counts + inf_counts\n",
    "        if total_invalid.sum() > 0:\n",
    "            logger.info(f\"Found {nan_counts.sum()} NaN values and {inf_counts.sum()} infinity values across {(total_invalid > 0).sum()} features\")\n",
    "        \n",
    "        # Create mask for valid values (no NaN or inf)\n",
    "        self.nan_mask = total_invalid == 0\n",
    "        \n",
    "        # Handle invalid values\n",
    "        X = self._handle_nan_values(X)\n",
    "        return X\n",
    "    \n",
    "    def _handle_nan_values(self, X):\n",
    "        \"\"\"Handle both NaN and infinity values based on strategy.\"\"\"\n",
    "        strategy = self.config['feature_scaling'].get('handle_nan_strategy', 'drop_feature')\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        if strategy == 'fill_zero':\n",
    "            # Handle both NaN and infinity\n",
    "            X_copy = np.nan_to_num(X_copy, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        elif strategy == 'fill_mean':\n",
    "            # Calculate means ignoring both NaN and infinity\n",
    "            finite_mask = np.isfinite(X_copy)\n",
    "            col_means = np.zeros(X_copy.shape[1])\n",
    "            \n",
    "            for col_idx in range(X_copy.shape[1]):\n",
    "                col_data = X_copy[:, col_idx]\n",
    "                finite_col_data = col_data[finite_mask[:, col_idx]]\n",
    "                if len(finite_col_data) > 0:\n",
    "                    col_means[col_idx] = np.mean(finite_col_data)\n",
    "            \n",
    "            # Replace both NaN and infinity with means\n",
    "            non_finite_mask = ~np.isfinite(X_copy)\n",
    "            for col_idx in range(X_copy.shape[1]):\n",
    "                X_copy[non_finite_mask[:, col_idx], col_idx] = col_means[col_idx]\n",
    "        \n",
    "        return X_copy\n",
    "    \n",
    "    def _detect_binary_features(self, X):\n",
    "        \"\"\"Detect binary features in the data.\"\"\"\n",
    "        unique_vals = [np.unique(X[:, i][np.isfinite(X[:, i])]) for i in range(X.shape[1])]\n",
    "        is_binary = [len(u) <= 2 and set(u).issubset({0, 1}) for u in unique_vals]\n",
    "        self.binary_feature_mask = np.array(is_binary)\n",
    "        binary_count = np.sum(self.binary_feature_mask)\n",
    "        if binary_count > 0:\n",
    "            logger.info(f\"Detected {binary_count} binary features\")\n",
    "    \n",
    "    def _create_valid_feature_mask(self, X, metadata: FeatureMetadata = None):\n",
    "        \"\"\"Create mask for valid features.\"\"\"\n",
    "        valid_mask = np.ones(X.shape[1], dtype=bool)\n",
    "        strategy = self.config['feature_scaling'].get('handle_nan_strategy', 'drop_feature')\n",
    "        \n",
    "        if strategy == 'drop_feature' and self.nan_mask is not None:\n",
    "            valid_mask = valid_mask & self.nan_mask\n",
    "            nan_count = np.sum(np.isnan(X).any(axis=0))\n",
    "            inf_count = np.sum(np.isinf(X).any(axis=0))\n",
    "            logger.info(f\"Dropped {np.sum(~self.nan_mask)} features with invalid values ({nan_count} with NaN, {inf_count} with infinity)\")\n",
    "        \n",
    "        variance_threshold = self.config['feature_scaling'].get('min_variance_threshold', 1e-8)\n",
    "        if variance_threshold > 0:\n",
    "            # Replace infinity with NaN so nanvar can handle it\n",
    "            X_for_var = X.copy()\n",
    "            X_for_var[np.isinf(X_for_var)] = np.nan\n",
    "            variances = np.nanvar(X_for_var, axis=0)\n",
    "            variances = np.nan_to_num(variances, nan=0.0)  # Replace any NaN variance with 0\n",
    "            \n",
    "            high_variance_mask = variances > variance_threshold\n",
    "            valid_mask = valid_mask & high_variance_mask\n",
    "            logger.info(f\"Dropped {np.sum(~high_variance_mask)} features with variance below {variance_threshold}\")\n",
    "            \n",
    "            # Free memory\n",
    "            del X_for_var\n",
    "            del variances\n",
    "        \n",
    "        self.valid_feature_mask = valid_mask\n",
    "        if metadata is not None:\n",
    "            metadata.valid_mask = valid_mask\n",
    "        \n",
    "        logger.info(f\"Final feature count: {np.sum(valid_mask)} of {X.shape[1]} original features\")\n",
    "        return valid_mask\n",
    "    \n",
    "    def _scale_features(self, X, fit=True):\n",
    "        \"\"\"Scale features with appropriate strategies.\"\"\"\n",
    "        if X is None or X.shape[0] == 0 or X.shape[1] == 0:\n",
    "            return X\n",
    "            \n",
    "        scale_continuous = self.config['feature_scaling'].get('scale_continuous', True)\n",
    "        scale_binary = self.config['feature_scaling'].get('scale_binary', False)\n",
    "        continuous_method = self.config['feature_scaling'].get('continuous_scaling_method', 'standard')\n",
    "        binary_method = self.config['feature_scaling'].get('binary_scaling_method', 'none')\n",
    "        \n",
    "        # If no scaling is required, clip and return.\n",
    "        if not scale_continuous and not scale_binary:\n",
    "            X_clipped = np.clip(X, np.finfo(np.float32).min, np.finfo(np.float32).max)\n",
    "            return X_clipped.astype(np.float32)\n",
    "        \n",
    "        # If binary feature detection is off or not available.\n",
    "        if self.binary_feature_mask is None or not self.config['feature_scaling'].get('auto_detect_binary', True):\n",
    "            if scale_continuous:\n",
    "                if fit:\n",
    "                    self.scaler = Utilities.get_scaler(continuous_method)\n",
    "                    if self.scaler is not None:\n",
    "                        X_scaled = self.scaler.fit_transform(X)\n",
    "                    else:\n",
    "                        X_scaled = X\n",
    "                else:\n",
    "                    if self.scaler is None:\n",
    "                        logger.warning(\"Scaler not fitted before transform. Returning unscaled features.\")\n",
    "                        X_scaled = X\n",
    "                    else:\n",
    "                        X_scaled = self.scaler.transform(X)\n",
    "            else:\n",
    "                X_scaled = X\n",
    "            X_clipped = np.clip(X_scaled, np.finfo(np.float32).min, np.finfo(np.float32).max)\n",
    "            return X_clipped.astype(np.float32)\n",
    "        \n",
    "        # When binary feature mask is available.\n",
    "        binary_valid = self.binary_feature_mask[self.valid_feature_mask]\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        if scale_continuous:\n",
    "            continuous_mask = ~binary_valid\n",
    "            if np.any(continuous_mask):\n",
    "                continuous_indices = np.where(continuous_mask)[0]\n",
    "                continuous_data = X[:, continuous_indices]\n",
    "                if fit:\n",
    "                    self.scaler = Utilities.get_scaler(continuous_method)\n",
    "                    if self.scaler is not None:\n",
    "                        X_processed[:, continuous_indices] = self.scaler.fit_transform(continuous_data)\n",
    "                        logger.info(f\"Applied {continuous_method} scaling to {len(continuous_indices)} continuous features\")\n",
    "                else:\n",
    "                    if self.scaler is None:\n",
    "                        logger.warning(\"Scaler not fitted before transform. Returning unscaled continuous features.\")\n",
    "                    else:\n",
    "                        X_processed[:, continuous_indices] = self.scaler.transform(continuous_data)\n",
    "                del continuous_data\n",
    "                \n",
    "        if scale_binary and np.any(binary_valid) and binary_method != 'none':\n",
    "            binary_indices = np.where(binary_valid)[0]\n",
    "            if fit:\n",
    "                self.binary_scaler = Utilities.get_scaler(binary_method)\n",
    "                if self.binary_scaler is not None:\n",
    "                    X_processed[:, binary_indices] = self.binary_scaler.fit_transform(X[:, binary_indices])\n",
    "                    logger.info(f\"Applied {binary_method} scaling to {len(binary_indices)} binary features\")\n",
    "            else:\n",
    "                if hasattr(self, 'binary_scaler') and self.binary_scaler is not None:\n",
    "                    X_processed[:, binary_indices] = self.binary_scaler.transform(X[:, binary_indices])\n",
    "                    \n",
    "        X_clipped = np.clip(X_processed, np.finfo(np.float32).min, np.finfo(np.float32).max)\n",
    "        return X_clipped.astype(np.float32)\n",
    "    \n",
    "    def update_metadata(self, metadata: FeatureMetadata):\n",
    "        \"\"\"Update the FeatureMetadata by filtering out features dropped by the valid mask.\"\"\"\n",
    "        if metadata is None or metadata.valid_mask is None:\n",
    "            logger.warning(\"No valid metadata or valid_mask provided for update.\")\n",
    "            return metadata\n",
    "            \n",
    "        filtered_names = [name for name, valid in zip(metadata.feature_names, metadata.valid_mask) if valid]\n",
    "        filtered_types = [typ for typ, valid in zip(metadata.feature_types, metadata.valid_mask) if valid]\n",
    "        updated_segment_lengths = [len(filtered_names)]\n",
    "        \n",
    "        logger.info(f\"Updated metadata: {len(filtered_names)} valid features retained out of {metadata.total_features()}.\")\n",
    "        \n",
    "        return FeatureMetadata(\n",
    "            feature_names=filtered_names,\n",
    "            feature_types=filtered_types,\n",
    "            segment_lengths=updated_segment_lengths,\n",
    "            valid_mask=np.array([True]*len(filtered_names))\n",
    "        )\n",
    "    \n",
    "    def save(self, output_path):\n",
    "        \"\"\"Save preprocessor state.\"\"\"\n",
    "        state = {\n",
    "            'binary_feature_mask': self.binary_feature_mask,\n",
    "            'valid_feature_mask': self.valid_feature_mask,\n",
    "            'scaler': self.scaler,\n",
    "            'binary_scaler': getattr(self, 'binary_scaler', None),\n",
    "            'nan_mask': getattr(self, 'nan_mask', None),\n",
    "            'config_feature_scaling': self.config['feature_scaling'],\n",
    "            'config_spectral': self.config['spectral']\n",
    "        }\n",
    "        dump(state, output_path)\n",
    "        logger.info(f\"Saved feature preprocessor to {output_path}\")\n",
    "\n",
    "print(\"Feature preprocessor loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Saving Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saving components loaded\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- FeatureMapper ---------------------- #\n",
    "class FeatureMapper(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        self.feature_map = {}\n",
    "        self.output_file = os.path.join(config['paths']['output_dir'](dataset_type), 'feature_mapping.jsonl')\n",
    "        self.full_mapping_file = os.path.join(config['paths']['output_dir'](dataset_type), 'feature_mapping_full.json')\n",
    "        \n",
    "    def collect_feature_info(self, metadata: FeatureMetadata, target_scaler=None):\n",
    "        \"\"\"Collect feature mapping information.\"\"\"\n",
    "        if metadata.valid_mask is None or len(metadata.valid_mask) != metadata.total_features():\n",
    "            logger.error(\"Mismatch between valid mask and total features in metadata.\")\n",
    "            raise ValueError(\"Inconsistent feature metadata.\")\n",
    "        \n",
    "        # Store the mapping with original indices (BEFORE filtering)\n",
    "        feature_indices = [i for i, _ in enumerate(metadata.feature_names)]\n",
    "        filtered_indices = [i for i, valid in zip(feature_indices, metadata.valid_mask) if valid]\n",
    "        filtered_names = [name for name, valid in zip(metadata.feature_names, metadata.valid_mask) if valid]\n",
    "        filtered_types = [typ for typ, valid in zip(metadata.feature_types, metadata.valid_mask) if valid]\n",
    "        \n",
    "        # Create feature map for filtered features with original indices\n",
    "        self.feature_map = {\n",
    "            i: {\n",
    "                'name': name, \n",
    "                'type': typ, \n",
    "                'new_index': i,\n",
    "                'original_index': original_idx\n",
    "            }\n",
    "            for i, (name, typ, original_idx) in enumerate(zip(filtered_names, filtered_types, filtered_indices))\n",
    "        }\n",
    "        \n",
    "        # Save complete feature information for backend\n",
    "        self.complete_feature_info = {\n",
    "            'all_feature_names': metadata.feature_names,\n",
    "            'all_feature_types': metadata.feature_types,\n",
    "            'valid_mask': metadata.valid_mask.tolist() if isinstance(metadata.valid_mask, np.ndarray) else metadata.valid_mask,\n",
    "            'filtered_indices': filtered_indices,\n",
    "            'filtered_names': filtered_names,\n",
    "            'filtered_types': filtered_types,\n",
    "            'input_dimension': len(metadata.feature_names),\n",
    "            'output_dimension': len(filtered_names),\n",
    "            'feature_scaling_config': self.config['feature_scaling'],\n",
    "            'target_scaling_config': self.config['target_scaling'],\n",
    "            'target_scaler_fitted': target_scaler is not None\n",
    "        }\n",
    "        \n",
    "    def save_mapping(self):\n",
    "        \"\"\"Save feature mapping to files.\"\"\"\n",
    "        if not self.feature_map:\n",
    "            logger.warning(\"No feature mapping to save\")\n",
    "            return\n",
    "            \n",
    "        # Save individual feature mappings as JSONL (for legacy compatibility)\n",
    "        os.makedirs(os.path.dirname(self.output_file), exist_ok=True)\n",
    "        with open(self.output_file, 'w') as f:\n",
    "            for idx, info in self.feature_map.items():\n",
    "                mapping = {\n",
    "                    'index': idx,\n",
    "                    'name': info['name'],\n",
    "                    'type': info['type'],\n",
    "                    'original_index': info['original_index']\n",
    "                }\n",
    "                f.write(json.dumps(mapping) + \"\\n\")\n",
    "                \n",
    "        # Save complete mapping information as single JSON for easier loading in backend\n",
    "        with open(self.full_mapping_file, 'w') as f:\n",
    "            json.dump(self.complete_feature_info, f, indent=2)\n",
    "                \n",
    "        logger.info(f\"Saved feature mapping for {len(self.feature_map)} features to {self.output_file}\")\n",
    "        logger.info(f\"Saved complete feature info to {self.full_mapping_file}\")\n",
    "\n",
    "# ---------------------- DataSaver ---------------------- #\n",
    "class DataSaver(ErrorHandlingMixin):\n",
    "    def __init__(self, config, dataset_type):\n",
    "        self.config = config\n",
    "        self.dataset_type = dataset_type\n",
    "        \n",
    "    def save_split(self, smiles_list, X, y, indices, filepath, is_features_scaled=True, is_targets_scaled=False):\n",
    "        \"\"\"Save regression data split.\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w') as f:\n",
    "                for idx in tqdm(indices, desc=f\"Saving to {os.path.basename(filepath)}\"):\n",
    "                    sample = {\n",
    "                        \"smiles\": smiles_list[idx],\n",
    "                        \"features\": X[idx].tolist(),\n",
    "                        \"target\": y[idx].tolist(),\n",
    "                        \"is_features_scaled\": is_features_scaled,\n",
    "                        \"is_targets_scaled\": is_targets_scaled\n",
    "                    }\n",
    "                    f.write(json.dumps(sample) + \"\\n\")\n",
    "                    # Clear sample to free memory\n",
    "                    sample = None\n",
    "            logger.info(f\"Saved {len(indices)} samples to {filepath}\")\n",
    "            return len(indices)\n",
    "        except Exception as e:\n",
    "            self.handle_error(e, f\"saving data to {filepath}\", data={\"file_path\": filepath, \"indices_count\": len(indices)})\n",
    "            return 0\n",
    "    \n",
    "    def save_seq2seq_split(self, seq2seq_data, indices, filepath):\n",
    "        \"\"\"Save seq2seq formatted data.\"\"\"\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "            with open(filepath, 'w') as f:\n",
    "                for idx in tqdm(indices, desc=f\"Saving seq2seq to {os.path.basename(filepath)}\"):\n",
    "                    sample = seq2seq_data[idx]\n",
    "                    # Convert numpy arrays to lists for JSON\n",
    "                    json_sample = {\n",
    "                        \"smiles\": sample['smiles'],\n",
    "                        \"input_sequence\": sample['input_sequence'].tolist(),\n",
    "                        \"output_sequence\": sample['output_sequence'].tolist(),\n",
    "                        \"output_mask\": sample['output_mask'].tolist(),\n",
    "                        \"original_peak_count\": sample['original_peak_count'],\n",
    "                        \"molecular_weight\": sample.get('molecular_weight', 0),\n",
    "                        \"exact_mass\": sample.get('exact_mass', 0)\n",
    "                    }\n",
    "                    f.write(json.dumps(json_sample) + \"\\n\")\n",
    "            logger.info(f\"Saved {len(indices)} seq2seq samples to {filepath}\")\n",
    "        except Exception as e:\n",
    "            self.handle_error(e, f\"saving seq2seq data to {filepath}\")\n",
    "    \n",
    "    def save_target_scaler(self, scaler, filepath):\n",
    "        \"\"\"Save target scaler if target scaling is enabled.\"\"\"\n",
    "        if scaler is not None:\n",
    "            dump(scaler, filepath)\n",
    "            logger.info(f\"Saved target scaler to {filepath}\")\n",
    "\n",
    "print(\"Data saving components loaded\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Execute Stage 2A: Process and Prepare ML Data\n",
    "\n",
    "Run the complete Stage 2A pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:12,465 - INFO - Loading data from ../data/tmp/hpj/raw_spectral_data.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2A: FEATURE COMBINATION AND ML PREPARATION\n",
      "============================================================\n",
      "Loading intermediate data from Stage 1...\n",
      "Initial memory usage: 217.2 MB (RSS), 0.2% of total\n",
      "\n",
      "Loading raw spectral data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading raw_spectral_data.jsonl: 2720it [00:00, 5052.98it/s]\n",
      "2025-06-23 09:05:13,021 - INFO - Loaded 2720 records from ../data/tmp/hpj/raw_spectral_data.jsonl\n",
      "2025-06-23 09:05:13,395 - INFO - Loading data from ../data/tmp/hpj/molecular_features.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading raw data: 307.2 MB (RSS), 0.2% of total\n",
      "\n",
      "Loading molecular features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading molecular_features.jsonl: 2720it [00:04, 627.68it/s]\n",
      "2025-06-23 09:05:17,731 - INFO - Loaded 2720 records from ../data/tmp/hpj/molecular_features.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after loading molecular features: 609.6 MB (RSS), 0.5% of total\n",
      "\n",
      "Loaded 2720 spectral records and 2720 molecule features\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"STAGE 2A: FEATURE COMBINATION AND ML PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load data from Stage 1\n",
    "print(\"Loading intermediate data from Stage 1...\")\n",
    "\n",
    "# Define paths\n",
    "dataset_type = STAGE2A_CONFIG['dataset_type']\n",
    "temp_dir = STAGE2A_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "raw_data_path = os.path.join(temp_dir, 'raw_spectral_data.jsonl')\n",
    "mol_features_path = os.path.join(temp_dir, 'molecular_features.jsonl')\n",
    "\n",
    "# Initialize data loader\n",
    "data_loader = DataLoader(STAGE2A_CONFIG, dataset_type)\n",
    "\n",
    "# Log initial memory usage\n",
    "print(f\"Initial memory usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"\")\n",
    "\n",
    "# Step 1: Load raw spectral data and convert to DataFrames\n",
    "print(\"Loading raw spectral data...\")\n",
    "raw_data_records = data_loader.load_data_from_jsonl(raw_data_path)\n",
    "raw_data = data_loader.convert_raw_data_to_df(raw_data_records)\n",
    "\n",
    "# Free up memory\n",
    "del raw_data_records\n",
    "\n",
    "print(f\"Memory usage after loading raw data: {Utilities.get_memory_usage()}\")\n",
    "\n",
    "# Step 2: Load molecular features\n",
    "print(\"\\nLoading molecular features...\")\n",
    "molecular_features = data_loader.load_data_from_jsonl(mol_features_path)\n",
    "\n",
    "print(f\"Memory usage after loading molecular features: {Utilities.get_memory_usage()}\")\n",
    "print(f\"\\nLoaded {len(raw_data)} spectral records and {len(molecular_features)} molecule features\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing processing components...\n",
      "Components initialized\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize all processing components\n",
    "print(\"Initializing processing components...\")\n",
    "spectral_processor = SpectralProcessor(STAGE2A_CONFIG, dataset_type)\n",
    "data_combiner = DataCombiner(STAGE2A_CONFIG, dataset_type)\n",
    "binning_processor = BinningProcessor(STAGE2A_CONFIG, dataset_type)\n",
    "regression_preparer = RegressionDataPreparer(STAGE2A_CONFIG, dataset_type)\n",
    "feature_preprocessor = FeaturePreprocessor(STAGE2A_CONFIG, dataset_type)\n",
    "data_saver = DataSaver(STAGE2A_CONFIG, dataset_type)\n",
    "feature_mapper = FeatureMapper(STAGE2A_CONFIG, dataset_type)\n",
    "\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    seq2seq_preparer = Seq2SeqDataPreparer(STAGE2A_CONFIG, dataset_type)\n",
    "    print(\"Seq2seq data preparer initialized\")\n",
    "\n",
    "print(\"Components initialized\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:17,749 - INFO - Processing spectral sequences (binning=enabled)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing spectral data...\n",
      "Processing binned spectra for regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing spectra: 100%|| 2720/2720 [00:07<00:00, 386.07it/s] \n",
      "2025-06-23 09:05:25,887 - INFO - Maximum observed m/z across all spectra: 683.0\n",
      "2025-06-23 09:05:25,888 - INFO - Configured max_mz for binning: 499\n",
      "2025-06-23 09:05:25,889 - INFO - 10 spectra (0.37%) have peaks beyond configured max_mz\n",
      "2025-06-23 09:05:25,891 - INFO - Processed spectral sequences for 2720 compounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after spectral processing: 646.0 MB (RSS), 0.5% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Process spectral data\n",
    "print(\"Processing spectral data...\")\n",
    "\n",
    "# For regression, we need binned spectra\n",
    "print(\"Processing binned spectra for regression...\")\n",
    "spectra_sequences_binned = spectral_processor.process(raw_data, use_binning=True)\n",
    "\n",
    "# For seq2seq, we need full resolution spectra\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"\\nProcessing full resolution spectra for seq2seq...\")\n",
    "    spectra_sequences_full = spectral_processor.process(raw_data, use_binning=False)\n",
    "\n",
    "# Free up raw data after processing\n",
    "del raw_data\n",
    "\n",
    "print(f\"\\nMemory usage after spectral processing: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining molecular features with spectral data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining data: 100%|| 2720/2720 [00:00<00:00, 82479.68it/s]\n",
      "2025-06-23 09:05:25,948 - INFO - Combined data: 2720 compounds, missing features for 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after data combination: 647.7 MB (RSS), 0.5% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Combine data\n",
    "print(\"Combining molecular features with spectral data...\")\n",
    "\n",
    "# Combine for regression (using binned spectra)\n",
    "combined_data_binned = data_combiner.process(spectra_sequences_binned, molecular_features)\n",
    "\n",
    "# Also combine for seq2seq if enabled (using full resolution)\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    combined_data_full = data_combiner.process(spectra_sequences_full, molecular_features)\n",
    "    del spectra_sequences_full  # Free memory\n",
    "\n",
    "# Free up memory after combining\n",
    "del spectra_sequences_binned\n",
    "del molecular_features\n",
    "\n",
    "print(f\"\\nMemory usage after data combination: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:25,966 - INFO - Maximum observed m/z for binning: 683.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating binned representations for regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Binning spectra: 100%|| 2720/2720 [00:08<00:00, 316.04it/s] \n",
      "2025-06-23 09:05:34,992 - INFO - Created binned representations for 2720 compounds\n",
      "2025-06-23 09:05:34,993 - INFO - Memory usage after binning: 944.8 MB (RSS), 0.7% of total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after binning: 944.8 MB (RSS), 0.7% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Bin spectral data for regression\n",
    "print(\"Creating binned representations for regression...\")\n",
    "binned_data = binning_processor.process(combined_data_binned)\n",
    "\n",
    "# Free up the combined data after binning\n",
    "del combined_data_binned\n",
    "\n",
    "print(f\"\\nMemory usage after binning: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving visualization sample...\n",
      "Saved 100 samples for visualization to ../data/results/hpj/full_featurised/visualization_data.pkl\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Save visualization sample for Stage 2B\n",
    "if STAGE2A_CONFIG['processing']['save_visualization_sample']:\n",
    "    print(\"Saving visualization sample...\")\n",
    "    visualization_sample = {k: binned_data[k] for k in random.sample(list(binned_data.keys()), \n",
    "                                                                     min(STAGE2A_CONFIG['processing']['visualization_sample_size'], \n",
    "                                                                         len(binned_data)))}\n",
    "    \n",
    "    # Save visualization data\n",
    "    viz_data_path = os.path.join(STAGE2A_CONFIG['paths']['output_dir'](dataset_type), 'visualization_data.pkl')\n",
    "    dump(visualization_sample, viz_data_path)\n",
    "    print(f\"Saved {len(visualization_sample)} samples for visualization to {viz_data_path}\")\n",
    "    \n",
    "    # Also save spectral processor stats\n",
    "    viz_stats = {\n",
    "        'max_observed_mz': spectral_processor.max_observed_mz,\n",
    "        'binning_processor_max_mz': binning_processor.max_observed_mz,\n",
    "        'spectral_config': STAGE2A_CONFIG['spectral']\n",
    "    }\n",
    "    viz_stats_path = os.path.join(STAGE2A_CONFIG['paths']['output_dir'](dataset_type), 'visualization_stats.json')\n",
    "    with open(viz_stats_path, 'w') as f:\n",
    "        json.dump(viz_stats, f, indent=2)\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:36,111 - INFO - Extracting features and targets for regression...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing regression data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing regression data: 100%|| 2720/2720 [00:01<00:00, 1799.58it/s]\n",
      "2025-06-23 09:05:37,669 - INFO - Memory usage after regression data preparation: 1240.9 MB (RSS), 0.9% of total\n",
      "2025-06-23 09:05:37,670 - WARNING - Feature dimension mismatch: expected 7181, got 7377\n",
      "2025-06-23 09:05:37,671 - INFO - Prepared regression matrices - X: (2720, 7377), y: (2720, 500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression data prepared:\n",
      "  Feature matrix shape: (2720, 7377)\n",
      "  Target matrix shape: (2720, 500)\n",
      "  Number of samples: 2720\n",
      "\n",
      "Memory usage after regression preparation: 1240.9 MB (RSS), 0.9% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Prepare regression data\n",
    "print(\"Preparing regression data...\")\n",
    "reg_result = regression_preparer.process(binned_data)\n",
    "\n",
    "if reg_result is None:\n",
    "    print(\"ERROR: Regression data preparation failed\")\n",
    "else:\n",
    "    X, y, all_smiles, feature_metadata = reg_result\n",
    "    print(f\"\\nRegression data prepared:\")\n",
    "    print(f\"  Feature matrix shape: {X.shape}\")\n",
    "    print(f\"  Target matrix shape: {y.shape}\")\n",
    "    print(f\"  Number of samples: {len(all_smiles)}\")\n",
    "    \n",
    "    # Free up binned data after regression preparation (keep if needed for seq2seq)\n",
    "    if not STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "        del binned_data\n",
    "    \n",
    "    print(f\"\\nMemory usage after regression preparation: {Utilities.get_memory_usage()}\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:37,841 - INFO - Found 8 NaN values and 0 infinity values across 4 features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:39,107 - INFO - Detected 7206 binary features\n",
      "2025-06-23 09:05:39,174 - INFO - Dropped 4 features with invalid values (4 with NaN, 0 with infinity)\n",
      "2025-06-23 09:05:39,712 - INFO - Dropped 236 features with variance below 1e-08\n",
      "2025-06-23 09:05:39,731 - INFO - Final feature count: 7137 of 7377 original features\n",
      "2025-06-23 09:05:40,114 - INFO - Saved feature preprocessor to ../data/results/hpj/full_featurised/feature_preprocessor.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature preprocessor saved to ../data/results/hpj/full_featurised/feature_preprocessor.pkl\n",
      "\n",
      "Preprocessed feature shape: (2720, 7137)\n",
      "Memory usage after feature preprocessing: 1812.6 MB (RSS), 1.4% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Preprocess features\n",
    "print(\"Preprocessing features...\")\n",
    "X_processed = feature_preprocessor.process(X, fit=True, metadata=feature_metadata)\n",
    "\n",
    "# Save preprocessor\n",
    "output_dir = STAGE2A_CONFIG['paths']['output_dir'](dataset_type)\n",
    "preprocessor_output = os.path.join(output_dir, 'feature_preprocessor.pkl')\n",
    "feature_preprocessor.save(preprocessor_output)\n",
    "print(f\"Feature preprocessor saved to {preprocessor_output}\")\n",
    "\n",
    "print(f\"\\nPreprocessed feature shape: {X_processed.shape}\")\n",
    "print(f\"Memory usage after feature preprocessing: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train/val/test sets...\n",
      "Split ratios: train=0.8, val=0.1, test=0.1\n",
      "\n",
      "Data split complete:\n",
      "  Train: 2176 samples\n",
      "  Val: 272 samples\n",
      "  Test: 272 samples\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Split data\n",
    "print(\"Splitting data into train/val/test sets...\")\n",
    "print(f\"Split ratios: train={STAGE2A_CONFIG['data_split']['train_size']}, \"\n",
    "      f\"val={STAGE2A_CONFIG['data_split']['val_size']}, \"\n",
    "      f\"test={STAGE2A_CONFIG['data_split']['test_size']}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(STAGE2A_CONFIG['data_split']['random_seed'])\n",
    "np.random.seed(STAGE2A_CONFIG['data_split']['random_seed'])\n",
    "\n",
    "# First split into train and temp\n",
    "train_size_ratio = STAGE2A_CONFIG['data_split']['train_size']\n",
    "X_train, X_temp, y_train, y_temp, smiles_train, smiles_temp = train_test_split(\n",
    "    X_processed, y, all_smiles, \n",
    "    test_size=1-train_size_ratio, \n",
    "    random_state=STAGE2A_CONFIG['data_split']['random_seed']\n",
    ")\n",
    "\n",
    "# Then split temp into validation and test\n",
    "val_ratio = STAGE2A_CONFIG['data_split']['val_size'] / (STAGE2A_CONFIG['data_split']['val_size'] + STAGE2A_CONFIG['data_split']['test_size'])\n",
    "X_val, X_test, y_val, y_test, smiles_val, smiles_test = train_test_split(\n",
    "    X_temp, y_temp, smiles_temp,\n",
    "    test_size=1-val_ratio,\n",
    "    random_state=STAGE2A_CONFIG['data_split']['random_seed']\n",
    ")\n",
    "\n",
    "# Free temporary variables\n",
    "del X_temp, y_temp, smiles_temp\n",
    "\n",
    "print(f\"\\nData split complete:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Val: {len(X_val)} samples\")\n",
    "print(f\"  Test: {len(X_test)} samples\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:40,519 - INFO - Target scaling disabled, returning original targets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing target scaling...\n",
      "Targets scaled: False\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Scale targets if enabled\n",
    "print(\"Processing target scaling...\")\n",
    "y_train_scaled, y_val_scaled, y_test_scaled = regression_preparer.scale_targets(y_train, y_val, y_test)\n",
    "\n",
    "# Save target scaler if used\n",
    "if STAGE2A_CONFIG['target_scaling']['enabled'] and regression_preparer.target_scaler is not None:\n",
    "    target_scaler_path = os.path.join(output_dir, 'target_scaler.pkl')\n",
    "    data_saver.save_target_scaler(regression_preparer.target_scaler, target_scaler_path)\n",
    "    print(f\"Target scaler saved to {target_scaler_path}\")\n",
    "\n",
    "# Determine if targets were scaled\n",
    "targets_scaled = STAGE2A_CONFIG['target_scaling']['enabled'] and regression_preparer.target_scaler is not None\n",
    "print(f\"Targets scaled: {targets_scaled}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:40,568 - INFO - Updated metadata: 7137 valid features retained out of 7377.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating feature metadata...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 09:05:40,944 - INFO - Saved feature mapping for 7137 features to ../data/results/hpj/full_featurised/feature_mapping.jsonl\n",
      "2025-06-23 09:05:40,947 - INFO - Saved complete feature info to ../data/results/hpj/full_featurised/feature_mapping_full.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature mapping saved\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Update and save feature metadata\n",
    "print(\"Updating feature metadata...\")\n",
    "updated_metadata = feature_preprocessor.update_metadata(feature_metadata)\n",
    "feature_mapper.collect_feature_info(updated_metadata, regression_preparer.target_scaler)\n",
    "feature_mapper.save_mapping()\n",
    "print(\"Feature mapping saved\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving regression data splits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving to train_data.jsonl: 100%|| 2176/2176 [00:06<00:00, 328.68it/s]\n",
      "2025-06-23 09:05:47,849 - INFO - Saved 2176 samples to ../data/results/hpj/full_featurised/train_data.jsonl\n",
      "Saving to val_data.jsonl: 100%|| 272/272 [00:00<00:00, 454.30it/s]\n",
      "2025-06-23 09:05:48,472 - INFO - Saved 272 samples to ../data/results/hpj/full_featurised/val_data.jsonl\n",
      "Saving to test_data.jsonl: 100%|| 272/272 [00:00<00:00, 441.84it/s]\n",
      "2025-06-23 09:05:49,097 - INFO - Saved 272 samples to ../data/results/hpj/full_featurised/test_data.jsonl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression data saved to:\n",
      "  ../data/results/hpj/full_featurised/train_data.jsonl\n",
      "  ../data/results/hpj/full_featurised/val_data.jsonl\n",
      "  ../data/results/hpj/full_featurised/test_data.jsonl\n",
      "\n",
      "Saved configuration to ../data/results/hpj/full_featurised/dataset_config.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Save regression splits\n",
    "print(\"Saving regression data splits...\")\n",
    "\n",
    "# Define output paths\n",
    "train_output = os.path.join(output_dir, 'train_data.jsonl')\n",
    "val_output = os.path.join(output_dir, 'val_data.jsonl')\n",
    "test_output = os.path.join(output_dir, 'test_data.jsonl')\n",
    "\n",
    "train_indices = list(range(len(smiles_train)))\n",
    "val_indices = list(range(len(smiles_val)))\n",
    "test_indices = list(range(len(smiles_test)))\n",
    "\n",
    "# Save with scaling information\n",
    "data_saver.save_split(smiles_train, X_train, y_train_scaled, train_indices, train_output, \n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled)\n",
    "data_saver.save_split(smiles_val, X_val, y_val_scaled, val_indices, val_output,\n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled)\n",
    "data_saver.save_split(smiles_test, X_test, y_test_scaled, test_indices, test_output,\n",
    "                     is_features_scaled=True, is_targets_scaled=targets_scaled)\n",
    "\n",
    "print(\"\\nRegression data saved to:\")\n",
    "print(f\"  {train_output}\")\n",
    "print(f\"  {val_output}\")\n",
    "print(f\"  {test_output}\")\n",
    "\n",
    "# Save dataset configuration\n",
    "config_path = os.path.join(output_dir, 'dataset_config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'dataset_type': dataset_type,\n",
    "        'stage2a_complete': True,\n",
    "        'num_samples': len(all_smiles),\n",
    "        'feature_dim': X_processed.shape[1],\n",
    "        'target_dim': y.shape[1],\n",
    "        'spectral_config': STAGE2A_CONFIG['spectral'],\n",
    "        'feature_scaling_config': STAGE2A_CONFIG['feature_scaling'],\n",
    "        'target_scaling_config': STAGE2A_CONFIG['target_scaling'],\n",
    "        'features_scaled': True,\n",
    "        'targets_scaled': targets_scaled,\n",
    "        'visualization_saved': STAGE2A_CONFIG['processing']['save_visualization_sample']\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nSaved configuration to {config_path}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory usage after saving all data: 1887.7 MB (RSS), 1.4% of total\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Generate seq2seq data if enabled\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"GENERATING SEQ2SEQ DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process seq2seq data\n",
    "    seq2seq_data = seq2seq_preparer.process(X_processed, combined_data_full, all_smiles)\n",
    "    \n",
    "    # Split seq2seq data using same indices as regression\n",
    "    # Create mapping from SMILES to indices\n",
    "    smiles_to_idx = {smiles: idx for idx, smiles in enumerate(all_smiles)}\n",
    "    \n",
    "    # Get indices for each split\n",
    "    train_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_train]\n",
    "    val_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_val]\n",
    "    test_indices_seq2seq = [smiles_to_idx[smiles] for smiles in smiles_test]\n",
    "    \n",
    "    # Create seq2seq splits\n",
    "    seq2seq_train = [seq2seq_data[i] for i in train_indices_seq2seq]\n",
    "    seq2seq_val = [seq2seq_data[i] for i in val_indices_seq2seq]\n",
    "    seq2seq_test = [seq2seq_data[i] for i in test_indices_seq2seq]\n",
    "    \n",
    "    # Define seq2seq output paths\n",
    "    seq2seq_output_dir = os.path.join(\n",
    "        STAGE2A_CONFIG['paths']['results_dir'](dataset_type),\n",
    "        STAGE2A_CONFIG['seq2seq']['output_subdir']\n",
    "    )\n",
    "    os.makedirs(seq2seq_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nSaving seq2seq data to {seq2seq_output_dir}...\")\n",
    "    \n",
    "    # Save seq2seq splits\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_train, \n",
    "        list(range(len(seq2seq_train))), \n",
    "        os.path.join(seq2seq_output_dir, 'train_data.jsonl')\n",
    "    )\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_val,\n",
    "        list(range(len(seq2seq_val))),\n",
    "        os.path.join(seq2seq_output_dir, 'val_data.jsonl')\n",
    "    )\n",
    "    data_saver.save_seq2seq_split(\n",
    "        seq2seq_test,\n",
    "        list(range(len(seq2seq_test))),\n",
    "        os.path.join(seq2seq_output_dir, 'test_data.jsonl')\n",
    "    )\n",
    "    \n",
    "    # Save PCA transformer\n",
    "    pca_path = os.path.join(seq2seq_output_dir, 'pca_transformer.pkl')\n",
    "    seq2seq_preparer.save_pca(pca_path)\n",
    "    \n",
    "    # Save seq2seq config\n",
    "    seq2seq_config = {\n",
    "        'pca_components': STAGE2A_CONFIG['seq2seq']['pca_components'],\n",
    "        'max_sequence_length': STAGE2A_CONFIG['seq2seq']['max_sequence_length'],\n",
    "        'input_dim': STAGE2A_CONFIG['seq2seq']['pca_components'],\n",
    "        'output_dim': 2,  # (mz, intensity)\n",
    "        'dataset_type': dataset_type,\n",
    "        'pca_explained_variance': float(np.sum(seq2seq_preparer.pca.explained_variance_ratio_))\n",
    "    }\n",
    "    \n",
    "    seq2seq_config_path = os.path.join(seq2seq_output_dir, 'seq2seq_config.json')\n",
    "    with open(seq2seq_config_path, 'w') as f:\n",
    "        json.dump(seq2seq_config, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nSeq2seq data saved to {seq2seq_output_dir}\")\n",
    "    print(f\"PCA explained variance: {seq2seq_config['pca_explained_variance']:.2%}\")\n",
    "    \n",
    "    # Clean up\n",
    "    del combined_data_full\n",
    "    del seq2seq_data\n",
    "    del seq2seq_train\n",
    "    del seq2seq_val\n",
    "    del seq2seq_test\n",
    "\n",
    "# Free up memory after saving data splits\n",
    "del X, y, X_processed\n",
    "del X_train, y_train, smiles_train, y_train_scaled\n",
    "del X_val, y_val, smiles_val, y_val_scaled\n",
    "del X_test, y_test, smiles_test, y_test_scaled\n",
    "\n",
    "print(f\"\\nMemory usage after saving all data: {Utilities.get_memory_usage()}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Pipeline Summary\n",
    "\n",
    "Display final summary of Stage 2A processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STAGE 2A SUMMARY\n",
      "============================================================\n",
      "  Dataset Type: hpj\n",
      "  Total Samples: 2720\n",
      "\n",
      "  Feature Dimensions: 7137\n",
      "\n",
      "  Max Observed m/z: 683.0 (configured max_mz: 499)\n",
      "\n",
      "  Scaling Configuration:\n",
      "    Features:\n",
      "      - Continuous: False (method: standard)\n",
      "      - Log transform: False\n",
      "      - Binary: False (method: none)\n",
      "    Targets:\n",
      "      - Enabled: False (method: minmax)\n",
      "      - Applied: False\n",
      "\n",
      "  Regression Output Files:\n",
      "    - ../data/results/hpj/full_featurised/train_data.jsonl\n",
      "    - ../data/results/hpj/full_featurised/val_data.jsonl\n",
      "    - ../data/results/hpj/full_featurised/test_data.jsonl\n",
      "    - ../data/results/hpj/full_featurised/feature_preprocessor.pkl\n",
      "    - ../data/results/hpj/full_featurised/feature_mapping.jsonl\n",
      "\n",
      "  Visualization Files:\n",
      "    - ../data/results/hpj/full_featurised/visualization_data.pkl\n",
      "    - ../data/results/hpj/full_featurised/visualization_stats.json\n",
      "\n",
      "  Final Memory Usage: 1887.7 MB (RSS), 1.4% of total\n",
      "\n",
      "============================================================\n",
      "STAGE 2A COMPLETE!\n",
      "Next: Run 02b_featurization_visualization.ipynb for visualizations\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Log summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2A SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "feature_dimensions = \"\"\n",
    "if feature_preprocessor.valid_feature_mask is not None:\n",
    "    feature_dimensions = f\"\\n  Feature Dimensions: {np.sum(feature_preprocessor.valid_feature_mask)}\"\n",
    "    \n",
    "# Include statistics about m/z range coverage in summary\n",
    "max_observed_mz_info = \"\"\n",
    "if hasattr(spectral_processor, 'max_observed_mz') and spectral_processor.max_observed_mz > 0:\n",
    "    max_observed_mz_info = (f\"\\n  Max Observed m/z: {spectral_processor.max_observed_mz:.1f} \" +\n",
    "                           f\"(configured max_mz: {STAGE2A_CONFIG['spectral']['max_mz']})\")\n",
    "    \n",
    "print(f\"  Dataset Type: {dataset_type}\")\n",
    "print(f\"  Total Samples: {len(all_smiles)}\")\n",
    "print(feature_dimensions)\n",
    "print(max_observed_mz_info)\n",
    "\n",
    "print(f\"\\n  Scaling Configuration:\")\n",
    "print(f\"    Features:\")\n",
    "print(f\"      - Continuous: {STAGE2A_CONFIG['feature_scaling']['scale_continuous']} (method: {STAGE2A_CONFIG['feature_scaling']['continuous_scaling_method']})\")\n",
    "print(f\"      - Log transform: {STAGE2A_CONFIG['feature_scaling']['apply_log_transform']}\")\n",
    "print(f\"      - Binary: {STAGE2A_CONFIG['feature_scaling']['scale_binary']} (method: {STAGE2A_CONFIG['feature_scaling']['binary_scaling_method']})\")\n",
    "print(f\"    Targets:\")\n",
    "print(f\"      - Enabled: {STAGE2A_CONFIG['target_scaling']['enabled']} (method: {STAGE2A_CONFIG['target_scaling']['scaling_method']})\")\n",
    "print(f\"      - Applied: {targets_scaled}\")\n",
    "\n",
    "print(f\"\\n  Regression Output Files:\")\n",
    "print(f\"    - {train_output}\")\n",
    "print(f\"    - {val_output}\")\n",
    "print(f\"    - {test_output}\")\n",
    "print(f\"    - {preprocessor_output}\")\n",
    "print(f\"    - {feature_mapper.output_file}\")\n",
    "if targets_scaled:\n",
    "    print(f\"    - {target_scaler_path}\")\n",
    "\n",
    "if STAGE2A_CONFIG['processing']['save_visualization_sample']:\n",
    "    print(f\"\\n  Visualization Files:\")\n",
    "    print(f\"    - {viz_data_path}\")\n",
    "    print(f\"    - {viz_stats_path}\")\n",
    "\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    print(f\"\\n  Seq2Seq Output Files:\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'train_data.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'val_data.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'test_data.jsonl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'pca_transformer.pkl')}\")\n",
    "    print(f\"    - {os.path.join(seq2seq_output_dir, 'seq2seq_config.json')}\")\n",
    "\n",
    "print(f\"\\n  Final Memory Usage: {Utilities.get_memory_usage()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STAGE 2A COMPLETE!\")\n",
    "print(\"Next: Run 02b_featurization_visualization.ipynb for visualizations\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Output Summary and Next Steps\n",
    "\n",
    "### Files Generated:\n",
    "\n",
    "**Regression Format (data/results/{dataset}/full_featurised/):**\n",
    "- `train_data.jsonl` - Training set with scaled features and targets\n",
    "- `val_data.jsonl` - Validation set\n",
    "- `test_data.jsonl` - Test set\n",
    "- `feature_preprocessor.pkl` - Feature scaling and preprocessing state\n",
    "- `target_scaler.pkl` - Target scaling state (if enabled)\n",
    "- `feature_mapping.jsonl` - Feature metadata for interpretability\n",
    "- `feature_mapping_full.json` - Complete feature and scaling information\n",
    "- `dataset_config.json` - Configuration metadata\n",
    "- `visualization_data.pkl` - Sample data for Stage 2B\n",
    "- `visualization_stats.json` - Statistics for Stage 2B\n",
    "\n",
    "**Seq2Seq Format (data/results/{dataset}/seq2seq_featurised/):**\n",
    "- `train_data.jsonl` - Training set with PCA features and full resolution spectra\n",
    "- `val_data.jsonl` - Validation set\n",
    "- `test_data.jsonl` - Test set\n",
    "- `pca_transformer.pkl` - PCA transformation matrix\n",
    "- `seq2seq_config.json` - Configuration and metadata\n",
    "\n",
    "### Next Steps:\n",
    "Run Stage 2B (02b_featurization_visualization.ipynb) to create comprehensive visualizations of the processed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated file sizes:\n",
      "\n",
      "Regression files:\n",
      "  - dataset_config.json: 0.00 MB\n",
      "  - feature_mapping.jsonl: 0.56 MB\n",
      "  - feature_mapping_full.json: 0.61 MB\n",
      "  - feature_preprocessor.pkl: 0.02 MB\n",
      "  - target_scaler.pkl: 0.02 MB\n",
      "  - test_data.jsonl: 10.78 MB\n",
      "  - train_data.jsonl: 86.10 MB\n",
      "  - val_data.jsonl: 10.77 MB\n",
      "  - visualization_data.pkl: 3.63 MB\n",
      "  - visualization_stats.json: 0.00 MB\n",
      "\n",
      "Temporary files (can be deleted after pipeline completion):\n",
      "  - molecular_features_3d.jsonl: 2.21 MB\n",
      "  - dataset_config.json: 0.00 MB\n",
      "  - dataset_metadata_3d.json: 0.00 MB\n",
      "  - 3d_features.pkl: 10.96 MB\n",
      "  - corrupted_records.jsonl: 0.00 MB\n",
      "  - corrupted_3d_records.jsonl: 0.00 MB\n",
      "  - dataset_config_3d.json: 0.00 MB\n",
      "  - molecular_features.jsonl: 111.92 MB\n",
      "  - feature_importance_3d.json: 0.00 MB\n",
      "  - raw_spectral_data.jsonl: 16.40 MB\n",
      "  - raw_spectral_data_3d.jsonl: 7.40 MB\n",
      "\n",
      "Total temporary storage: 148.89 MB\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Display file sizes\n",
    "import os\n",
    "\n",
    "print(\"Generated file sizes:\")\n",
    "print(\"\\nRegression files:\")\n",
    "output_dir = STAGE2A_CONFIG['paths']['output_dir'](dataset_type)\n",
    "if os.path.exists(output_dir):\n",
    "    for file in sorted(os.listdir(output_dir)):\n",
    "        file_path = os.path.join(output_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "if STAGE2A_CONFIG['seq2seq']['enabled']:\n",
    "    print(\"\\nSeq2seq files:\")\n",
    "    seq2seq_dir = os.path.join(STAGE2A_CONFIG['paths']['results_dir'](dataset_type), \n",
    "                               STAGE2A_CONFIG['seq2seq']['output_subdir'])\n",
    "    if os.path.exists(seq2seq_dir):\n",
    "        for file in sorted(os.listdir(seq2seq_dir)):\n",
    "            file_path = os.path.join(seq2seq_dir, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "                print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "\n",
    "print(\"\\nTemporary files (can be deleted after pipeline completion):\")\n",
    "temp_dir = STAGE2A_CONFIG['paths']['temp_dir'](dataset_type)\n",
    "if os.path.exists(temp_dir):\n",
    "    total_temp_size = 0\n",
    "    for file in os.listdir(temp_dir):\n",
    "        file_path = os.path.join(temp_dir, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            total_temp_size += size_mb\n",
    "            print(f\"  - {file}: {size_mb:.2f} MB\")\n",
    "    print(f\"\\nTotal temporary storage: {total_temp_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio729p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
