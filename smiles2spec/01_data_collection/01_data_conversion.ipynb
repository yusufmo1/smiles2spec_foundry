{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Conversion: MSP to JSONL for Mass Spectrometry Prediction\n",
    "\n",
    "This notebook implements the conversion pipeline from raw MSP format data to JSONL format compatible with the mass spectrometry prediction pipeline.\n",
    "\n",
    "## Overview:\n",
    "\n",
    "Mass Spectrometry Prediction (MSP) files contain spectral data in a text-based format. Each entry includes:\n",
    "- Metadata fields (Name, Comments, etc.)\n",
    "- Peak data as m/z and intensity pairs\n",
    "\n",
    "The conversion process extracts:\n",
    "$$\\text{SMILES} \\rightarrow \\text{Molecular Structure}$$\n",
    "$$\\text{Peaks} \\rightarrow [(m/z_1, I_1), (m/z_2, I_2), ..., (m/z_n, I_n)]$$\n",
    "\n",
    "Where:\n",
    "- $m/z$ = mass-to-charge ratio\n",
    "- $I$ = intensity value\n",
    "\n",
    "**Key Features:**\n",
    "- Robust SMILES extraction from various MSP formats\n",
    "- Peak data validation and normalization\n",
    "- Error tracking for corrupted records\n",
    "- Compatibility with multiple MSP sources (GNPS, MoNA, etc.)\n",
    "\n",
    "**Input**: `data/raw/{dataset_name}/*.msp`  \n",
    "**Output**: `data/input/{dataset_name}/spectral_data.jsonl`  \n",
    "**Format**: `{\"smiles\": \"...\", \"peaks\": [[mz, intensity], ...]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import required libraries and configure the conversion environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA CONVERSION: MSP TO JSONL\n",
      "================================================================================\n",
      "Timestamp: 2025-07-17 17:06:48\n",
      "\n",
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "print(\"DATA CONVERSION: MSP TO JSONL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\nEnvironment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Path Setup\n",
    "\n",
    "Define dataset configuration and directory structure.\n",
    "\n",
    "### Directory Structure:\n",
    "```\n",
    "data/\n",
    "├── raw/\n",
    "│   └── {dataset_name}/\n",
    "│       └── *.msp\n",
    "└── input/\n",
    "    └── {dataset_name}/\n",
    "        ├── spectral_data.jsonl\n",
    "        └── corrupted_records.jsonl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFIGURATION\n",
      "----------------------------------------\n",
      "Dataset: GNPS\n",
      "Raw data directory: ../data/raw/GNPS\n",
      "Output directory: ../data/input/GNPS\n",
      "\n",
      "Validation parameters:\n",
      "  - Minimum peaks: 1\n",
      "  - Maximum m/z: 5000\n",
      "  - Maximum intensity: 1e+10\n"
     ]
    }
   ],
   "source": [
    "# Conversion configuration\n",
    "CONVERSION_CONFIG = {\n",
    "    'dataset': {\n",
    "        'name': 'GNPS',  # Change to any dataset name in data/raw/\n",
    "        'msp_filename': None,  # Set to specific filename, or None to auto-detect\n",
    "    },\n",
    "    'paths': {\n",
    "        'base_dir': Path('../'),\n",
    "        'raw_data_dir': lambda base: base / 'data' / 'raw',\n",
    "        'input_data_dir': lambda base: base / 'data' / 'input',\n",
    "    },\n",
    "    'validation': {\n",
    "        'min_peaks': 1,\n",
    "        'max_mz': 5000,\n",
    "        'max_intensity': 1e10,\n",
    "        'min_smiles_length': 2,\n",
    "    },\n",
    "    'processing': {\n",
    "        'verbose': True,\n",
    "        'save_corrupted': True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Set up paths\n",
    "BASE_DIR = CONVERSION_CONFIG['paths']['base_dir']\n",
    "RAW_DATA_DIR = CONVERSION_CONFIG['paths']['raw_data_dir'](BASE_DIR)\n",
    "INPUT_DATA_DIR = CONVERSION_CONFIG['paths']['input_data_dir'](BASE_DIR)\n",
    "\n",
    "DATASET_NAME = CONVERSION_CONFIG['dataset']['name']\n",
    "DATASET_RAW_DIR = RAW_DATA_DIR / DATASET_NAME\n",
    "DATASET_INPUT_DIR = INPUT_DATA_DIR / DATASET_NAME\n",
    "\n",
    "# Create output directory\n",
    "DATASET_INPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nCONFIGURATION\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Dataset: {DATASET_NAME}\")\n",
    "print(f\"Raw data directory: {DATASET_RAW_DIR}\")\n",
    "print(f\"Output directory: {DATASET_INPUT_DIR}\")\n",
    "print(f\"\\nValidation parameters:\")\n",
    "print(f\"  - Minimum peaks: {CONVERSION_CONFIG['validation']['min_peaks']}\")\n",
    "print(f\"  - Maximum m/z: {CONVERSION_CONFIG['validation']['max_mz']}\")\n",
    "print(f\"  - Maximum intensity: {CONVERSION_CONFIG['validation']['max_intensity']:.0e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MSP File Detection\n",
    "\n",
    "Auto-detect MSP files in the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing MSP file: ../data/raw/GNPS/GNPS.msp\n"
     ]
    }
   ],
   "source": [
    "def find_msp_file(dataset_dir: Path, filename: Optional[str] = None) -> Path:\n",
    "    \"\"\"Find MSP file in dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_dir: Directory containing MSP files\n",
    "        filename: Specific filename to use, or None for auto-detection\n",
    "        \n",
    "    Returns:\n",
    "        Path to MSP file\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If no MSP file is found\n",
    "    \"\"\"\n",
    "    if filename:\n",
    "        msp_path = dataset_dir / filename\n",
    "        if msp_path.exists():\n",
    "            return msp_path\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Specified MSP file not found: {msp_path}\")\n",
    "    \n",
    "    # Auto-detect MSP files\n",
    "    msp_files = list(dataset_dir.glob('*.msp'))\n",
    "    \n",
    "    if not msp_files:\n",
    "        raise FileNotFoundError(f\"No MSP files found in {dataset_dir}\")\n",
    "    \n",
    "    if len(msp_files) == 1:\n",
    "        return msp_files[0]\n",
    "    \n",
    "    # Multiple MSP files found\n",
    "    print(f\"\\nMultiple MSP files found in {dataset_dir}:\")\n",
    "    for i, f in enumerate(msp_files):\n",
    "        print(f\"  {i+1}. {f.name}\")\n",
    "    \n",
    "    # Use the first one or the one matching dataset name\n",
    "    for f in msp_files:\n",
    "        if DATASET_NAME.lower() in f.name.lower():\n",
    "            print(f\"\\nAuto-selected: {f.name}\")\n",
    "            return f\n",
    "    \n",
    "    print(f\"\\nUsing first file: {msp_files[0].name}\")\n",
    "    return msp_files[0]\n",
    "\n",
    "# Find the MSP file to process\n",
    "msp_file_path = find_msp_file(DATASET_RAW_DIR, CONVERSION_CONFIG['dataset']['msp_filename'])\n",
    "print(f\"\\nProcessing MSP file: {msp_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MSP Parsing Functions\n",
    "\n",
    "Functions for parsing MSP format and extracting spectral data.\n",
    "\n",
    "### MSP Format Structure:\n",
    "```\n",
    "Name: Compound Name\n",
    "Comments: \"SMILES=C1CC1\" \"InChI=...\" ...\n",
    "Num Peaks: N\n",
    "mz1 intensity1\n",
    "mz2 intensity2\n",
    "...\n",
    "```\n",
    "\n",
    "### Parsing Strategy:\n",
    "1. Split entries by double newlines\n",
    "2. Extract metadata from key-value pairs\n",
    "3. Parse peak data after \"Num Peaks:\" line\n",
    "4. Extract SMILES from Comments field using regex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_msp_file(file_path: str) -> List[Dict]:\n",
    "    \"\"\"Parse MSP file and extract entries as dictionaries.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to MSP file\n",
    "        \n",
    "    Returns:\n",
    "        List of parsed entries\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    raw_entries = content.split('\\n\\n')\n",
    "    entries = []\n",
    "    \n",
    "    print(f\"\\nParsing {len(raw_entries)} potential entries...\")\n",
    "    \n",
    "    for raw_entry in tqdm(raw_entries, desc=\"Parsing MSP entries\"):\n",
    "        if not raw_entry.strip():\n",
    "            continue\n",
    "        entry = parse_single_msp_entry(raw_entry)\n",
    "        if entry:\n",
    "            entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def parse_single_msp_entry(raw_entry: str) -> Optional[Dict]:\n",
    "    \"\"\"Parse a single MSP entry.\n",
    "    \n",
    "    Args:\n",
    "        raw_entry: Raw text of single MSP entry\n",
    "        \n",
    "    Returns:\n",
    "        Parsed entry dictionary or None if parsing fails\n",
    "    \"\"\"\n",
    "    lines = raw_entry.strip().split('\\n')\n",
    "    entry = {}\n",
    "    peak_section = False\n",
    "    peaks = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('Num Peaks:'):\n",
    "            try:\n",
    "                entry['num_peaks'] = int(line.split(':', 1)[1].strip())\n",
    "                peak_section = True\n",
    "                continue\n",
    "            except ValueError:\n",
    "                continue\n",
    "        \n",
    "        if peak_section:\n",
    "            try:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    mz = float(parts[0])\n",
    "                    intensity = float(parts[1])\n",
    "                    peaks.append([mz, intensity])\n",
    "            except ValueError:\n",
    "                continue\n",
    "        else:\n",
    "            if ':' in line:\n",
    "                key, value = line.split(':', 1)\n",
    "                key = key.strip().lower().replace(' ', '_')\n",
    "                value = value.strip()\n",
    "                \n",
    "                if key == 'comments':\n",
    "                    entry['comments'] = value\n",
    "                elif key == 'name':\n",
    "                    entry['name'] = value\n",
    "    \n",
    "    entry['peaks'] = peaks\n",
    "    return entry if peaks else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SMILES Extraction and Validation\n",
    "\n",
    "Functions for extracting and validating SMILES strings from MSP comments.\n",
    "\n",
    "### SMILES Extraction Strategy:\n",
    "\n",
    "Multiple regex patterns are used to handle various MSP formats:\n",
    "- GNPS format: `\"SMILES=...\"`\n",
    "- MoNA format: `\"computed SMILES=...\"`\n",
    "- Variations in capitalization and spacing\n",
    "\n",
    "### Validation Criteria:\n",
    "\n",
    "Valid SMILES strings must:\n",
    "1. Have length ≥ 2 characters\n",
    "2. Contain only valid SMILES characters\n",
    "3. Not be placeholder values (N/A, NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_smiles_from_comments(comments: str) -> Optional[str]:\n",
    "    \"\"\"Extract SMILES string from MSP Comments field.\n",
    "    \n",
    "    Args:\n",
    "        comments: Comments field from MSP entry\n",
    "        \n",
    "    Returns:\n",
    "        Extracted SMILES string or None\n",
    "    \"\"\"\n",
    "    if not comments:\n",
    "        return None\n",
    "    \n",
    "    # Common patterns across different MSP sources\n",
    "    patterns = [\n",
    "        r'\"SMILES=([^\"]+)\"',\n",
    "        r'\"computed SMILES=([^\"]+)\"',\n",
    "        r'\"SMILES\\s*=\\s*([^\"]+)\"',\n",
    "        r'\"computed\\s+SMILES\\s*=\\s*([^\"]+)\"',\n",
    "        r'SMILES=([^\\s;]+)',  # Without quotes\n",
    "        r'Smiles=([^\\s;]+)',  # Case variation\n",
    "        r'\"Smiles=([^\"]+)\"'  # Case variation with quotes\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, comments, re.IGNORECASE)\n",
    "        if match:\n",
    "            smiles = match.group(1).strip()\n",
    "            if smiles and smiles != 'N/A' and smiles != 'NA':\n",
    "                return smiles\n",
    "    \n",
    "    return None\n",
    "\n",
    "def validate_smiles_basic(smiles: str) -> bool:\n",
    "    \"\"\"Basic SMILES validation.\n",
    "    \n",
    "    Args:\n",
    "        smiles: SMILES string to validate\n",
    "        \n",
    "    Returns:\n",
    "        True if SMILES appears valid\n",
    "    \"\"\"\n",
    "    if not smiles or len(smiles) < CONVERSION_CONFIG['validation']['min_smiles_length']:\n",
    "        return False\n",
    "    \n",
    "    # Extended character set for broader SMILES compatibility\n",
    "    valid_chars = set('CNOSFPB[]()=#+\\\\/-@123456789.%cnosfpb')\n",
    "    return all(c in valid_chars or c.isupper() or c.islower() for c in smiles)\n",
    "\n",
    "def validate_peaks(peaks: List[List[float]]) -> bool:\n",
    "    \"\"\"Validate peak data.\n",
    "    \n",
    "    Args:\n",
    "        peaks: List of [m/z, intensity] pairs\n",
    "        \n",
    "    Returns:\n",
    "        True if peaks are valid\n",
    "    \"\"\"\n",
    "    if not peaks or len(peaks) < CONVERSION_CONFIG['validation']['min_peaks']:\n",
    "        return False\n",
    "    \n",
    "    for peak in peaks:\n",
    "        if len(peak) != 2:\n",
    "            return False\n",
    "        mz, intensity = peak\n",
    "        # Validation ranges from config\n",
    "        if not (0 <= mz <= CONVERSION_CONFIG['validation']['max_mz']) or \\\n",
    "           not (0 <= intensity <= CONVERSION_CONFIG['validation']['max_intensity']):\n",
    "            return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Conversion Pipeline\n",
    "\n",
    "Convert parsed MSP entries to JSONL format with error tracking.\n",
    "\n",
    "### Conversion Process:\n",
    "\n",
    "For each MSP entry:\n",
    "1. Extract SMILES from comments\n",
    "2. Validate SMILES structure\n",
    "3. Validate peak data\n",
    "4. Convert to JSONL format\n",
    "5. Track corrupted records\n",
    "\n",
    "### Error Tracking:\n",
    "\n",
    "Corrupted records are saved with:\n",
    "- Entry index and name\n",
    "- Error description\n",
    "- Relevant context for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_msp_to_jsonl(entries: List[Dict]) -> Tuple[List[Dict], Dict]:\n",
    "    \"\"\"Convert parsed MSP entries to JSONL format.\n",
    "    \n",
    "    Args:\n",
    "        entries: List of parsed MSP entries\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (jsonl_entries, statistics)\n",
    "    \"\"\"\n",
    "    jsonl_entries = []\n",
    "    stats = {\n",
    "        'total_entries': len(entries),\n",
    "        'successful': 0,\n",
    "        'corrupted_records': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nConverting {len(entries)} entries to JSONL format...\")\n",
    "    \n",
    "    for i, entry in enumerate(tqdm(entries, desc=\"Converting to JSONL\")):\n",
    "        try:\n",
    "            # Extract and validate SMILES\n",
    "            smiles = extract_smiles_from_comments(entry.get('comments', ''))\n",
    "            if not smiles or not validate_smiles_basic(smiles):\n",
    "                stats['corrupted_records'].append({\n",
    "                    'index': i,\n",
    "                    'name': entry.get('name', 'Unknown'),\n",
    "                    'error': 'Invalid or missing SMILES',\n",
    "                    'comments': entry.get('comments', '')[:200]  # First 200 chars for debugging\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Validate peaks\n",
    "            peaks = entry.get('peaks', [])\n",
    "            if not validate_peaks(peaks):\n",
    "                stats['corrupted_records'].append({\n",
    "                    'index': i,\n",
    "                    'name': entry.get('name', 'Unknown'),\n",
    "                    'error': 'Invalid peaks',\n",
    "                    'num_peaks': len(peaks)\n",
    "                })\n",
    "                continue\n",
    "            \n",
    "            # Create JSONL entry\n",
    "            jsonl_entries.append({\n",
    "                'smiles': smiles,\n",
    "                'peaks': peaks\n",
    "            })\n",
    "            stats['successful'] += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            stats['corrupted_records'].append({\n",
    "                'index': i,\n",
    "                'name': entry.get('name', 'Unknown'),\n",
    "                'error': f'Conversion error: {str(e)}'\n",
    "            })\n",
    "    \n",
    "    return jsonl_entries, stats\n",
    "\n",
    "def save_jsonl(data: List[Dict], output_path: str) -> None:\n",
    "    \"\"\"Save data to JSONL format.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries to save\n",
    "        output_path: Output file path\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_corrupted_records(corrupted_records: List[Dict], output_path: str) -> None:\n",
    "    \"\"\"Save corrupted records for analysis.\n",
    "    \n",
    "    Args:\n",
    "        corrupted_records: List of corrupted record information\n",
    "        output_path: Output file path\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for record in corrupted_records:\n",
    "            f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Execute Conversion Pipeline\n",
    "\n",
    "Run the complete conversion pipeline from MSP to JSONL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXECUTING CONVERSION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Parsing 23802 potential entries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing MSP entries: 100%|██████████| 23802/23802 [00:11<00:00, 2101.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsed 23801 entries from GNPS.msp\n",
      "\n",
      "Converting 23801 entries to JSONL format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting to JSONL: 100%|██████████| 23801/23801 [00:01<00:00, 12739.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONVERSION RESULTS\n",
      "----------------------------------------\n",
      "Successfully converted: 23630 entries\n",
      "Failed conversions: 171 entries\n",
      "Success rate: 99.3%\n",
      "\n",
      "Output saved to: ../data/input/GNPS/spectral_data.jsonl\n",
      "Corrupted records saved to: ../data/input/GNPS/corrupted_records.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXECUTING CONVERSION PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Parse MSP file\n",
    "entries = parse_msp_file(str(msp_file_path))\n",
    "print(f\"\\nParsed {len(entries)} entries from {msp_file_path.name}\")\n",
    "\n",
    "# Convert to JSONL format\n",
    "jsonl_data, conversion_stats = convert_msp_to_jsonl(entries)\n",
    "\n",
    "print(\"\\nCONVERSION RESULTS\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Successfully converted: {conversion_stats['successful']} entries\")\n",
    "print(f\"Failed conversions: {len(conversion_stats['corrupted_records'])} entries\")\n",
    "\n",
    "# Calculate success rate\n",
    "success_rate = (conversion_stats['successful'] / conversion_stats['total_entries'] * 100 \n",
    "                if conversion_stats['total_entries'] > 0 else 0)\n",
    "print(f\"Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "# Save results\n",
    "output_file = DATASET_INPUT_DIR / 'spectral_data.jsonl'\n",
    "corrupted_file = DATASET_INPUT_DIR / 'corrupted_records.jsonl'\n",
    "\n",
    "save_jsonl(jsonl_data, str(output_file))\n",
    "print(f\"\\nOutput saved to: {output_file}\")\n",
    "\n",
    "if conversion_stats['corrupted_records'] and CONVERSION_CONFIG['processing']['save_corrupted']:\n",
    "    save_corrupted_records(conversion_stats['corrupted_records'], str(corrupted_file))\n",
    "    print(f\"Corrupted records saved to: {corrupted_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Analysis\n",
    "\n",
    "Analyze the converted data to understand its characteristics.\n",
    "\n",
    "### Quality Metrics:\n",
    "\n",
    "- **SMILES Length Distribution**: Indicates molecular complexity\n",
    "- **Peak Count Distribution**: Shows spectrum richness\n",
    "- **m/z Range**: Mass range coverage\n",
    "- **Intensity Range**: Dynamic range of measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA QUALITY ANALYSIS\n",
      "================================================================================\n",
      "Dataset: GNPS\n",
      "Total records: 23,630\n",
      "\n",
      "SMILES Statistics:\n",
      "  - Mean length: 71.8\n",
      "  - Min length: 3\n",
      "  - Max length: 642\n",
      "  - Std deviation: 43.2\n",
      "\n",
      "Peak Statistics:\n",
      "  - Mean peaks per spectrum: 878.7\n",
      "  - Min peaks: 1\n",
      "  - Max peaks: 361421\n",
      "  - Std deviation: 5281.7\n",
      "\n",
      "m/z Statistics:\n",
      "  - Range: 20.4 - 4971.1\n",
      "  - Mean: 453.3\n",
      "  - Median: 263.2\n",
      "\n",
      "Intensity Statistics:\n",
      "  - Range: 0.00e+00 - 1.04e+09\n",
      "  - Mean: 7.61e+02\n",
      "  - Median: 1.00e-01\n",
      "\n",
      "SAMPLE ENTRY\n",
      "----------------------------------------\n",
      "SMILES: CC(N(O)CCCCCNC(CCC(N(O)CCCCCNC(CCC(N(O)CCCCN)=O)=O...\n",
      "Number of peaks: 1415\n",
      "First 3 peaks: [[97.798515, 0.005925], [97.833961, 0.012199], [99.689133, 0.011502]]\n"
     ]
    }
   ],
   "source": [
    "if jsonl_data:\n",
    "    print(\"\\nDATA QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Dataset: {DATASET_NAME}\")\n",
    "    print(f\"Total records: {len(jsonl_data):,}\")\n",
    "    \n",
    "    # SMILES analysis\n",
    "    smiles_lengths = [len(entry['smiles']) for entry in jsonl_data]\n",
    "    print(f\"\\nSMILES Statistics:\")\n",
    "    print(f\"  - Mean length: {np.mean(smiles_lengths):.1f}\")\n",
    "    print(f\"  - Min length: {min(smiles_lengths)}\")\n",
    "    print(f\"  - Max length: {max(smiles_lengths)}\")\n",
    "    print(f\"  - Std deviation: {np.std(smiles_lengths):.1f}\")\n",
    "    \n",
    "    # Peak statistics\n",
    "    peak_counts = [len(entry['peaks']) for entry in jsonl_data]\n",
    "    all_mz_values = [peak[0] for entry in jsonl_data for peak in entry['peaks']]\n",
    "    all_intensities = [peak[1] for entry in jsonl_data for peak in entry['peaks']]\n",
    "    \n",
    "    print(f\"\\nPeak Statistics:\")\n",
    "    print(f\"  - Mean peaks per spectrum: {np.mean(peak_counts):.1f}\")\n",
    "    print(f\"  - Min peaks: {min(peak_counts)}\")\n",
    "    print(f\"  - Max peaks: {max(peak_counts)}\")\n",
    "    print(f\"  - Std deviation: {np.std(peak_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\nm/z Statistics:\")\n",
    "    print(f\"  - Range: {min(all_mz_values):.1f} - {max(all_mz_values):.1f}\")\n",
    "    print(f\"  - Mean: {np.mean(all_mz_values):.1f}\")\n",
    "    print(f\"  - Median: {np.median(all_mz_values):.1f}\")\n",
    "    \n",
    "    print(f\"\\nIntensity Statistics:\")\n",
    "    print(f\"  - Range: {min(all_intensities):.2e} - {max(all_intensities):.2e}\")\n",
    "    print(f\"  - Mean: {np.mean(all_intensities):.2e}\")\n",
    "    print(f\"  - Median: {np.median(all_intensities):.2e}\")\n",
    "    \n",
    "    # Sample entry\n",
    "    print(f\"\\nSAMPLE ENTRY\")\n",
    "    print(\"-\" * 40)\n",
    "    sample = jsonl_data[0]\n",
    "    print(f\"SMILES: {sample['smiles'][:50]}{'...' if len(sample['smiles']) > 50 else ''}\")\n",
    "    print(f\"Number of peaks: {len(sample['peaks'])}\")\n",
    "    print(f\"First 3 peaks: {sample['peaks'][:3]}\")\n",
    "else:\n",
    "    print(\"\\nNo data converted successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "Analyze conversion failures to identify potential issues.\n",
    "\n",
    "### Common Error Types:\n",
    "\n",
    "1. **Invalid or missing SMILES**: Comments field doesn't contain valid SMILES\n",
    "2. **Invalid peaks**: Peak data doesn't meet validation criteria\n",
    "3. **Conversion errors**: Unexpected errors during processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR ANALYSIS\n",
      "================================================================================\n",
      "Total failed conversions: 171\n",
      "\n",
      "Error Type Breakdown:\n",
      "----------------------------------------\n",
      "  Invalid or missing SMILES: 171 (100.0%)\n",
      "\n",
      "Sample Failed Conversions (First 5):\n",
      "----------------------------------------\n",
      "\n",
      "1. Entry 394:\n",
      "   Name: Methylmycofactocinol-9\n",
      "   Error: Invalid or missing SMILES\n",
      "   Comments excerpt: \"SMILES=OC1=C(O)C(C)(C)C(N1)CC(C=C2)=CC=C2OC(C3O)OC(COC([C@@H]4O)OC(COC(C5O)OC(COC(C6O)OC(COC(C7O)OC...\n",
      "\n",
      "2. Entry 395:\n",
      "   Name: GlycylAHDP-8\n",
      "   Error: Invalid or missing SMILES\n",
      "   Comments excerpt: \"SMILES=OC(C(COC([C@@H]1O)OC(COC(C2O)OC(COC(C3O)OC(COC(C4O)OC(COC(C5O)OC(COC(C6O)OC(COC(C7O)OC(CO)[C...\n",
      "\n",
      "3. Entry 396:\n",
      "   Name: Methylmycofactocinol-8\n",
      "   Error: Invalid or missing SMILES\n",
      "   Comments excerpt: \"SMILES=OC1=C(O)C(C)(C)C(N1)CC(C=C2)=CC=C2OC(C3O)OC(COC([C@@H]4O)OC(COC(C5O)OC(COC(C6O)OC(COC(C7O)OC...\n",
      "\n",
      "4. Entry 397:\n",
      "   Name: Methylmycofactocinone-8\n",
      "   Error: Invalid or missing SMILES\n",
      "   Comments excerpt: \"SMILES=O=C(C1=O)NC(C1(C)C)CC(C=C2)=CC=C2OC(C3O)OC(COC([C@@H]4O)OC(COC(C5O)OC(COC(C6O)OC(COC(C7O)OC(...\n",
      "\n",
      "5. Entry 398:\n",
      "   Name: Mycofactocinol-8\n",
      "   Error: Invalid or missing SMILES\n",
      "   Comments excerpt: \"SMILES=OC1=C(O)C(C)(C)C(N1)CC(C=C2)=CC=C2OC(C3O)OC(COC([C@@H]4O)OC(COC(C5O)OC(COC(C6O)OC(COC(C7O)OC...\n"
     ]
    }
   ],
   "source": [
    "if conversion_stats['corrupted_records']:\n",
    "    print(\"\\nERROR ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total failed conversions: {len(conversion_stats['corrupted_records'])}\")\n",
    "    \n",
    "    # Analyze error types\n",
    "    error_types = {}\n",
    "    for record in conversion_stats['corrupted_records']:\n",
    "        error = record['error'].split(':')[0]\n",
    "        error_types[error] = error_types.get(error, 0) + 1\n",
    "    \n",
    "    print(\"\\nError Type Breakdown:\")\n",
    "    print(\"-\" * 40)\n",
    "    for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True):\n",
    "        percentage = count / len(conversion_stats['corrupted_records']) * 100\n",
    "        print(f\"  {error_type}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Show sample errors for debugging\n",
    "    print(\"\\nSample Failed Conversions (First 5):\")\n",
    "    print(\"-\" * 40)\n",
    "    for i, record in enumerate(conversion_stats['corrupted_records'][:5]):\n",
    "        print(f\"\\n{i+1}. Entry {record['index']}:\")\n",
    "        print(f\"   Name: {record['name']}\")\n",
    "        print(f\"   Error: {record['error']}\")\n",
    "        if 'comments' in record:\n",
    "            print(f\"   Comments excerpt: {record['comments'][:100]}...\")\n",
    "        if 'num_peaks' in record:\n",
    "            print(f\"   Number of peaks: {record['num_peaks']}\")\n",
    "else:\n",
    "    print(\"\\nNo conversion errors encountered!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Format Validation\n",
    "\n",
    "Validate that the output format is compatible with the downstream pipeline.\n",
    "\n",
    "### Pipeline Requirements:\n",
    "\n",
    "The featurization pipeline expects:\n",
    "- JSONL format with one record per line\n",
    "- Each record must have 'smiles' and 'peaks' fields\n",
    "- 'smiles': string containing valid SMILES notation\n",
    "- 'peaks': list of [m/z, intensity] pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FORMAT VALIDATION\n",
      "================================================================================\n",
      "Validation Results:\n",
      "  - Required keys present: ✓\n",
      "  - Data types valid: ✓\n",
      "  - Format compatible: ✓\n",
      "\n",
      "============================================================\n",
      "✓ CONVERSION SUCCESSFUL\n",
      "============================================================\n",
      "\n",
      "Ready for pipeline processing!\n",
      "Next step: Set dataset_type = 'GNPS' in the featurization notebook\n",
      "Output file: ../data/input/GNPS/spectral_data.jsonl\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\nFORMAT VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Validate format compatibility with existing pipeline\n",
    "if jsonl_data:\n",
    "    # Check sample entry\n",
    "    sample = jsonl_data[0]\n",
    "    required_keys = {'smiles', 'peaks'}\n",
    "    has_required_keys = required_keys.issubset(sample.keys())\n",
    "    \n",
    "    # Validate data types\n",
    "    valid_types = (\n",
    "        isinstance(sample['smiles'], str) and\n",
    "        isinstance(sample['peaks'], list) and\n",
    "        all(isinstance(p, list) and len(p) == 2 for p in sample['peaks'][:5]) and\n",
    "        all(isinstance(p[0], (int, float)) and isinstance(p[1], (int, float)) \n",
    "            for p in sample['peaks'][:5])\n",
    "    )\n",
    "    \n",
    "    print(\"Validation Results:\")\n",
    "    print(f\"  - Required keys present: {'✓' if has_required_keys else '✗'}\")\n",
    "    print(f\"  - Data types valid: {'✓' if valid_types else '✗'}\")\n",
    "    print(f\"  - Format compatible: {'✓' if has_required_keys and valid_types else '✗'}\")\n",
    "    \n",
    "    if has_required_keys and valid_types:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"✓ CONVERSION SUCCESSFUL\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nReady for pipeline processing!\")\n",
    "        print(f\"Next step: Set dataset_type = '{DATASET_NAME}' in the featurization notebook\")\n",
    "        print(f\"Output file: {output_file}\")\n",
    "    else:\n",
    "        print(\"\\n⚠ WARNING: Format validation failed!\")\n",
    "        print(\"Please check the conversion process.\")\n",
    "else:\n",
    "    print(\"No data to validate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio729p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
