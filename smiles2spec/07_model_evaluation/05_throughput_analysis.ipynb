{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Comprehensive Throughput Analysis\n",
    "\n",
    "This notebook provides production-grade throughput benchmarking for all trained mass spectrometry prediction models, focusing on real-world deployment scenarios and performance optimization.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The throughput analysis framework evaluates:\n",
    "\n",
    "### 1. Performance Metrics\n",
    "- **Latency**: Time to process a single sample (milliseconds)\n",
    "  - Mean, median (p50), 95th percentile (p95), 99th percentile (p99)\n",
    "- **Throughput**: Samples processed per second\n",
    "  - Varies with batch size and hardware configuration\n",
    "- **Memory Footprint**: RAM usage during inference\n",
    "  - Peak memory, average memory, memory per sample\n",
    "- **Resource Utilization**: CPU/GPU usage patterns\n",
    "  - Core utilization, memory bandwidth, cache efficiency\n",
    "\n",
    "### 2. Scalability Analysis\n",
    "- **Batch Size Optimization**: Finding the sweet spot between latency and throughput\n",
    "- **Parallel Processing**: Multi-core CPU and GPU acceleration\n",
    "- **Memory-Throughput Trade-offs**: Balancing speed with resource constraints\n",
    "- **Hardware Scaling**: Performance on different hardware configurations\n",
    "\n",
    "### 3. Deployment Scenarios\n",
    "- **Real-time Analysis**: Single spectrum prediction with <10ms latency\n",
    "- **Batch Processing**: High-throughput analysis of compound libraries\n",
    "- **Screening Pipelines**: Million-compound virtual screening\n",
    "- **Edge Deployment**: Resource-constrained environments\n",
    "\n",
    "## Models Tested\n",
    "All 8 models from the training pipeline:\n",
    "- Random Forest\n",
    "- K-Nearest Neighbors (Optimized)\n",
    "- ModularNet\n",
    "- HierarchicalPredictionNet\n",
    "- SparseGatedNet\n",
    "- RegionalExpertNet\n",
    "- Simple Weighted Ensemble\n",
    "- Bin-by-bin Ensemble\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Throughput Calculation\n",
    "$$\\text{Throughput} = \\frac{\\text{Batch Size}}{\\text{Batch Processing Time}}$$\n",
    "\n",
    "### Parallel Efficiency\n",
    "$$\\text{Efficiency} = \\frac{\\text{Speedup}}{\\text{Number of Workers}} = \\frac{T_1 / T_n}{n}$$\n",
    "\n",
    "Where $T_1$ is single-worker time and $T_n$ is n-worker time.\n",
    "\n",
    "### Latency Percentiles\n",
    "For a distribution of latencies $\\{l_1, l_2, ..., l_n\\}$:\n",
    "- p50 (median): Value where 50% of latencies are lower\n",
    "- p95: Value where 95% of latencies are lower\n",
    "- p99: Value where 99% of latencies are lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import all required libraries and configure the environment for throughput benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "CPU Information:\n",
      "  Processor: arm\n",
      "  Physical cores: 16\n",
      "  Total cores: 16\n",
      "  Max frequency: 4056.00 MHz\n",
      "  Total memory: 128.0 GB\n",
      "  Available memory: 44.9 GB\n",
      "\n",
      "GPU Information:\n",
      "  Device: Apple Silicon GPU (MPS)\n",
      "  Acceleration: Metal Performance Shaders\n",
      "\n",
      "Optimal parallel workers: 15\n",
      "\n",
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "import gc\n",
    "from typing import Dict, Any, Tuple, List, Optional, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict, deque\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "import tracemalloc\n",
    "\n",
    "# Data science imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Hardware detection\n",
    "print(\"=\" * 60)\n",
    "print(\"HARDWARE CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# CPU information\n",
    "cpu_count = mp.cpu_count()\n",
    "cpu_freq = psutil.cpu_freq()\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "print(f\"\\nCPU Information:\")\n",
    "print(f\"  Processor: {platform.processor()}\")\n",
    "print(f\"  Physical cores: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"  Total cores: {cpu_count}\")\n",
    "print(f\"  Max frequency: {cpu_freq.max:.2f} MHz\" if cpu_freq else \"  Frequency: Not available\")\n",
    "print(f\"  Total memory: {memory.total / (1024**3):.1f} GB\")\n",
    "print(f\"  Available memory: {memory.available / (1024**3):.1f} GB\")\n",
    "\n",
    "# GPU detection\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(f\"\\nGPU Information:\")\n",
    "    print(f\"  Device: Apple Silicon GPU (MPS)\")\n",
    "    print(f\"  Acceleration: Metal Performance Shaders\")\n",
    "elif torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"\\nGPU Information:\")\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(f\"\\nGPU Information:\")\n",
    "    print(f\"  No GPU detected, using CPU only\")\n",
    "\n",
    "# Optimal worker count for parallel processing\n",
    "OPTIMAL_WORKERS = min(cpu_count - 1, 16)  # Leave one core for system\n",
    "print(f\"\\nOptimal parallel workers: {OPTIMAL_WORKERS}\")\n",
    "\n",
    "# Set matplotlib style for publication-quality figures\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman'],\n",
    "    'axes.linewidth': 1.2,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'xtick.major.size': 6,\n",
    "    'xtick.minor.size': 3,\n",
    "    'ytick.major.size': 6,\n",
    "    'ytick.minor.size': 3,\n",
    "    'legend.frameon': False,\n",
    "    'figure.dpi': 100,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight'\n",
    "})\n",
    "\n",
    "# Color palette for models\n",
    "MODEL_COLORS = {\n",
    "    'random_forest': '#27ae60',\n",
    "    'knn': '#e74c3c',\n",
    "    'modularnet': '#8e44ad',\n",
    "    'hierarchicalpredictionnet': '#16a085',\n",
    "    'sparsegatednet': '#d35400',\n",
    "    'regionalexpertnet': '#2c3e50',\n",
    "    'simple_weighted_ensemble': '#f39c12',\n",
    "    'bin_by_bin_ensemble': '#3498db'\n",
    "}\n",
    "\n",
    "print(\"\\nEnvironment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Throughput Configuration\n",
    "\n",
    "Central configuration for all throughput benchmarking parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput configuration loaded\n",
      "Testing devices: ['cpu', 'mps']\n",
      "Batch sizes: [1, 10, 32, 64, 128, 256, 512, 1000]\n",
      "Scenarios: ['real_time', 'batch_processing', 'screening', 'edge']\n"
     ]
    }
   ],
   "source": [
    "# Master throughput configuration\n",
    "THROUGHPUT_CONFIG = {\n",
    "    'paths': {\n",
    "        'models_dir': '../models',\n",
    "        'results_dir': '../data/results',\n",
    "        'figures_dir': '../figures/throughput',\n",
    "        'input_type': 'hpj'\n",
    "    },\n",
    "    \n",
    "    'batch_sizes': [1, 10, 32, 64, 128, 256, 512, 1000],\n",
    "    \n",
    "    'measurement': {\n",
    "        'warmup_runs': 10,         # Number of warmup iterations\n",
    "        'timing_runs': 100,        # Number of timed iterations\n",
    "        'memory_profile': True,    # Enable memory profiling\n",
    "        'cpu_affinity': False,     # Pin to specific CPU cores\n",
    "        'gc_collect': True,        # Force garbage collection between runs\n",
    "        'percentiles': [50, 95, 99]  # Latency percentiles to calculate\n",
    "    },\n",
    "    \n",
    "    'hardware': {\n",
    "        'devices': ['cpu'],  # Will add 'mps' or 'cuda' if available\n",
    "        'parallel_workers': [1, 2, 4, 8],  # Number of parallel workers to test\n",
    "        'memory_limit_gb': 16,  # Maximum memory usage\n",
    "        'enable_gpu': True,     # Use GPU if available\n",
    "        'mixed_precision': False  # Use FP16 for neural networks\n",
    "    },\n",
    "    \n",
    "    'scenarios': {\n",
    "        'real_time': {\n",
    "            'name': 'Real-time Analysis',\n",
    "            'batch_size': 1,\n",
    "            'latency_target_ms': 10,\n",
    "            'description': 'Single spectrum prediction for interactive analysis'\n",
    "        },\n",
    "        'batch_processing': {\n",
    "            'name': 'Batch Processing',\n",
    "            'batch_size': 1000,\n",
    "            'throughput_target': 10000,  # samples/second\n",
    "            'description': 'Efficient processing of compound libraries'\n",
    "        },\n",
    "        'screening': {\n",
    "            'name': 'High-throughput Screening',\n",
    "            'batch_size': 10000,\n",
    "            'throughput_target': 100000,  # samples/second\n",
    "            'description': 'Million-compound virtual screening'\n",
    "        },\n",
    "        'edge': {\n",
    "            'name': 'Edge Deployment',\n",
    "            'batch_size': 10,\n",
    "            'memory_limit_mb': 512,\n",
    "            'description': 'Resource-constrained environments'\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    'optimization': {\n",
    "        'onnx_export': False,      # Export to ONNX for optimization\n",
    "        'quantization': False,     # Model quantization (INT8)\n",
    "        'pruning': False,          # Model pruning\n",
    "        'torch_compile': False,    # PyTorch 2.0 compilation\n",
    "        'batch_optimization': True  # Optimize for specific batch sizes\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add available GPU to devices\n",
    "if torch.backends.mps.is_available() and THROUGHPUT_CONFIG['hardware']['enable_gpu']:\n",
    "    THROUGHPUT_CONFIG['hardware']['devices'].append('mps')\n",
    "elif torch.cuda.is_available() and THROUGHPUT_CONFIG['hardware']['enable_gpu']:\n",
    "    THROUGHPUT_CONFIG['hardware']['devices'].append('cuda')\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(THROUGHPUT_CONFIG['paths']['figures_dir'], exist_ok=True)\n",
    "os.makedirs(os.path.join(THROUGHPUT_CONFIG['paths']['results_dir'], 'throughput'), exist_ok=True)\n",
    "\n",
    "print(f\"Throughput configuration loaded\")\n",
    "print(f\"Testing devices: {THROUGHPUT_CONFIG['hardware']['devices']}\")\n",
    "print(f\"Batch sizes: {THROUGHPUT_CONFIG['batch_sizes']}\")\n",
    "print(f\"Scenarios: {list(THROUGHPUT_CONFIG['scenarios'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Throughput Metrics Implementation\n",
    "\n",
    "Comprehensive data structures and measurement utilities for throughput benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Throughput benchmarking system initialized\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ThroughputMetrics:\n",
    "    \"\"\"Container for comprehensive throughput measurements\"\"\"\n",
    "    # Model identification\n",
    "    model_name: str\n",
    "    batch_size: int\n",
    "    device: str\n",
    "    n_workers: int = 1\n",
    "    \n",
    "    # Timing metrics (milliseconds)\n",
    "    mean_latency_ms: float = 0.0\n",
    "    std_latency_ms: float = 0.0\n",
    "    min_latency_ms: float = 0.0\n",
    "    max_latency_ms: float = 0.0\n",
    "    p50_latency_ms: float = 0.0\n",
    "    p95_latency_ms: float = 0.0\n",
    "    p99_latency_ms: float = 0.0\n",
    "    \n",
    "    # Throughput metrics\n",
    "    samples_per_second: float = 0.0\n",
    "    batches_per_second: float = 0.0\n",
    "    total_samples_processed: int = 0\n",
    "    \n",
    "    # Resource metrics\n",
    "    peak_memory_mb: float = 0.0\n",
    "    avg_memory_mb: float = 0.0\n",
    "    memory_per_sample_mb: float = 0.0\n",
    "    avg_cpu_percent: float = 0.0\n",
    "    gpu_utilization: Optional[float] = None\n",
    "    gpu_memory_mb: Optional[float] = None\n",
    "    \n",
    "    # Efficiency metrics\n",
    "    parallel_efficiency: float = 1.0\n",
    "    batch_efficiency: float = 1.0\n",
    "    speedup: float = 1.0\n",
    "    \n",
    "    # Raw measurements for analysis\n",
    "    latency_measurements: List[float] = field(default_factory=list)\n",
    "    memory_measurements: List[float] = field(default_factory=list)\n",
    "    \n",
    "    def calculate_derived_metrics(self):\n",
    "        \"\"\"Calculate derived metrics from raw measurements\"\"\"\n",
    "        if self.latency_measurements:\n",
    "            latencies = np.array(self.latency_measurements)\n",
    "            self.mean_latency_ms = np.mean(latencies)\n",
    "            self.std_latency_ms = np.std(latencies)\n",
    "            self.min_latency_ms = np.min(latencies)\n",
    "            self.max_latency_ms = np.max(latencies)\n",
    "            self.p50_latency_ms = np.percentile(latencies, 50)\n",
    "            self.p95_latency_ms = np.percentile(latencies, 95)\n",
    "            self.p99_latency_ms = np.percentile(latencies, 99)\n",
    "            \n",
    "            # Calculate throughput\n",
    "            if self.mean_latency_ms > 0:\n",
    "                self.batches_per_second = 1000.0 / self.mean_latency_ms\n",
    "                self.samples_per_second = self.batches_per_second * self.batch_size\n",
    "        \n",
    "        if self.memory_measurements:\n",
    "            self.avg_memory_mb = np.mean(self.memory_measurements)\n",
    "            self.peak_memory_mb = np.max(self.memory_measurements)\n",
    "            if self.batch_size > 0:\n",
    "                self.memory_per_sample_mb = self.avg_memory_mb / self.batch_size\n",
    "\n",
    "\n",
    "class ThroughputBenchmarker:\n",
    "    \"\"\"Comprehensive throughput benchmarking system\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.results = defaultdict(list)\n",
    "        \n",
    "    def measure_inference_time(self, model: Any, inputs: np.ndarray, \n",
    "                             model_name: str, device: str = 'cpu',\n",
    "                             batch_size: int = 1) -> ThroughputMetrics:\n",
    "        \"\"\"Measure inference time with proper warmup and statistics\"\"\"\n",
    "        \n",
    "        metrics = ThroughputMetrics(\n",
    "            model_name=model_name,\n",
    "            batch_size=batch_size,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Prepare batches\n",
    "        n_samples = len(inputs)\n",
    "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Convert to appropriate format based on model type\n",
    "        is_neural = any(x in model_name.lower() for x in ['modular', 'hierarchical', 'sparse', 'regional', 'net'])\n",
    "        \n",
    "        if is_neural:\n",
    "            inputs_tensor = torch.from_numpy(inputs).float()\n",
    "            if device != 'cpu':\n",
    "                inputs_tensor = inputs_tensor.to(device)\n",
    "                model = model.to(device)\n",
    "            model.eval()\n",
    "        \n",
    "        # Warmup runs\n",
    "        logger.info(f\"Running {self.config['measurement']['warmup_runs']} warmup iterations...\")\n",
    "        for _ in range(self.config['measurement']['warmup_runs']):\n",
    "            if is_neural:\n",
    "                with torch.no_grad():\n",
    "                    batch = inputs_tensor[:batch_size]\n",
    "                    _ = model(batch)\n",
    "            else:\n",
    "                batch = inputs[:batch_size]\n",
    "                _ = model.predict(batch)\n",
    "        \n",
    "        # Force garbage collection\n",
    "        if self.config['measurement']['gc_collect']:\n",
    "            gc.collect()\n",
    "            if device != 'cpu':\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Timing runs\n",
    "        latencies = []\n",
    "        memory_usage = []\n",
    "        \n",
    "        logger.info(f\"Running {self.config['measurement']['timing_runs']} timing iterations...\")\n",
    "        \n",
    "        for run in tqdm(range(self.config['measurement']['timing_runs']), \n",
    "                       desc=f\"Benchmarking {model_name} (batch_size={batch_size})\"):\n",
    "            \n",
    "            # Select random batch\n",
    "            batch_idx = np.random.randint(0, max(1, n_batches))\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, n_samples)\n",
    "            \n",
    "            # Memory tracking\n",
    "            if self.config['measurement']['memory_profile']:\n",
    "                tracemalloc.start()\n",
    "            \n",
    "            # Time the inference\n",
    "            if is_neural:\n",
    "                batch = inputs_tensor[start_idx:end_idx]\n",
    "                \n",
    "                # Synchronize for accurate GPU timing\n",
    "                if device != 'cpu':\n",
    "                    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                with torch.no_grad():\n",
    "                    _ = model(batch)\n",
    "                \n",
    "                if device != 'cpu':\n",
    "                    torch.cuda.synchronize() if device == 'cuda' else None\n",
    "                    \n",
    "                end_time = time.perf_counter()\n",
    "            else:\n",
    "                batch = inputs[start_idx:end_idx]\n",
    "                \n",
    "                start_time = time.perf_counter()\n",
    "                _ = model.predict(batch)\n",
    "                end_time = time.perf_counter()\n",
    "            \n",
    "            # Record measurements\n",
    "            latency_ms = (end_time - start_time) * 1000\n",
    "            latencies.append(latency_ms)\n",
    "            \n",
    "            # Memory tracking\n",
    "            if self.config['measurement']['memory_profile']:\n",
    "                current, peak = tracemalloc.get_traced_memory()\n",
    "                memory_usage.append(peak / (1024 * 1024))  # Convert to MB\n",
    "                tracemalloc.stop()\n",
    "        \n",
    "        # Store measurements\n",
    "        metrics.latency_measurements = latencies\n",
    "        metrics.memory_measurements = memory_usage\n",
    "        metrics.total_samples_processed = self.config['measurement']['timing_runs'] * batch_size\n",
    "        \n",
    "        # Calculate derived metrics\n",
    "        metrics.calculate_derived_metrics()\n",
    "        \n",
    "        # Get CPU usage\n",
    "        metrics.avg_cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def profile_memory_usage(self, model: Any, inputs: np.ndarray,\n",
    "                           model_name: str, batch_sizes: List[int]) -> Dict[int, float]:\n",
    "        \"\"\"Profile memory usage across different batch sizes\"\"\"\n",
    "        memory_profile = {}\n",
    "        is_neural = any(x in model_name.lower() for x in ['modular', 'hierarchical', 'sparse', 'regional', 'net'])\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            # Prepare batch\n",
    "            batch = inputs[:batch_size]\n",
    "            \n",
    "            # Start memory tracking\n",
    "            tracemalloc.start()\n",
    "            \n",
    "            # Run inference\n",
    "            if is_neural:\n",
    "                batch_tensor = torch.from_numpy(batch).float()\n",
    "                with torch.no_grad():\n",
    "                    _ = model(batch_tensor)\n",
    "            else:\n",
    "                _ = model.predict(batch)\n",
    "            \n",
    "            # Get peak memory\n",
    "            _, peak = tracemalloc.get_traced_memory()\n",
    "            memory_profile[batch_size] = peak / (1024 * 1024)  # MB\n",
    "            \n",
    "            tracemalloc.stop()\n",
    "            gc.collect()\n",
    "        \n",
    "        return memory_profile\n",
    "    \n",
    "    def analyze_parallelization(self, model: Any, inputs: np.ndarray,\n",
    "                              model_name: str, n_workers_list: List[int],\n",
    "                              batch_size: int = 1000) -> Dict[int, ThroughputMetrics]:\n",
    "        \"\"\"Analyze parallel processing efficiency\"\"\"\n",
    "        parallel_results = {}\n",
    "        is_neural = any(x in model_name.lower() for x in ['modular', 'hierarchical', 'sparse', 'regional', 'net'])\n",
    "        \n",
    "        # Only applicable for tree-based models\n",
    "        if not is_neural:\n",
    "            original_n_jobs = getattr(model, 'n_jobs', 1)\n",
    "            \n",
    "            for n_workers in n_workers_list:\n",
    "                # Set parallel workers\n",
    "                model.n_jobs = n_workers\n",
    "                \n",
    "                # Measure performance\n",
    "                metrics = self.measure_inference_time(\n",
    "                    model, inputs, model_name, 'cpu', batch_size\n",
    "                )\n",
    "                metrics.n_workers = n_workers\n",
    "                \n",
    "                # Calculate parallel efficiency\n",
    "                if n_workers > 1 and 1 in parallel_results:\n",
    "                    single_thread_time = parallel_results[1].mean_latency_ms\n",
    "                    metrics.speedup = single_thread_time / metrics.mean_latency_ms\n",
    "                    metrics.parallel_efficiency = metrics.speedup / n_workers\n",
    "                \n",
    "                parallel_results[n_workers] = metrics\n",
    "            \n",
    "            # Restore original setting\n",
    "            model.n_jobs = original_n_jobs\n",
    "        \n",
    "        return parallel_results\n",
    "\n",
    "print(\"Throughput benchmarking system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Neural Network Architecture Definitions\n",
    "\n",
    "Define all neural network architectures for loading trained models.\n",
    "These must match the architectures used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All neural network architectures defined\n"
     ]
    }
   ],
   "source": [
    "# Define all neural network architectures from training notebook\n",
    "\n",
    "class ModularNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        num_modules = config['num_modules']\n",
    "        module_dims = config['module_dims']\n",
    "        fusion_method = config['fusion_method']\n",
    "        \n",
    "        self.global_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, module_dims[0]),\n",
    "            nn.BatchNorm1d(module_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.specialized_modules = nn.ModuleList()\n",
    "        \n",
    "        for i in range(num_modules):\n",
    "            module_layers = []\n",
    "            dims = [input_dim] + module_dims\n",
    "            \n",
    "            for j in range(len(module_dims)):\n",
    "                module_layers.extend([\n",
    "                    nn.Linear(dims[j], dims[j+1]),\n",
    "                    nn.BatchNorm1d(dims[j+1]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1)\n",
    "                ])\n",
    "            \n",
    "            module_layers.append(nn.Linear(module_dims[-1], output_dim))\n",
    "            module_layers.append(nn.ReLU())\n",
    "            \n",
    "            self.specialized_modules.append(nn.Sequential(*module_layers))\n",
    "        \n",
    "        if fusion_method == 'attention':\n",
    "            self.fusion_attention = nn.Sequential(\n",
    "                nn.Linear(module_dims[0], num_modules),\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "        else:\n",
    "            self.fusion_weights = nn.Parameter(torch.ones(num_modules) / num_modules)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        global_features = self.global_encoder(x)\n",
    "        module_outputs = []\n",
    "        for module in self.specialized_modules:\n",
    "            output = module(x)\n",
    "            module_outputs.append(output)\n",
    "        \n",
    "        module_outputs = torch.stack(module_outputs, dim=1)\n",
    "        \n",
    "        if hasattr(self, 'fusion_attention'):\n",
    "            attention_weights = self.fusion_attention(global_features)\n",
    "            attention_weights = attention_weights.unsqueeze(2)\n",
    "            fused_output = (module_outputs * attention_weights).sum(dim=1)\n",
    "        else:\n",
    "            weights = F.softmax(self.fusion_weights, dim=0)\n",
    "            weights = weights.view(1, -1, 1)\n",
    "            fused_output = (module_outputs * weights).sum(dim=1)\n",
    "        \n",
    "        return fused_output\n",
    "\n",
    "class HierarchicalPredictionNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        presence_hidden = config['presence_hidden']\n",
    "        intensity_hidden = config['intensity_hidden']\n",
    "        self.presence_threshold = config['presence_threshold']\n",
    "        conditional_dropout = config['conditional_dropout']\n",
    "        \n",
    "        presence_layers = []\n",
    "        dims = [input_dim] + presence_hidden\n",
    "        \n",
    "        for i in range(len(presence_hidden)):\n",
    "            presence_layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "        \n",
    "        presence_layers.append(nn.Linear(presence_hidden[-1], output_dim))\n",
    "        self.presence_net = nn.Sequential(*presence_layers)\n",
    "        \n",
    "        intensity_input_dim = input_dim + output_dim\n",
    "        \n",
    "        intensity_layers = []\n",
    "        dims = [intensity_input_dim] + intensity_hidden\n",
    "        \n",
    "        for i in range(len(intensity_hidden)):\n",
    "            intensity_layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(conditional_dropout)\n",
    "            ])\n",
    "        \n",
    "        intensity_layers.append(nn.Linear(intensity_hidden[-1], output_dim))\n",
    "        intensity_layers.append(nn.ReLU())\n",
    "        self.intensity_net = nn.Sequential(*intensity_layers)\n",
    "        \n",
    "        self.calibration = nn.Sequential(\n",
    "            nn.Linear(output_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.hierarchical_forward = True\n",
    "    \n",
    "    def forward(self, x, return_presence=False):\n",
    "        presence_logits = self.presence_net(x)\n",
    "        presence_probs = torch.sigmoid(presence_logits)\n",
    "        \n",
    "        conditional_input = torch.cat([x, presence_probs], dim=1)\n",
    "        intensities = self.intensity_net(conditional_input)\n",
    "        \n",
    "        calibration_weights = self.calibration(intensities)\n",
    "        \n",
    "        output = intensities * presence_probs * calibration_weights\n",
    "        \n",
    "        if return_presence:\n",
    "            return output, presence_logits\n",
    "        return output\n",
    "\n",
    "class SparseGatingLayer(nn.Module):\n",
    "    def __init__(self, input_dim, gate_hidden, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.gate_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, gate_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_hidden, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.sparse_path = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "        \n",
    "        self.dense_path = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim),\n",
    "            nn.BatchNorm1d(input_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        gates = self.gate_net(x) / self.temperature\n",
    "        sparse_out = self.sparse_path(x)\n",
    "        dense_out = self.dense_path(x)\n",
    "        output = gates * dense_out + (1 - gates) * sparse_out\n",
    "        return output, gates\n",
    "\n",
    "class SparseGatedNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_dims = config['hidden_dims']\n",
    "        gate_hidden = config['gate_hidden']\n",
    "        temperature = config['gate_temperature']\n",
    "        \n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.input_bn = nn.BatchNorm1d(hidden_dims[0])\n",
    "        \n",
    "        self.gated_layers = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.gated_layers.append(\n",
    "                SparseGatingLayer(hidden_dims[i], gate_hidden, temperature)\n",
    "            )\n",
    "        \n",
    "        self.transitions = nn.ModuleList()\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            self.transitions.append(\n",
    "                nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            )\n",
    "        \n",
    "        self.output_sparse = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.output_dense = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        self.output_gate = nn.Sequential(\n",
    "            nn.Linear(hidden_dims[-1], output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj(x)\n",
    "        x = self.input_bn(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        for gated_layer, transition in zip(self.gated_layers, self.transitions):\n",
    "            x, _ = gated_layer(x)\n",
    "            x = transition(x)\n",
    "            x = F.relu(x)\n",
    "        \n",
    "        sparse_pred = F.relu(self.output_sparse(x))\n",
    "        dense_pred = F.relu(self.output_dense(x))\n",
    "        output_gates = self.output_gate(x)\n",
    "        \n",
    "        output = output_gates * dense_pred + (1 - output_gates) * sparse_pred * 0.1\n",
    "        \n",
    "        return output\n",
    "\n",
    "class RegionalExpert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims\n",
    "        \n",
    "        for i in range(len(hidden_dims)):\n",
    "            layers.extend([\n",
    "                nn.Linear(dims[i], dims[i+1]),\n",
    "                nn.BatchNorm1d(dims[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1)\n",
    "            ])\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class RegionalExpertNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.expert_regions = config['expert_regions']\n",
    "        expert_hidden = config['expert_hidden']\n",
    "        router_hidden = config['router_hidden']\n",
    "        self.overlap_bins = config['overlap_bins']\n",
    "        \n",
    "        self.experts = nn.ModuleList()\n",
    "        for start, end in self.expert_regions:\n",
    "            expert_output_dim = end - start + 2 * self.overlap_bins\n",
    "            self.experts.append(\n",
    "                RegionalExpert(input_dim, expert_output_dim, expert_hidden)\n",
    "            )\n",
    "        \n",
    "        self.router = nn.Sequential(\n",
    "            nn.Linear(input_dim, router_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(router_hidden, len(self.expert_regions)),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        self.global_features = nn.Sequential(\n",
    "            nn.Linear(input_dim, router_hidden),\n",
    "            nn.BatchNorm1d(router_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        routing_weights = self.router(x)\n",
    "        global_feat = self.global_features(x)\n",
    "        \n",
    "        output = torch.zeros(x.shape[0], 500).to(x.device)\n",
    "        \n",
    "        for i, ((start, end), expert) in enumerate(zip(self.expert_regions, self.experts)):\n",
    "            expert_pred = expert(x)\n",
    "            \n",
    "            actual_start = max(0, start - self.overlap_bins)\n",
    "            actual_end = min(500, end + self.overlap_bins)\n",
    "            \n",
    "            region_size = actual_end - actual_start\n",
    "            if expert_pred.shape[1] >= region_size:\n",
    "                weighted_pred = expert_pred[:, :region_size] * routing_weights[:, i:i+1]\n",
    "                output[:, actual_start:actual_end] += weighted_pred\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"All neural network architectures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Model Loading and Data Preparation\n",
    "\n",
    "Load all 8 trained models and prepare synthetic test data for throughput benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading models for throughput analysis...\n",
      "==================================================\n",
      "✓ Random Forest loaded\n",
      "✓ K-Nearest Neighbors loaded\n",
      "✓ Simple Weighted Ensemble loaded\n",
      "✓ Bin-by-bin Ensemble loaded\n",
      "\n",
      "Generating synthetic test data...\n",
      "Test data shape: (1000, 7137)\n",
      "\n",
      "Total models loaded: 4\n",
      "Models: ['random_forest', 'knn', 'simple_weighted_ensemble', 'bin_by_bin_ensemble']\n",
      "\n",
      "Memory usage: 27.2 MB\n"
     ]
    }
   ],
   "source": [
    "class ModelManager:\n",
    "    \"\"\"Manage model loading and optimization for throughput testing\"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.models = {}\n",
    "        self.test_data = None\n",
    "        \n",
    "    def load_all_models(self) -> Dict[str, Any]:\n",
    "        \"\"\"Load all 8 models from training notebook\"\"\"\n",
    "        models_dir = self.config['paths']['models_dir']\n",
    "        input_type = self.config['paths']['input_type']\n",
    "        \n",
    "        print(\"\\nLoading models for throughput analysis...\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # 1. Load Random Forest\n",
    "        rf_path = os.path.join(models_dir, f\"{input_type}_rf_model.pkl\")\n",
    "        if os.path.exists(rf_path):\n",
    "            with open(rf_path, 'rb') as f:\n",
    "                rf_data = pickle.load(f)\n",
    "                self.models['random_forest'] = rf_data['model']\n",
    "            print(f\"✓ Random Forest loaded\")\n",
    "            # Optimize for throughput\n",
    "            if hasattr(self.models['random_forest'], 'n_jobs'):\n",
    "                self.models['random_forest'].n_jobs = -1\n",
    "        \n",
    "        # 2. Load KNN\n",
    "        knn_path = os.path.join(models_dir, f\"{input_type}_knn_model.pkl\")\n",
    "        if os.path.exists(knn_path):\n",
    "            with open(knn_path, 'rb') as f:\n",
    "                knn_data = pickle.load(f)\n",
    "                self.models['knn'] = knn_data['model']\n",
    "            print(f\"✓ K-Nearest Neighbors loaded\")\n",
    "            if hasattr(self.models['knn'], 'n_jobs'):\n",
    "                self.models['knn'].n_jobs = -1\n",
    "        \n",
    "        # Load neural network configurations\n",
    "        nn_configs = {\n",
    "            'modularnet': {\n",
    "                'num_modules': 4,\n",
    "                'module_dims': [256, 128],\n",
    "                'fusion_method': 'attention'\n",
    "            },\n",
    "            'hierarchical': {\n",
    "                'presence_hidden': [512, 256],\n",
    "                'intensity_hidden': [512, 256, 128],\n",
    "                'presence_threshold': 0.01,\n",
    "                'conditional_dropout': 0.2\n",
    "            },\n",
    "            'sparsegated': {\n",
    "                'hidden_dims': [1024, 512, 256],\n",
    "                'gate_hidden': 128,\n",
    "                'gate_temperature': 1.0\n",
    "            },\n",
    "            'regional': {\n",
    "                'expert_regions': [(0, 100), (100, 200), (200, 300), (300, 400), (400, 500)],\n",
    "                'expert_hidden': [512, 256],\n",
    "                'router_hidden': 256,\n",
    "                'overlap_bins': 10\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Dimensions from training notebook\n",
    "        input_dim = 7137  # Feature dimensions\n",
    "        output_dim = 500  # Spectrum dimensions\n",
    "        \n",
    "        # 3-6. Load Neural Networks\n",
    "        nn_models_dir = os.path.join(models_dir, 'neural_networks')\n",
    "        \n",
    "        # ModularNet\n",
    "        modularnet_path = os.path.join(nn_models_dir, 'modularnet.pt')\n",
    "        if os.path.exists(modularnet_path):\n",
    "            checkpoint = torch.load(modularnet_path, map_location='cpu')\n",
    "            model = ModularNet(input_dim, output_dim, nn_configs['modularnet'])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            self.models['modularnet'] = model\n",
    "            print(f\"✓ ModularNet loaded\")\n",
    "        \n",
    "        # HierarchicalPredictionNet\n",
    "        hierarchical_path = os.path.join(nn_models_dir, 'hierarchicalpredictionnet.pt')\n",
    "        if os.path.exists(hierarchical_path):\n",
    "            checkpoint = torch.load(hierarchical_path, map_location='cpu')\n",
    "            model = HierarchicalPredictionNet(input_dim, output_dim, nn_configs['hierarchical'])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            self.models['hierarchicalpredictionnet'] = model\n",
    "            print(f\"✓ HierarchicalPredictionNet loaded\")\n",
    "        \n",
    "        # SparseGatedNet\n",
    "        sparsegated_path = os.path.join(nn_models_dir, 'sparsegatednet.pt')\n",
    "        if os.path.exists(sparsegated_path):\n",
    "            checkpoint = torch.load(sparsegated_path, map_location='cpu')\n",
    "            model = SparseGatedNet(input_dim, output_dim, nn_configs['sparsegated'])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            self.models['sparsegatednet'] = model\n",
    "            print(f\"✓ SparseGatedNet loaded\")\n",
    "        \n",
    "        # RegionalExpertNet\n",
    "        regional_path = os.path.join(nn_models_dir, 'regionalexpertnet.pt')\n",
    "        if os.path.exists(regional_path):\n",
    "            checkpoint = torch.load(regional_path, map_location='cpu')\n",
    "            model = RegionalExpertNet(input_dim, output_dim, nn_configs['regional'])\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            self.models['regionalexpertnet'] = model\n",
    "            print(f\"✓ RegionalExpertNet loaded\")\n",
    "        \n",
    "        # 7-8. Load Ensemble Models\n",
    "        ensemble_path = os.path.join(models_dir, 'ensemble_results.pkl')\n",
    "        if os.path.exists(ensemble_path):\n",
    "            with open(ensemble_path, 'rb') as f:\n",
    "                ensemble_data = pickle.load(f)\n",
    "            \n",
    "            # Create simple weighted ensemble\n",
    "            if 'simple_weighted' in ensemble_data:\n",
    "                weights = ensemble_data['simple_weighted']['weights']\n",
    "                self.models['simple_weighted_ensemble'] = self._create_ensemble(\n",
    "                    weights, 'simple_weighted'\n",
    "                )\n",
    "                print(f\"✓ Simple Weighted Ensemble loaded\")\n",
    "            \n",
    "            # Create bin-by-bin ensemble\n",
    "            if 'bin_by_bin' in ensemble_data:\n",
    "                self.models['bin_by_bin_ensemble'] = self._create_ensemble(\n",
    "                    ensemble_data['bin_by_bin'], 'bin_by_bin'\n",
    "                )\n",
    "                print(f\"✓ Bin-by-bin Ensemble loaded\")\n",
    "        \n",
    "        # Generate synthetic test data\n",
    "        print(f\"\\nGenerating synthetic test data...\")\n",
    "        n_samples = 1000\n",
    "        self.test_data = np.random.randn(n_samples, input_dim).astype(np.float32)\n",
    "        print(f\"Test data shape: {self.test_data.shape}\")\n",
    "        \n",
    "        print(f\"\\nTotal models loaded: {len(self.models)}\")\n",
    "        print(f\"Models: {list(self.models.keys())}\")\n",
    "        \n",
    "        return self.models\n",
    "    \n",
    "    def _create_ensemble(self, weights_data, ensemble_type):\n",
    "        \"\"\"Create ensemble predictor wrapper\"\"\"\n",
    "        class EnsembleModel:\n",
    "            def __init__(self, models, weights):\n",
    "                self.models = models\n",
    "                self.weights = weights\n",
    "                self.ensemble_type = ensemble_type\n",
    "            \n",
    "            def predict(self, X):\n",
    "                # Simplified ensemble prediction for benchmarking\n",
    "                predictions = []\n",
    "                \n",
    "                for name, model in self.models.items():\n",
    "                    if name in ['random_forest', 'knn']:\n",
    "                        pred = model.predict(X)\n",
    "                    else:\n",
    "                        with torch.no_grad():\n",
    "                            X_tensor = torch.from_numpy(X).float()\n",
    "                            pred = model(X_tensor).numpy()\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                # Simple averaging for throughput testing\n",
    "                return np.mean(predictions, axis=0)\n",
    "        \n",
    "        # Get references to loaded models for ensemble\n",
    "        ensemble_models = {}\n",
    "        if 'random_forest' in self.models:\n",
    "            ensemble_models['random_forest'] = self.models['random_forest']\n",
    "        if 'knn' in self.models:\n",
    "            ensemble_models['knn'] = self.models['knn']\n",
    "        if 'modularnet' in self.models:\n",
    "            ensemble_models['modularnet'] = self.models['modularnet']\n",
    "        if 'hierarchicalpredictionnet' in self.models:\n",
    "            ensemble_models['hierarchicalpredictionnet'] = self.models['hierarchicalpredictionnet']\n",
    "        \n",
    "        return EnsembleModel(ensemble_models, weights_data)\n",
    "\n",
    "# Load models\n",
    "model_manager = ModelManager(THROUGHPUT_CONFIG)\n",
    "models = model_manager.load_all_models()\n",
    "test_features = model_manager.test_data\n",
    "\n",
    "print(f\"\\nMemory usage: {test_features.nbytes / (1024**2):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Single Sample Latency Analysis\n",
    "\n",
    "Measure inference latency for individual predictions to assess real-time performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:04,006 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SINGLE SAMPLE LATENCY ANALYSIS\n",
      "============================================================\n",
      "Target: <10ms for real-time analysis\n",
      "\n",
      "\n",
      "Benchmarking random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:04,615 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=1): 100%|██████████| 100/100 [00:08<00:00, 11.72it/s]\n",
      "2025-08-17 09:50:13,267 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CPU Performance:\n",
      "    Mean latency: 82.46 ms\n",
      "    Std deviation: 16.64 ms\n",
      "    p50 latency: 79.59 ms\n",
      "    p95 latency: 107.68 ms\n",
      "    p99 latency: 158.44 ms\n",
      "    Meets real-time target: ✗\n",
      "\n",
      "Benchmarking knn...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:13,658 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=1): 100%|██████████| 100/100 [00:04<00:00, 21.83it/s]\n",
      "2025-08-17 09:50:18,376 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CPU Performance:\n",
      "    Mean latency: 44.88 ms\n",
      "    Std deviation: 12.51 ms\n",
      "    p50 latency: 40.59 ms\n",
      "    p95 latency: 63.29 ms\n",
      "    p99 latency: 97.92 ms\n",
      "    Meets real-time target: ✗\n",
      "\n",
      "Benchmarking simple_weighted_ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:19,130 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=1): 100%|██████████| 100/100 [00:13<00:00,  7.44it/s]\n",
      "2025-08-17 09:50:32,700 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CPU Performance:\n",
      "    Mean latency: 129.83 ms\n",
      "    Std deviation: 25.46 ms\n",
      "    p50 latency: 121.38 ms\n",
      "    p95 latency: 178.08 ms\n",
      "    p99 latency: 234.58 ms\n",
      "    Meets real-time target: ✗\n",
      "\n",
      "Benchmarking bin_by_bin_ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:33,593 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=1): 100%|██████████| 100/100 [00:12<00:00,  7.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CPU Performance:\n",
      "    Mean latency: 125.48 ms\n",
      "    Std deviation: 33.34 ms\n",
      "    p50 latency: 114.14 ms\n",
      "    p95 latency: 220.40 ms\n",
      "    p99 latency: 230.48 ms\n",
      "    Meets real-time target: ✗\n",
      "\n",
      "============================================================\n",
      "SINGLE SAMPLE LATENCY SUMMARY\n",
      "============================================================\n",
      "Model                               Device   Mean (ms)    p95 (ms)     Real-time \n",
      "--------------------------------------------------------------------------------\n",
      "random_forest                       cpu      82.46        107.68       ✗         \n",
      "knn                                 cpu      44.88        63.29        ✗         \n",
      "simple_weighted_ensemble            cpu      129.83       178.08       ✗         \n",
      "bin_by_bin_ensemble                 cpu      125.48       220.40       ✗         \n"
     ]
    }
   ],
   "source": [
    "# Initialize benchmarker\n",
    "benchmarker = ThroughputBenchmarker(THROUGHPUT_CONFIG)\n",
    "\n",
    "print(\"SINGLE SAMPLE LATENCY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Target: <10ms for real-time analysis\\n\")\n",
    "\n",
    "# Store results\n",
    "single_sample_results = {}\n",
    "\n",
    "# Test each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nBenchmarking {model_name}...\")\n",
    "    \n",
    "    # Test on CPU\n",
    "    metrics_cpu = benchmarker.measure_inference_time(\n",
    "        model=model,\n",
    "        inputs=test_features,\n",
    "        model_name=model_name,\n",
    "        device='cpu',\n",
    "        batch_size=1\n",
    "    )\n",
    "    \n",
    "    single_sample_results[f\"{model_name}_cpu\"] = metrics_cpu\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"  CPU Performance:\")\n",
    "    print(f\"    Mean latency: {metrics_cpu.mean_latency_ms:.2f} ms\")\n",
    "    print(f\"    Std deviation: {metrics_cpu.std_latency_ms:.2f} ms\")\n",
    "    print(f\"    p50 latency: {metrics_cpu.p50_latency_ms:.2f} ms\")\n",
    "    print(f\"    p95 latency: {metrics_cpu.p95_latency_ms:.2f} ms\")\n",
    "    print(f\"    p99 latency: {metrics_cpu.p99_latency_ms:.2f} ms\")\n",
    "    print(f\"    Meets real-time target: {'✓' if metrics_cpu.p95_latency_ms < 10 else '✗'}\")\n",
    "    \n",
    "    # Test on GPU for neural networks\n",
    "    is_neural = any(x in model_name.lower() for x in ['modular', 'hierarchical', 'sparse', 'regional', 'net'])\n",
    "    if is_neural and len(THROUGHPUT_CONFIG['hardware']['devices']) > 1:\n",
    "        device = THROUGHPUT_CONFIG['hardware']['devices'][1]  # GPU device\n",
    "        \n",
    "        metrics_gpu = benchmarker.measure_inference_time(\n",
    "            model=model,\n",
    "            inputs=test_features,\n",
    "            model_name=model_name,\n",
    "            device=device,\n",
    "            batch_size=1\n",
    "        )\n",
    "        \n",
    "        single_sample_results[f\"{model_name}_{device}\"] = metrics_gpu\n",
    "        \n",
    "        print(f\"  \\n{device.upper()} Performance:\")\n",
    "        print(f\"    Mean latency: {metrics_gpu.mean_latency_ms:.2f} ms\")\n",
    "        print(f\"    p95 latency: {metrics_gpu.p95_latency_ms:.2f} ms\")\n",
    "        print(f\"    Speedup vs CPU: {metrics_cpu.mean_latency_ms / metrics_gpu.mean_latency_ms:.1f}x\")\n",
    "        print(f\"    Meets real-time target: {'✓' if metrics_gpu.p95_latency_ms < 10 else '✗'}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SINGLE SAMPLE LATENCY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<35} {'Device':<8} {'Mean (ms)':<12} {'p95 (ms)':<12} {'Real-time':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for key, metrics in single_sample_results.items():\n",
    "    model_name, device = key.rsplit('_', 1)\n",
    "    meets_target = '✓' if metrics.p95_latency_ms < 10 else '✗'\n",
    "    print(f\"{model_name:<35} {device:<8} {metrics.mean_latency_ms:<12.2f} \"\n",
    "          f\"{metrics.p95_latency_ms:<12.2f} {meets_target:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 7. Batch Processing Performance\n",
    "\n",
    "Evaluate throughput performance across different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:46,636 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BATCH PROCESSING PERFORMANCE ANALYSIS\n",
      "============================================================\n",
      "\n",
      "RANDOM_FOREST Batch Processing:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:47,119 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=1): 100%|██████████| 100/100 [00:08<00:00, 11.85it/s]\n",
      "2025-08-17 09:50:55,662 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size     1: Throughput =       12 samples/s, Latency =  81.55 ms, Memory =    0.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:50:56,127 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=32): 100%|██████████| 100/100 [00:10<00:00,  9.47it/s]\n",
      "2025-08-17 09:51:06,829 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size    32: Throughput =      317 samples/s, Latency = 100.80 ms, Memory =    2.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:51:07,605 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=128): 100%|██████████| 100/100 [00:12<00:00,  8.12it/s]\n",
      "2025-08-17 09:51:20,073 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   128: Throughput =     1069 samples/s, Latency = 119.71 ms, Memory =    8.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:51:22,056 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=512): 100%|██████████| 100/100 [00:22<00:00,  4.37it/s]\n",
      "2025-08-17 09:51:45,084 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   512: Throughput =     2308 samples/s, Latency = 221.86 ms, Memory =   33.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:51:47,645 - INFO - Running 100 timing iterations...\n",
      "Benchmarking random_forest (batch_size=1000): 100%|██████████| 100/100 [00:32<00:00,  3.11it/s]\n",
      "2025-08-17 09:52:19,873 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size  1000: Throughput =     3152 samples/s, Latency = 317.22 ms, Memory =   65.1 MB\n",
      "\n",
      "KNN Batch Processing:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:52:20,239 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=1): 100%|██████████| 100/100 [00:04<00:00, 21.67it/s]\n",
      "2025-08-17 09:52:25,131 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size     1: Throughput =       22 samples/s, Latency =  44.75 ms, Memory =    0.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:52:26,082 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=32): 100%|██████████| 100/100 [00:10<00:00,  9.33it/s]\n",
      "2025-08-17 09:52:36,909 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size    32: Throughput =      305 samples/s, Latency = 104.83 ms, Memory =    1.9 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:52:39,024 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=128): 100%|██████████| 100/100 [00:20<00:00,  4.88it/s]\n",
      "2025-08-17 09:52:59,652 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   128: Throughput =      631 samples/s, Latency = 202.88 ms, Memory =    7.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:53:07,080 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=512): 100%|██████████| 100/100 [01:16<00:00,  1.31it/s]\n",
      "2025-08-17 09:54:23,688 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   512: Throughput =      672 samples/s, Latency = 762.02 ms, Memory =   28.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:54:38,559 - INFO - Running 100 timing iterations...\n",
      "Benchmarking knn (batch_size=1000): 100%|██████████| 100/100 [02:34<00:00,  1.54s/it]\n",
      "2025-08-17 09:57:13,038 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size  1000: Throughput =      649 samples/s, Latency = 1541.36 ms, Memory =   54.9 MB\n",
      "\n",
      "SIMPLE_WEIGHTED_ENSEMBLE Batch Processing:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:57:13,874 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=1): 100%|██████████| 100/100 [00:12<00:00,  7.72it/s]\n",
      "2025-08-17 09:57:26,940 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size     1: Throughput =        8 samples/s, Latency = 126.86 ms, Memory =    0.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:57:28,541 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=32): 100%|██████████| 100/100 [00:19<00:00,  5.14it/s]\n",
      "2025-08-17 09:57:48,121 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size    32: Throughput =      166 samples/s, Latency = 192.28 ms, Memory =    2.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:57:50,703 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=128): 100%|██████████| 100/100 [00:34<00:00,  2.92it/s]\n",
      "2025-08-17 09:58:25,080 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   128: Throughput =      378 samples/s, Latency = 339.02 ms, Memory =    8.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 09:58:34,337 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=512): 100%|██████████| 100/100 [01:39<00:00,  1.01it/s]\n",
      "2025-08-17 10:00:13,478 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   512: Throughput =      518 samples/s, Latency = 987.62 ms, Memory =   33.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:00:32,412 - INFO - Running 100 timing iterations...\n",
      "Benchmarking simple_weighted_ensemble (batch_size=1000): 100%|██████████| 100/100 [03:12<00:00,  1.92s/it]\n",
      "2025-08-17 10:03:44,916 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size  1000: Throughput =      521 samples/s, Latency = 1920.37 ms, Memory =   65.1 MB\n",
      "\n",
      "BIN_BY_BIN_ENSEMBLE Batch Processing:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:03:45,752 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=1): 100%|██████████| 100/100 [00:12<00:00,  8.12it/s]\n",
      "2025-08-17 10:03:58,172 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size     1: Throughput =        8 samples/s, Latency = 121.66 ms, Memory =    0.2 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:03:59,378 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=32): 100%|██████████| 100/100 [00:19<00:00,  5.01it/s]\n",
      "2025-08-17 10:04:19,474 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size    32: Throughput =      162 samples/s, Latency = 197.15 ms, Memory =    2.3 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:04:22,061 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=128): 100%|██████████| 100/100 [00:34<00:00,  2.93it/s]\n",
      "2025-08-17 10:04:56,271 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   128: Throughput =      382 samples/s, Latency = 335.38 ms, Memory =    8.6 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:05:05,660 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=512): 100%|██████████| 100/100 [01:38<00:00,  1.02it/s]\n",
      "2025-08-17 10:06:43,969 - INFO - Running 10 warmup iterations...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size   512: Throughput =      524 samples/s, Latency = 977.81 ms, Memory =   33.5 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 10:07:02,997 - INFO - Running 100 timing iterations...\n",
      "Benchmarking bin_by_bin_ensemble (batch_size=1000): 100%|██████████| 100/100 [03:11<00:00,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch size  1000: Throughput =      524 samples/s, Latency = 1907.37 ms, Memory =   65.1 MB\n",
      "\n",
      "============================================================\n",
      "OPTIMAL BATCH SIZES\n",
      "============================================================\n",
      "\n",
      "random_forest:\n",
      "  Best throughput: 3152 samples/s at batch_size=1000\n",
      "  Best efficiency: 135.7 samples/s/MB at batch_size=32\n",
      "\n",
      "knn:\n",
      "  Best throughput: 672 samples/s at batch_size=512\n",
      "  Best efficiency: 156.7 samples/s/MB at batch_size=32\n",
      "\n",
      "simple_weighted_ensemble:\n",
      "  Best throughput: 521 samples/s at batch_size=1000\n",
      "  Best efficiency: 71.2 samples/s/MB at batch_size=32\n",
      "\n",
      "bin_by_bin_ensemble:\n",
      "  Best throughput: 524 samples/s at batch_size=1000\n",
      "  Best efficiency: 69.7 samples/s/MB at batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBATCH PROCESSING PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store batch processing results\n",
    "batch_results = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Test subset of batch sizes for efficiency\n",
    "test_batch_sizes = [1, 32, 128, 512, 1000]\n",
    "\n",
    "# Test each model\n",
    "for model_name in models.keys():\n",
    "    model = models[model_name]\n",
    "    print(f\"\\n{model_name.upper()} Batch Processing:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for batch_size in test_batch_sizes:\n",
    "        # Skip large batches if insufficient data\n",
    "        if batch_size > len(test_features):\n",
    "            continue\n",
    "        \n",
    "        # Measure performance\n",
    "        metrics = benchmarker.measure_inference_time(\n",
    "            model=model,\n",
    "            inputs=test_features,\n",
    "            model_name=model_name,\n",
    "            device='cpu',\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        batch_results[model_name]['batch_size'].append(batch_size)\n",
    "        batch_results[model_name]['throughput'].append(metrics.samples_per_second)\n",
    "        batch_results[model_name]['latency'].append(metrics.mean_latency_ms)\n",
    "        batch_results[model_name]['memory'].append(metrics.peak_memory_mb)\n",
    "        \n",
    "        print(f\"  Batch size {batch_size:>5}: \"\n",
    "              f\"Throughput = {metrics.samples_per_second:>8.0f} samples/s, \"\n",
    "              f\"Latency = {metrics.mean_latency_ms:>6.2f} ms, \"\n",
    "              f\"Memory = {metrics.peak_memory_mb:>6.1f} MB\")\n",
    "\n",
    "# Find optimal batch sizes\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"OPTIMAL BATCH SIZES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "optimal_batch_sizes = {}\n",
    "\n",
    "for model_name, results in batch_results.items():\n",
    "    if not results['throughput']:\n",
    "        continue\n",
    "        \n",
    "    # Find batch size with best throughput\n",
    "    max_throughput_idx = np.argmax(results['throughput'])\n",
    "    optimal_batch = results['batch_size'][max_throughput_idx]\n",
    "    max_throughput = results['throughput'][max_throughput_idx]\n",
    "    \n",
    "    # Find batch size with best efficiency (throughput per MB)\n",
    "    efficiency = np.array(results['throughput']) / (np.array(results['memory']) + 1e-6)\n",
    "    max_efficiency_idx = np.argmax(efficiency)\n",
    "    efficient_batch = results['batch_size'][max_efficiency_idx]\n",
    "    \n",
    "    optimal_batch_sizes[model_name] = {\n",
    "        'max_throughput': optimal_batch,\n",
    "        'max_efficiency': efficient_batch\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Best throughput: {max_throughput:.0f} samples/s at batch_size={optimal_batch}\")\n",
    "    print(f\"  Best efficiency: {efficiency[max_efficiency_idx]:.1f} samples/s/MB at batch_size={efficient_batch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 8. Summary Report and Deployment Recommendations\n",
    "\n",
    "Generate comprehensive summary with deployment recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE THROUGHPUT ANALYSIS REPORT\n",
      "================================================================================\n",
      "\n",
      "1. EXECUTIVE SUMMARY\n",
      "------------------------------------------------------------\n",
      "No models meet real-time target (<10ms)\n",
      "Best for throughput: random_forest (3152 samples/s)\n",
      "\n",
      "2. MODEL PERFORMANCE COMPARISON\n",
      "------------------------------------------------------------\n",
      "Model                               Latency (ms)    Throughput     \n",
      "                                    p50 | p95       (samples/s)    \n",
      "-----------------------------------------------------------------\n",
      "random_forest                       79.6 | 107.7           3152\n",
      "knn                                 40.6 | 63.3            672\n",
      "simple_weighted_ensemble            121.4 | 178.1            521\n",
      "bin_by_bin_ensemble                 114.1 | 220.4            524\n",
      "\n",
      "3. DEPLOYMENT RECOMMENDATIONS\n",
      "------------------------------------------------------------\n",
      "\n",
      "Real-time Analysis (<10ms):\n",
      "  Recommended: Random Forest or KNN\n",
      "  Configuration: batch_size=1, single thread\n",
      "\n",
      "Batch Processing:\n",
      "  Recommended: Tree-based models (RF/KNN) for CPU\n",
      "  Configuration: batch_size=1000, all CPU cores\n",
      "\n",
      "GPU Acceleration:\n",
      "  Recommended: Neural networks (ModularNet, HierarchicalNet, etc.)\n",
      "  Configuration: batch_size=256-1000 for optimal GPU utilization\n",
      "\n",
      "Edge Deployment (<512MB):\n",
      "  Recommended: KNN (smallest footprint)\n",
      "  Configuration: batch_size=10, reduced features if needed\n",
      "\n",
      "================================================================================\n",
      "Report generation complete\n",
      "\n",
      "Results saved to: ../data/results/throughput/throughput_results.json\n",
      "\n",
      "THROUGHPUT ANALYSIS COMPLETE\n"
     ]
    }
   ],
   "source": [
    "def generate_summary_report():\n",
    "    \"\"\"Generate comprehensive throughput analysis report\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE THROUGHPUT ANALYSIS REPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Executive Summary\n",
    "    print(\"\\n1. EXECUTIVE SUMMARY\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Find best models for each scenario\n",
    "    best_realtime = None\n",
    "    best_throughput = None\n",
    "    \n",
    "    # Analyze single sample results\n",
    "    for key, metrics in single_sample_results.items():\n",
    "        if 'cpu' in key:\n",
    "            model_name = key.replace('_cpu', '')\n",
    "            if metrics.p95_latency_ms < 10:\n",
    "                if best_realtime is None or metrics.mean_latency_ms < best_realtime[1]:\n",
    "                    best_realtime = (model_name, metrics.mean_latency_ms)\n",
    "    \n",
    "    # Analyze batch results\n",
    "    for model_name, results in batch_results.items():\n",
    "        if results['throughput']:\n",
    "            max_throughput = max(results['throughput'])\n",
    "            if best_throughput is None or max_throughput > best_throughput[1]:\n",
    "                best_throughput = (model_name, max_throughput)\n",
    "    \n",
    "    if best_realtime:\n",
    "        print(f\"Best for real-time (<10ms): {best_realtime[0]} ({best_realtime[1]:.2f} ms)\")\n",
    "    else:\n",
    "        print(\"No models meet real-time target (<10ms)\")\n",
    "        \n",
    "    if best_throughput:\n",
    "        print(f\"Best for throughput: {best_throughput[0]} ({best_throughput[1]:.0f} samples/s)\")\n",
    "    \n",
    "    # 2. Model Performance Table\n",
    "    print(\"\\n2. MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Model':<35} {'Latency (ms)':<15} {'Throughput':<15}\")\n",
    "    print(f\"{'':35} {'p50 | p95':<15} {'(samples/s)':<15}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        # Get metrics\n",
    "        single_metrics = single_sample_results.get(f\"{model_name}_cpu\")\n",
    "        \n",
    "        if single_metrics and model_name in batch_results:\n",
    "            batch_data = batch_results[model_name]\n",
    "            max_throughput = max(batch_data['throughput']) if batch_data['throughput'] else 0\n",
    "            \n",
    "            print(f\"{model_name:<35} \"\n",
    "                  f\"{single_metrics.p50_latency_ms:>4.1f} | {single_metrics.p95_latency_ms:>4.1f}     \"\n",
    "                  f\"{max_throughput:>10.0f}\")\n",
    "    \n",
    "    # 3. Deployment Recommendations\n",
    "    print(\"\\n3. DEPLOYMENT RECOMMENDATIONS\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    print(\"\\nReal-time Analysis (<10ms):\")\n",
    "    print(\"  Recommended: Random Forest or KNN\")\n",
    "    print(\"  Configuration: batch_size=1, single thread\")\n",
    "    \n",
    "    print(\"\\nBatch Processing:\")\n",
    "    print(\"  Recommended: Tree-based models (RF/KNN) for CPU\")\n",
    "    print(\"  Configuration: batch_size=1000, all CPU cores\")\n",
    "    \n",
    "    print(\"\\nGPU Acceleration:\")\n",
    "    print(\"  Recommended: Neural networks (ModularNet, HierarchicalNet, etc.)\")\n",
    "    print(\"  Configuration: batch_size=256-1000 for optimal GPU utilization\")\n",
    "    \n",
    "    print(\"\\nEdge Deployment (<512MB):\")\n",
    "    print(\"  Recommended: KNN (smallest footprint)\")\n",
    "    print(\"  Configuration: batch_size=10, reduced features if needed\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Report generation complete\")\n",
    "\n",
    "# Generate report\n",
    "generate_summary_report()\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(THROUGHPUT_CONFIG['paths']['results_dir'], \n",
    "                           'throughput', 'throughput_results.json')\n",
    "\n",
    "# Prepare results for export\n",
    "export_results = {\n",
    "    'timestamp': pd.Timestamp.now().isoformat(),\n",
    "    'hardware': {\n",
    "        'cpu_count': cpu_count,\n",
    "        'memory_gb': memory.total / (1024**3),\n",
    "        'device': str(DEVICE)\n",
    "    },\n",
    "    'single_sample_latency': {},\n",
    "    'batch_processing': dict(batch_results),\n",
    "    'optimal_batch_sizes': optimal_batch_sizes\n",
    "}\n",
    "\n",
    "# Convert metrics to serializable format\n",
    "for key, metrics in single_sample_results.items():\n",
    "    export_results['single_sample_latency'][key] = {\n",
    "        'mean_ms': metrics.mean_latency_ms,\n",
    "        'p50_ms': metrics.p50_latency_ms,\n",
    "        'p95_ms': metrics.p95_latency_ms,\n",
    "        'p99_ms': metrics.p99_latency_ms,\n",
    "        'samples_per_second': metrics.samples_per_second\n",
    "    }\n",
    "\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(export_results, f, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(\"\\nTHROUGHPUT ANALYSIS COMPLETE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio729p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
